
Introduction 

The period of crisis and upheaval which the world has been experiencing for some years now has given rise today to concrete perspectives concerning the application of new information and communication technology in all training-related activities.
Sometimes, this training sector has difficulty implementing these new tools now available because the sector is so very anchored to its traditions.
Once this sector is modernized, it becomes more efficient with a tendency to become more closely integrated within the enterprise, which in turn may adjust more quickly to changes or developments in the market.

In this special Labour Education issue, we propose an overview of how these new information and communication technologies are applied in the areas of education and training with a view to adapting them better to the needs of workers, trade unions, the public service, enterprises and the economy as a whole.
However, we cannot within the constraints of this exercise provide any exhaustive analysis or discussion.
We have attempted to present and describe the facts as simply as possible, trying at the same time to be clear and understandable.
Though the technical terms are explained, the reader will find a glossary at the end.

In the first part, we shall deal briefly with certain socio-economic aspects of the advent of new technologies in our society.

In the second part, we shall deal with the different types of training activities by listing the traditional methods and by attempting to make a distinction between production methods and teaching supports.
Next we shall proceed to examine the technical tools available on the market and the best advantages they offer.
We shall also provide a brief overview of the methodology to be respected in determining the different stages in the production and the dissemination of documents.

In the third part, we shall take a look at the best application of these new technical tools in terms of each medium and how to produce and disseminate information on all media.
The information we provide is the latest currently available.
The tables comparing the products we present as illustrations are not meant to be exhaustive, nor are the figures used completely exact.

Notwithstanding, we do hope that these tips will be useful to the reader.

Finally, we shall try to highlight the advantages enjoyed by using the new information and communication technologies in the field of training.
In each case, we shall try to stick to a concrete approach for the sake of the reader trying to chart these new grounds opening up before us today, and where it is not easy to find one's bearings and one's way.
We hope that the glossary at the end of this document will prove very useful to non-specialists who feel lost in this jungle of acronyms and strange names.

Attached to this Labour Education edition is a diskette on which you will find an example of an interactive application which you may use on your IBM-compatible personal computer under the Windows programme, or on your Macintosh under System 7.

PART ONE 

Towards a new technological system 

Figure 1

The current technological system

In most countries, unemployment has been steadily rising for the last few years.
New information processing and communication technologies are foremost among the most often cited reasons for this: 
"An increasing number of studies link the dwindling demand for unskilled workers to the technological revolution seen over the last few years" ( World Employment 1995, An ILO Report , p. 52).

Although applying new technologies creates new jobs, the overall picture is not entirely positive.

Earlier technological adjustment crises during the eighteenth and nineteenth centuries did not give rise to immediate offsetting effects.
There are no automatic scales which immediately replace lost jobs.
Economists are divided as to whether the labour market should be left to find its own equilibrium or whether governments should intervene actively to speed up the trend.

One project, known as Information Highways launched by the Vice-President of the United States of America, Edward Gore, and another known as "The White Book" launched by the former President of the European Commission, Jacques Delors, bear proof of a willingness to steer and stimulate ongoing changes.

Programmes of an experimental nature or based on incentives complement these projects to facilitate, as it were, the establishment of infrastructures deemed necessary for the development of the new economic activities we are already witnessing and which should result in the creation of new jobs.
The G7 meeting which took place in February 1995 on this topic illustrate the interest which industrialized countries have in such issues.

Technologies result from the application of prevailing scientific knowledge of each century in the areas of production, management and communication.
They take shape in the form of technologies which combine material and know-how and coincide with new economic activities, new jobs, and a new cycle of prosperity.

Throughout the history of mankind, introducing new technologies has clashed violently with social and economic structures, giving rise to workers' riots and sabotage of machinery.

The technological system which has settled in has little by little led to a hierarchical pyramid model, which is the very foundation of enterprises and institutions.
Interaction between human beings functions within the enterprise on the model of biological networks which, due to its horizontal nature, runs counter to the top-down power structure.
But the system we now find ourselves in since the introduction of information and communication technologies adds new dimensions of flexibility, globalization, an economy based on invisible transactions, and complexity management.
These dimensions are difficult to imagine for they imply a radical change in attitude which is often hard to realize.

The fractal society 

According to Andràs November, with the introduction of microprocessors in the production process, we can no longer draw a clear line of distinction between industrial technology (exclusively applied in manufacture) and information technology (used for administrative activities).
They all converge in the "knowledge industries" which imply the application of knowledge and the acquisition of extensive information at the level of the manufacturer's workshop and the utilization of sophisticated electronic machines...
With the application of the new technologies, information becomes one of the main resources in production activities.
Mastering information (i.e. memory capacity, processing and transmission of data) is a condition for the utilisation of other factors of production.

(Andràs November: New technologies and socio-economic changes , IILS-ILO, Geneva, 1990, p. 20).

In workshops and offices, the microprocessor takes pride of place, discreetly positioned at the centre of any system.
All assignments can be computerized: administration and accounts, Electronic Document Management (EDM), Electronic Document Interchange (EDI), Computer-Assisted Teaching (CAT), Computer-Assisted Design (CAD), Computer-Assisted Manufacture (CAM), Computer-Assisted Production Management (CAPM).
It has been said that the microprocessor is the steam-engine of our time. (Jacques Lesourne, Les mille sentiers de l'avenir , Paris, 1981, p. 281).
Now all the microprocessor still has to do is win over households and bring society closer together.

Information technology, which lies at the core of this new technological system, is a powerful complexity management and simulation tool.
Minds that are set in their habits and steeped in the ease of traditions do not adjust easily to it.
Today, over and above mechanized models and biological networks, the challenge lies in considering the complexity of nature, simulating chaos, simultaneously managing all factors, dealing with disasters, favouring individual creativity within the group and seeking maximum optimization of processes in terms of design, production, information, and dissemination of values and products.

Through a series of rational calculations on "iterative" equations, the computer displays on the screen natural shapes called "fractal" images: This fractal model is at the heart of nature, as much in its harmonious development as in its accidents or disasters: cloud formation, lightning flashes, erosion of mountains, of coastlines, structures of leaves and trees, plant growth, or social or economic phenomena.

Fractal images have the fascinating property of being identical both in the overall image and in the detailed image.
Reality is thus simulated in the slightest detail, the enterprise in turn becoming virtual like reality.

By a disturbing effect of symmetry, if a fractal equation describes the image of a natural form, then conversely, any image of natural forms (and any image for that matter) may be converted to a series of fractal equations.
This is therefore the equivalent of an ultimate form of image Compression" whereby images are reduced, as it were, to their "mathematical quintessence".

The pyramid appears upside down.
The individual becomes more efficient.
Equipped with a microcomputer which has microprocessors becoming more powerful by the day, and with greater, quicker and more affordable memory capacity, work capacity is increased.
Through the internal network and increasingly easy and economical connections on the local or international telecommunications networks, communication is quicker and allows personal access to more extensive sources of information.
It is in the enterprise's interest to foster and channel the creativity of its collaborators and to optimize communication among them.

The individual who is equipped to work and communicate within a network is more inclined to become a partner (internal or external) rather than an employee of the enterprise which seeks flexibility and competitiveness.
Autonomy, work capacity and reflexes are much superior to those which decisionmaking circles and classic enterprise protocol allow.
In some cases, an individual may even become a competitor for the enterprise which does not change quickly enough when faced with market competition and globalization.

The enterprise has become virtual, split up as it has been for some time now among its many partners.
It exists only through its communications network and the information flow which is in constant circulation among all the parts.
The enterprise is everywhere and nowhere in search of virtuous circles and the biggest, most profitable market possible.
The principle is: zero stock, zero error.
Those who are not caught up in this whirlwind are not guests at the banquet: they stand by the wayside, outsiders.
It would appear that this is the price to be paid for economic development and the production of wealth.

Figure 2

fractal images

Figure 3

The virtual enterprise 

Trade unionism and new communication technologies 

In response to these changes, trade unions are striving to take new stances in order to fulfil better their mission, which is to protect workers.
Among many such missions is the vital role they must play in safeguarding social links or in bringing society closer together.
There are those who are still inclined to condemn the use of computers in the workplace, or telejobs at home, insofar as these help to increase the profits of investors even more at the expense of workers.

They have difficulty accepting this tool which they tend to consider as an instrument of alienation.
Other more extreme trade unions develop resistance movements, symbolically destroying computers, as did the Luddites in England during the eighteenth century.
The computer is an emblematic target, the visible face of a sprawling system which has wound its way everywhere and which has led to the collapse of the values and bearings which had been the very foundation of the life of workers as well as of the organization of enterprises and trade unions ever since the Industrial Revolution.

The computer may well prove to be a fabulous tool for human and economic development.
Like the other social partners trade unions are in need of new communication technologies.
Furthermore, most of them already use computers for their mail, administration, and for editing their newsletters, etc. Moreover, the bigger federations and trade unions already use computers which are linked to telecommunications networks for the purpose of consulting data banks or exchanging electronic mail.
They can gain access to law documents, international standards and agreements; they can fuel the documentation, research and reflection necessary to design work documents, or obtain vital information to carry out successful negotiations with partners who for their part are well informed.

Trade unions also need to master new communication technologies if they are to train their executive members and their trainers: the very persons who will train the unionists of tomorrow, among them young workers, an increasing number of whom will themselves be equipped (in industrialized countries) with their own microcomputers connected to networks, as already is the case with their television sets and their telephones.

All the snags which stood in the way of the widespread use of new communication technologies are now giving way.
As regards the difficulty of utilization and the cost of equipment, with the new, modern disk-operating systems that offer user-friendly graphic interfaces and on-line help systems, using computers has become very simple and no longer requires training in computer science or obscure languages.
The overpowering rate at which microprocessors have been developing and furthermore their wide distribution also mean cheaper equipment readily available to private individuals in industrialized countries, and thus within the reach of institutions in developing countries.

Another snag is installing telephone connections and their high costs.
The complex methods and the cost of accessing servers and data banks is yet another problem.

However, the popularization of Internet through World Wide Web facilitates the agreements between telecommunications operators and commercial servers who propose attractive rates for the enterprise and even for the informed amateur.

Electronic mail and documentary research can be carried out in an open manner throughout the world at the price of a telephone conversation plus a rental charge (generally inexpensive) at the server location to which the user is connected.
Important structures such as enterprises or public institutions can also consider the idea of becoming information servers themselves with simple, practical and economical technical solutions.
A worldwide interconnection is underway and is already foreshadowing these information highways we are being promised for tomorrow.

These tools are neither the exclusive reserve of enterprises for increasing profits nor are they meant solely to nurture the passion of professionals or informed amateurs.
Workers and the organizations which represent or train them can and must get hold of them resolutely - in all countries.
Solutions exist even in the least developed countries.
In cases where they do not exist, arrangements can be made with the national or private administrative authorities which are responsible for telecommunications, or the relevant international organizations which may help develop these new means of communication.

The attitude of the major international trade union organizations and the survey carried out by the Labour Telematics Centre (Manchester, UK) in 1990 on the use of telematics by trade unions show that such practices are becoming more prevalent.
In its Report on the preliminary project on trade union organisation, education and training and the possibilities offered by information technology and telecommunications entitled Telematics: Golden opportunity and impossible feat published in May 1994, the Labour Telematics Centre drew up a list of recommendations and proposals based on a logical approach which appears to be on the right track even if it needs to be updated yet again.
In this field, projects, like products in the business sector, have an increasingly shorter life span.

With this in mind, the ILO, and more particularly the Bureau for Workers' Activities, is taking measures which are likely to favour the move towards developing an integrated approach to new communication technologies with all its partners: international organizations, trade unions, and organizations dedicated to workers' education and development.
This issue of Labour Education hopes to contribute (in whatever modest way) to the fulfilment of this goal.

Changes in the media 

In the media industries as in other sectors, traditional structures are adjusting to these new technologies: skills are evolving, jobs are disappearing.

Enterprises which do not adjust are probably doomed to disappear.
New professions are cropping up but it is difficult to say at this time whether the employment figure is positive in this sector as it is in other sectors.

Once the new information-processing technologies are applied to those media whose very calling is to deal with information, we might be tempted to say that "we have come full circle".
Yet is it a question of completing a virtuous circle which generates economic miracles? 
According to some observers, the economy based on invisible transactions creates potentially infinite added value since it is not limited by the constraints of material scarcity.
The data, ideas and images market constitutes a veritable mine of jobs.
(Charles Goldfinger, "Le travail dans l'économie de l'immatériel", in Le Monde , 23 May 1995).

There is much talk about the computerization of traditional media industries: the advent of multimedia is now an irreversible fact.
All the traditional channels are involved: press, publishing, radio, television, telecommunications.
The microcomputer offers the possibility to edit all types of documents using its traditional applications, but above all it tends to integrate all applications in a new and single support called "multimedia".

Multimedia is a new medium meant only for use with the microcomputer, which enables still or moving images, texts, sound and video to be combined in one product (often called "application").
Each element can be accessed at the request of the user, following the methods prescribed by the product designers.
This new medium can be published on any computer support within the limits of the features and performance of each of these supports: hard drive, diskette, or CDROM.

Multimedia can also be published on-line on local networks , or via the telecommunications network, on local or international servers - or even on Internet and its wider network, World Wide Web, for example.
This is a novel and revolutionary way of publishing and disseminating information and knowledge.
A new script is thus being unfurled before our very eyes, the full implications and potential of which still cannot be gauged.

Telecommunications and television networks will be more and more overtaken by the computer network.
In due course, it seems that all the centralized information broadcasting systems will have to undergo profound changes.
This concerns both telecommunications systems and television channels as well as the vast centralized computer systems.

The media and training 

Formal instruction, vocational training (initial or lifelong) and non-formal training are affected in diverse ways by all of these changes.
On the one hand, we must train to day's youth to master the computer tools which they will be using tomorrow and of which they will have to make the best use.
On the other hand, we must train the workers who are faced with new operations which they have to put into practice today.

Finally, we must adapt training techniques and pedagogical resources management techniques used in education as well as formal and informal training.
All training methods should take into account new communication technologies if they are to be more effective and better adapt themselves to changes in the environment.

Universities, administrative bodies and research-related activities are already quite considerably computerized.
With lower prices, students tend more and more to obtain their personal computer systems.
Certain kinds of instruction are already being carried out using computerized methods, sometimes through distance learning.
In schools, too, computers are the order of the day and are being used more and more widely.

Libraries and documentation centres are being computerized.
Vocational training institutes and enterprises are gradually following suit.
There is talk about the learning enterprise .
The wave is already sweeping through industrialized and rapidly developing countries.

In developing countries and in the poorer countries, the trend is occurring at an uneven pace.

Structural adjustment often brings with it economic difficulties and development projects bring changes in technological systems.
The dilemma of introducing new technologies rears its head as it comes face to face with what it could mean in the long run both in the case of adopting them or not adopting them.
As regards training, investment in new technologies should be made at least at the level of training trainers, executives and engineers.
Yet certain measures may also be taken in the case of agricultural workers or small businesses.
Cost surveys carried out in Africa or Latin America, and published in Innovations in educational and training technologies (Geneva ILO, 1991, p. 128) prove especially important in this regard.

Whatever the case, the "dilemma" of accepting new technologies in Third World countries cannot be resolved in a simplistic way - adopting or refusing them: it is resolved rather like an association between new technologies and local technologies with a view to expanding a country's technological capacity.

Notwithstanding, the "technological dilemma" is still far from resolved.
However, the uncertainty displayed towards new technologies is not limited to developing countries; it concerns everyone since it results from the inability to predict all the possible effects that may be unleashed with the accelerated introduction of the technologies.

(Andràs November, Nouvelles technologies et mutations socio-économiques , Geneva, IILS-ILO, 1990, p. 168).

PART TWO 

Communication and training 

The International Labour Organization points out in paragraph 70.1 of its Programme and Budget for Fee period 1994/95 and within the framework of its extensive Training programme, that: 

"The overall objectives of this major programme are: the acquisition by workers of the skills which allow them to engage in freely chosen and productive employment and adapt to changing labour market needs; increased efficiency and effectiveness of public and private training systems; the participation of employers' and workers' organizations with governments in the formulation and implementation of training policies and programmes; and enhanced opportunities for vulnerable groups to improve their skills, productivity and income".

Moreover, ILO Recommendation No. 150 of 4 June 1975 stipulates: 

in paragraph 15(1) that: 
"Members should gradually extend, adapt and harmonize the vocational training systems of their respective countries to meet the needs for vocational training throughout life of both young persons and adults in all sectors of the economy and branches of economic activity and at all levels of skill and responsibility".

and in paragraph 17(2) that:
"In the provision of training, advantage should be taken, as appropriate, of mass media, mobile units, and correspondence courses and other self-instruction programmes".

In 20 years, considerable progress has certainly been made in terms of the application of the recommendations in paragraph 15, yet for a few years now, under the pressure of the economic crisis, the globalization of the economy and structural adjustment, thrusts ahead followed by set-backs, side-tracking or delays have been observed in fulfilling these goals.
New trends have emerged, such as centralizing company training, practising alternate training, introducing new technologies in the field of training, developing self-taught systems or reducing funds allocated to training programmes, to mention just a few of the trends.

As for paragraph 17, measures have been taken to apply the recommendations in the majority of industrialized and newly-industrialized countries (NICs), under the supervision of the member States and/or in collaboration with public or private training bodies.

Similarly, in developing countries, some programmes have been implemented by the member States (often in collaboration with international organizations or within the framework of bilateral aid) in order to set up the necessary infrastructure.
Still, there remains a lot to be done, particularly in the least developed countries.

In collaboration with the partners concerned, projects have been set up, financed and brought to a successful conclusion.
The results have been satisfactory more or less.
They did not always meet initial expectations.
Among the factors which affect the smooth running of such programmes is one which is not always identified and very rarely alluded to in a clear way - lack of know-how in the area of communication problems .
Decision-makers, project planners, training specialists, trainers (regular or temporary), experts in the different subjects taught, are not always communication specialists able to make coherent and pertinent global proposals, in accurate technical terms, when faced with the problems of communication posed by the definition and implementation of the projects.
Yet education and training do imply using protocols, media, aids and systems which can only be defined through a sound knowledge of communication techniques and technologies.
Although it is a complex and vast scientific field, it is, nonetheless, a grouping of sciences which has given rise to technologies, technical methods, and to methodological approaches.
Unfortunately, mastering these is not always deemed necessary by project designers.

Accordingly, in the fairly recent past, training programmes or communication operations were designed, though not always adapted as they needed to be to either the needs or the target population concerned.
Training methods were set up, sometimes using audiovisual technologies, without taking sufficiently into account the local, economic, cultural and traditional structural constraints.
Training policies or methods were sometimes transferred with the best of intentions from wealthy countries to the least developed countries, without being properly adapted.
Long-distance training institutions or mobile training methods were implemented without conducting sufficient indepth studies of the communication infrastructure, or without taking into consideration the long-term maintenance of the equipment.

The list of these shortfalls could be lengthened.

More recently, the concept of self-training was integrated into training programmes which brought into play "multimedia kits" (composed of written aids, audio and/or video cassettes, and in some cases diskettes), which were often quite successful.
Several computer-assisted teaching programmes were developed.
In general, the results were very good, even though there was not unanimous support/participation.

So in order to respond to this need to "increase the returns and efficiency of training systems" and to "follow up the changing needs of the job market" and so contribute to better utilization of the new communication technologies in the field of training, we shall attempt to acquire a better understanding of communication techniques used in the field of training, as they stand today, not taking theory into account.
We wish to remain as close as possible to the practical concerns of our readers.

We shall make a distinction between the different contexts and media which come into play in the field of training, both in terms of their traditional forms and their more modern aspects.
We shall point out the new uses which are emerging.
We shall provide a few simple methodological hints concerning the production of documents and the definition of editing platforms.
Next, we shall examine in a simplified manner the practical application of these techniques and the advantages they bring to teaching and training.

New communication technologies should not systematically and indiscriminately be brought in to eradicate the traditional methods used in education and training: they should complement them whenever desirable, possible and necessary.
Most of the methods which bring into play human relations are irreplaceable.
Any changes should be carefully considered and should be studied and planned.

The different methods and techniques applied in education and training 

Training is carried out through means of communication called media by using resources, which are the same as tools.
Today training has at its disposal new media and new tools.
The policies and traditions which dominate in the design of programmes and methods and in the setting up of training systems result in the use of different types of techniques and operations which may be represented as follows: 
- imitation of a model, a teacher or a tutor 
- writing and drawing 
- oral expression 
- use of audiovisual aids 
- responsible management of the full use of different resources 
- structuring different areas of knowledge 
- participation and collaboration with peers 
- documentary research 


Figure 4

Classification of training techniques

These activities are not necessarily all to be found within the framework of a training or educational activity, but they may come into play at specific times or continuously depending on their useful features, which may prove more timely in dealing with certain aspects of the operation.

Yet most times these activities are not chosen for their high performance or specific potential.
They are merely imposed by economic constraints, the context or customs.

Training by imitation and simulation 

The oldest form of training lies in imitation , which consists of observing and reproducing as faithfully as possible operations carried out by the "teacher".
The level of skill required is attained by a series of corrections made by the teacher according to the trainee's performance.
This form of training, which is usually practised by the craftsman, is today just as widely used in the enterprise, where it is complemented by alternate sessions of theoretical training.
In certain countries, this type of training allows for the attainment and authentication of very high levels of qualification.

This practical method is fairly easy to apply in a traditional craft industry environment, but it can be adapted very easily within the enterprise.
It holds many advantages, affording a training approach through direct contact with the enterprise's environment, staff and equipment which the trainee must master under the sole guidance of his or her tutor.
It can be geared to populations with very low levels of formal education and have good chances of success.
The person so trained finds place in an enterprise and becomes immediately operational.
Accordingly, he or she stands a very good chance of being employed at the end of such training, hardly a negligible asset in these difficult times for young people in search of a job.

Simulation 

Paradoxically, this time-honoured form of training lends itself today to one of the most spectacular breakthroughs in the area of applying technological innovations to training.
In fact, the computer which, as we said, is a fabulous reality simulation device, is being used increasingly for training in the know-how needed for the running or supervision of complex systems .

The complex system under consideration is modelled after a computer equipped with all the parameters which can vary.
Modification of one or several of these parameters results in the modification of the image on the screen of the status of the overall system in real time and in an interactive manner.
The trainee can thus learn how to pilot the system in his or her own way without actually mobilizing the real system - which would incur costs (exorbitant in some cases) linked to the functioning and the out-of-production state of the real system.
The representation of the operations to be undertaken and of the system's different elements may be made in abstract or symbolic terms.
Yet the increased power and capacity of microprocessors and memories, in addition to the drop in prices, allows today for very realistic representations which offer the trainee greater utilization comfort as well as better cognitive return .

So it is possible today to monitor a nuclear plant, pilot an aircraft, steer the course of a train or a giant oil tanker, monitor a steel-production plant or the construction of a building, or manage an enterprise, a town, an ant's nest, a living creature's health, a garden, or even to carry out stock-market or surgical operations.
Indeed, we could never list all the activities which may be learnt rather easily and quickly with this kind of simulation, and which would be very difficult to achieve using traditional methods.

Virtual reality 

Another threshold has been crossed in terms of perfecting this type of training by simulation with the development of what is now called "virtual reality" .
This technology enables simulation of a three-dimensional (3-D) space, in stereoscope with special lenses, but also 3-D on a screen.
The user is immersed in this space where he can move about using a suitable technique.
The computer calculates in real time the pairs of simultaneous images which it displays in the lenses worn by the user, in front of each eye.
So, the user has the impression that he is moving about in this socalled virtual space which only exists through the computer's calculations.

Any environment can be simulated in this way for training purposes.
The immersion of the trainee is even more complete than the case of simulation on the screen and the returns of the training session is even greater.
The cost of running such applications are rather high for the moment, but it is quite possible that this area will develop by leaps and bounds.

Strangely enough, these computer simulations are being used for by-products in the form of children's games on specialized "game display units", or microcomputers.
Published on diskette or CD-ROM and even on-line on telecommunications networks, they have opened up a very profitable niche on the market - that of educational-game applications .
This is a market in which the big international communications groups have invested and from which they are drawing huge profits.
This explains why quite a number of observers predict that this market will develop considerably in the coming years, and that it has a great future.

Didactic objectives, strictly speaking, have also been targeted: today it is possible to obtain on the market, at very affordable prices, educational products for initial training in selftaught languages, mathematics, sciences which simulate what is often a game environment.
The computer is an untiring tutor and allows the learner to go through the course at his or her own pace and go over the points at leisure as many times as necessary.

If the application designers have provided for it, follow up of the learner is possible.
His or her track can be recorded, as well as the level of performance.
If it is used in the context of tutored learning , the tutor can be apprised of progress made and intervene to work on weak points.
But anyone who owns a microcomputer and who has the necessary motivation can freely and spontaneously use learning tools of this nature.

The first trials conducted in this field a few years ago under the name of "Computer- Assisted Teaching" did not at the time turn out to be a commercial success.
Besides, this type of resources suffered for a long Time on account of this apparent "flop".
The degree of abstraction imposed by microcomputers and the diskoperating systems of the time, their poor ergonomics - none of this allowed for very realistic representations nor for very effective cognitive environments for non-computer scientists.
For a long time, the public believed that the use of computers would always be reserved for computer scientists.
Fortunately, that is no longer the case: our children are there to remind us of this fact if we are not convinced.

The advent of intuitive and ergonomic graphic interfaces of modern disk-operating systems went far in making available to everyone the basic applications of the microcomputer.

Computer science has been able to develop more user-friendly learning applications.
Teachers know well what extent visualization of phenomena, the graphic representation on the screen of knowledge and their structuring in the form of charts or diagrams corresponding to criteria of cognitive ergonomics, are fundamental aspects in the process of acquiring and structuring knowledge .

The prices currently displayed on the computer product and programmes market sanction their use for educational ends, at least at the institutional level, in all countries - even the poorest.
Obviously, in the latter case, the aim is not yet to equip each student with a microcomputer, and even in the wealthy countries this stage is still far off.
Yet it is quite realistic (and especially in the poorest countries), whenever possible, to introduce these to train trainers, engineers, administrators, enterprise managers or workers' representatives.

Printed aids and desktop publishing 

Graphics and script are among the most ancient means we have inherited to codify and depict reality.

Throughout several millennia and especially these past few centuries, technologies linked to these means of communication have evolved a great deal.
Drawing, engraving, painting, calligraphy, literature and printing have become dynamic arts which continue to foster the curiosity of crowds, the passion of artists and wealthy patrons the world over.
Each discovery of a prehistoric engraving, a cave painting, a clay plaque covered with cuneiform characters, or an archaic manuscript is an important feat for science.

Script is the first means invented by man throughout a long process of gestation to structure information and record it on a reliable medium which could be transported and weather the ravages of time.

This explains why script has been the favourite medium in education and training for centuries in most regions of the world.
Nowadays, mastering written and spoken language is at the very heart of all educational systems, even though the advantages for teaching purposes of the visual and audiovisual aids are slowly being recognized.

The technical tools and graphic arts techniques have developed at the same pace as the typical technological innovations which have emerged in one region or another of the world throughout history.
The most recent dates back to the advent of what is called word processing and electronic mail.
The typewriter, which had dominated the office ever since the turn of the last century, has recently been dethroned by the microcomputer.

Word processing 

Several software packages with the most sophisticated of features can be found on the market to carry out the necessary routine tasks, facilitating the production of written documents on one's own.
We shall list the functions offered by these softwares which allow us to produce printable documents on a computer printer.

We shall deal separately with the different tasks of entering data, setting up, filing and the output of documents.

- keying in the text 
- importing printed texts and automatic recognition of the characters of these digitized texts
- importing graphics and photographs 


- text layout 
- page setting 
- on-line access to thesauruses 
- assistance in checking spelling 
- chart creation 
- creation of automatic calculation tables 
- graphic illustrations 
- photo retouch 


filing: 
Each finished document is presented in the form of an icon and/or its name.
It may be kept in files or directories on the microcomputer's hard drive.
The document may also be copied onto a peripheral medium such as a diskette or magnetic tape.
The document may be accessed through an automatic search, looking through the list or by direct accessing of files with the mouse using the features of the diskoperating system.

- printing on a jet or laser printer (black and white or colour)
- direct telefax from the computer connected to a modem, from send off to receipt 
- electronic mail on local, telematic or telecommunications networks 


Each software is characterized by the pertinence and multiplicity of its functions, its learning speed (as close as possible to no time at all), the ease and comfort of its utilization.
These features vary greatly from one software to another.
We speak of the "ergonomics" of a software.
This ergonomics depends on the software itself, but above all on the platform's disk operating system (i.e., the kind of microcomputer) for which the software was developed.

Desktop publishing 

This family of software used to create printable written or graphic documents is classified under the heading "office automation" or "desktop publishing" (DTP) .
Their utilization in the fields of education and training helps accelerate and optimize the production of written documents , organize their filing and retrieval logically and facilitate their re-use, making it possible to draw on the full capital of documents and processes.
They can be printed, photocopied autonomously in the appropriate form, then bound and disseminated to users.
For instance, the document you are reading right now was originally written on a portable, personal microcomputer equipped with Word 5.1 for MacOS programme, and saved in the different formats currently used in the ILO before being sent to the printer who converted it into specific typefaces and page make-up.

Many organizations dedicated to training and educating workers or protecting their rights already use these methods for their mail, accounts, or administration.
They are using them more and more for editing internal or external communication aids: reports, speech es, forms, tracts, posters, publicity leaflets press articles, periodic information bulletins, books or any other aid necessary to accomplish their mission.

Yet DTP software can also be used for more specific needs in the field of training.
In fact, designing and organizing a training system can be made much easier by using these methods.
With the final document produced for a previous training programme, it is possible to re-use the document for a new training course, keeping the same structure and the same presentation and only altering the elements which need to be altered.

Scheduling arrangements can be set up on a single and coherent model, whereby all the parameters needed between several simultaneous training activities can be coordinated.
Modifications due to accidental or unforeseen changes can be taken into account in an almost automatic manner and worked into the general schedule.

Through a coherent filing system on the hard drive or any other saving aid, access to any document is almost instantaneous.
Certain sections of the contents or of the modules which were developed for training purposes may be easily re-used, or re-adapted for a new training programme, making it possible to optimize documents and functions.

Certain types of exercises structured on "calculation sheets" (or tables) can be corrected automatically, the results displayed in a tabular form, and the statistics concerning each trainee can thus be kept in a very efficient way.

Special software for supervising trainees also exist, but they are more supervision software than DTP software.

A well-designed, dear, illustrated and wellpresented written training aid is much more efficient and pleasant to use than a clumsily illustrated handout, typed on a stencil, duplicated and stapled, such as we still see from time to time.
The trainee can use it during the course, at home for homework, or file it away to keep for reference.
Unfortunately, in certain countries too often students have to settle for the good old handout and blessed are they that it still exists!

In this way, the production of the overall package of documents published for training purposes can be optimized and harmonized by using the new technologies.
The result is a better mastering of the general flow of the operations and of the external and internal image of the institution.
This coherence in the overall image - which the "graphic chart" of an institution or a training programme reflects - is very important for its returns and is not negligible in the final analysis.
From the economic perspective, even if the initial investment appears quite large (this is less and less so), the advantages obtained and the savings made often in the short term - are undeniable.

Audio features and computer-assisted presentation 

Under this heading we place all the oral presentations a trainer is called on to make in front of a fairly large group of persons present in a room, or sometimes far away (by television or telecommunications) within a training context.
This may take the form of a lesson, a talk, a presentation, a lecture.
It is an exercise for which the trainer (professional or temporary) is more or less prepared and which, to a large extent, brings into play the performing arts or of "rhetoric" (the art of eloquence and debate in ancient Greece).
Certain persons are naturally more at ease than others in this kind of exercise, but it can be learned - and with the aid of the new technologies too.

It appears that the success of a training programme has always depended largely on the nature and quality of the relationship the trainer manages to establish with the trainee.
That depends on the affinity which may exist spontaneously between the two, but above all, on the fellow feeling, emulation and respect which the trainer is likely to inspire in his or her audience.

If fellow feeling and respect are lacking, they must be compensated with self-assurance and confidence.

A difficult art 

This aspect is complex and difficult to master.

It entails mastery of oneself and of the information to be imparted but above all, perfect mastery of the relationship with the trainees.
It implies detecting needs and knowing how to deal with them, even if they are not always very manifest.
It is imperative to satisfy these needs quickly to ensure the smooth continuation of the operation and its success.

At the same time, the trainer must control the use of the documents or didactic materials he illustrates and on which he bases his lesson.
If he is poorly organized, he will not be able to find what he needs at the right time and his presentation will suffer.

That is a key point in the art of mastering this type of situation: being able to give a smooth running and presentation without any lulls.
To achieve this, a projection of flawless (not to mention striking and attractive) documents, which complement perfectly the oral presentation, and through which the audience can follow the progression step by step, is often a great asset.
Today, there are many perfectly effective technologies available to achieve this kind of result.

Audiovisual technologies offer scope for the projection of graphics and titles with an overhead projector, photographs with a slide projector, videos with a video cassette recorder and a television set or a video projector.
This is widely practised in certain institutions which have substantial funds - funds which are not within the reach of everyone.

The fact that one person has to master (even a very talented person, as trainers often are) such an arsenal while he or she expresses himself or herself in front of an audience is no easy feat.
The overhead projector on its own is difficult enough an instrument for the trainer to use properly during this kind of presentation, so imagine if we add the slide and the video.
For reasons of greater efficiency, an operator and a technical laboratory are often required.

Computer-assisted presentation 

The most practical solutions currently bring into play computer-assisted presentation or CAPre software and systems.
For this purpose, the speaker (trainer, lecturer, etc.) prepares on his microcomputer the contents of his presentation with the help of his favourite word processor, so that his work can be displayed on the screen when and if the need arises.

Next, with the help of a computer-assisted presentation software, he can retrieve the chapter headings of his oral presentation, the main points of his argument to adapt them visually to the projection.
These headings or short sentences will then be combined with all the graphic (still or moving), photographic and sometimes even videographic or sound illustrations he wishes to use and which would first have been digitized and stored on the hard drive in the form of computer files.
He will be able to import them in his application, use them in whatever order he wishes, group them, paginate them, present them in colour, and display them with all sorts of fairly sophisticated features which he can use following his own CAP software.

The trainer has the possibility of projecting all the items which he will have prepared in advance and saved in the form of a single file on his hard drive with the help of his CAP software.
The projection can be carried out in the order he planned, at the pace he wishes, in a continuous manner or one screen after another, with a simple click of the mouse or command touch of the keyboard.
All this can be achieved with his computer (preferably portable) which is connected to a liquid crystal projection screen , using an overhead projector .

The result will be even better if the room in which his presentation is to take place is equipped with a video projector specially adapted to the computer projection, which is often called a multimedia projector .
In this case, the quality of the images and the visual comfort will be excellent, and the presentation all the more pertinent and effective.

A presentation of this kind can be very easily modified to suit different needs, and can be enriched progressively, and even directly recorded from the computer onto a video cassette depending on the type of system and CAP software being used, if necessary.

In the case of video recording, one should not expect to obtain the same picture quality on the video cassette as on the microcomputer.
This is especially the case of the quality of letters and graphics which are not displayed clearly on the television, depending on the available video equipment.

Audiovisual aids and computer-assisted audiovisuals 

Ever since it was invented, film has been used to record reality and analyse it from a scientific perspective.
One might even say that it was invented for that purpose.
The very first techniques used by Marey and Muybridge were to study the flight of birds and the paces of a horse in motion and of man.

The educational vocation of cinema and then of television were developed to a large extent within the context of training.
Yet the entertaining aspects of these two media, and their flashy careers as mass media entertainment, have certainly interfered with their initial vocation.
If, pitched against the economic juggernauts which are the big motion pictures, and if, compared to television game shows, talk shows, news or soap operas, the documentary always manages to resist and keep its place, it is not by any account a very popular kind of programme, even though it enjoys a large audience.

Although its use may not be widespread among the community of teachers and trainers, the film captured and used live on a television channel, recorded on a cassette, or even extracted from the establishment's video library has, in spite of everything, found its little "niche" in the world of training.
Furthermore, it is odd that, as regards training trainers the audiovisual question is not addressed in a more systematic way, highlighting its advantages and specifying precisely the most pertinent uses to which it could be put in the context of education and training.

The video cassette 

Today, the cassette is the favourite film medium in the world of training.
Its easiness to use, duplicate, file, and distribute makes it a choice medium.
However, it is necessary to underline a few important points.
Each time a cassette is run, wear and loss of quality result, even if slight.
Copying a VHS cassette onto another VHS cassette results in a significant deterioration of picture colour and in increased "noise", that is, interference.
Furthermore, the picture and sound are recorded in a magnetic form on the tape.
This tape becomes demagnetized with time, so that the items recorded on a cassette do not enjoy a long life-span.

The use of the video cassette within the context of training is carried out by viewing the entire document recorded, or by viewing one or several excerpts which the trainer will need to replay several times.
To achieve this, the video cassette needs to be rewound each time which requires a certain amount of time.
In addition, the systems used for finding the beginning or end of the sequence are not always very accurate.
All of this interferes with the smooth running of the trainer's presentation.
It is said that the video cassette does not interact very well .
The video cassette recorder does not provide a readymade answer.

Different types of uses for the video can readily be imagined.
The video is an ideal means for portraying the reality of the functioning or of the feature of a machine, of a system or of a particular context.
We shall not dwell on film used for educational purposes, for the cause served is obvious.
Because the length can vary, it may be used to present only specific blocks of knowledge.
The contents may be reported on by a commentator - present (or absent) in the picture - and illustrated by as many still or moving graphics, as many video sequences, commentaries, sound effects and music titles or subtitles as necessary.

As seen earlier, an application developed on a CAPre software can also be recorded on a video cassette.

The sound corresponding to the speaker's oral presentation can be recorded on a video cassette recorder and added to the picture.

More generally, the video cassette enables us to view directly on a screen or to record the sequence of actions or presentations carried out within the context of training.
This allows the trainer, for example, to comment on and correct the way the trainees do their practical exercises.

The trainer's presentation in front of his or her group of trainees can also be shot, projected on a monitor (or a big screen), or pre-recorded, if this meets a particular need.
The result of a shot of this kind can seldom be used as such, unless sophisticated technical tools are available (several cameras, editing equipment) allowing for direct (or on-line) editing .
If not, pre- recorded (or off-line) editing of the different relevant elements and illustrations is done after the recording takes place, in an editing studio.
Then the film is duplicated and distributed.

Educational television 

With still more sophisticated means a programme of this kind can be broadcast to other countries on a television channel .
But these methods are costly and difficult to implement: several specialized technicians, video material, sound equipment, special lighting, specially equipped studios, hertz television broadcasting network, etc, are all needed.
When all is counted the bill is heavy.
Furthermore, this type of programme - especially if it involves a taped course - is not appreciated by everyone.
General-interest television channel programmers tend to schedule the programmes during late evening, night, or early morning.

But there are television channels whose objective is in fact to broadcast educational programmes.
However, even in this case, the taped course strictly speaking is not very popular.
In fact, with a fair mastery of audiovisual language and a little creativity, it is possible to present didactic material in a much more attractive manner than in the form of a televised course.
This can be a documentary, a short module, a video clip or an interview illustrated with specific sequences.
These are the types of documents which are run on educational television channels during prime time.
Courses as such are run at night-time, when they can be taped at home on cassettes by interested persons who can then use them in their own way depending on their motivation as trainee or trainer.

These televised educational programmes can be broadcast on the classic hertz television network, but this has been saturated for a long time now in industrialized countries.
Today we can find educational television programmes on cable or hertz television broadcasting networks.
Cabled networks (on coaxial television cable) are usually found in densely populated areas and can be subscribed to.
They offer dozens of simultaneous programmes, some of which may have an educational bent.
Satellite networks allow general-interest programmes to be broadcast in certain regions of the world, but a certain number of them run educational programmes.
To pick up these, a suitable satellite dish is needed.

What computerization brings to the video: virtual editing 

Obviously, computer technologies have invaded the entire area of video production.
What before was possible only with very expensive video equipment is now possible using a specific multimedia microcomputerized system which is considerably cheaper, especially for what is known as virtual editing of video films of the right quality and suitable for use in the field of training.

The necessary video, graphic, photographic and audio documents can thus be obtained in the traditional way on their original aids, whether or not they have been specially produced for specific needs or located through documentary search and acquired.
The useful items (or parts) of these documents will be examined, clearly marked and listed to produce the final version of the film.

The microcomputerized system used in virtual editing then allows the user to digitize all of these items in order to record them on the very powerful hard disk as so many specific computer files.
Then begins the virtual editing.
Using an editing software , it then becomes possible to assemble successively all these items in the desired order by adjusting their length, adding all the necessary transition effects, the titles and sub-titles, and the graphics of still or moving illustrations.
Commentaries, interviews, sound effects or additional music can also be mixed, their length can be modified, they may be slowed down, accelerated, their volume can be increased or decreased, etc.
All the ideas can be tested, kept or deleted as easily as the features and the ergonomics of the editing platform and software afford.

Once the film has been "virtually" edited it will be put together in the form of a single file which can be displayed on a microcomputer by using a helpful little reading programme.
All that remains to be done then is to record it on a video cassette using a video cassette recorder.
It is possible to make as many copies as necessary, on any video cassette format, without damaging the original file.
The advantage of this method lies in the fact that it is possible to intervene at any time to modify one or several items - e.g. to edit a new version without actually having to do over the editing from beginning to end as is the case with traditional video.

Once it is duplicated on a VHS cassette, the film can be broadcast on the chosen network.

This film can just as well be recorded, alone or accompanied by all sorts of other elements, on any computer-type medium corresponding to a market or a particular type of utilization such as CD-ROM, CD-i or CD-Video media.
This type of medium has many advantages as compared with the video cassette.
There is no contact between the reading head and the medium, thus no wear.
It is a lot more compact, has a longer life-span and enjoys excellent interactivity.
Access of one sequence to another is virtually instantaneous.

Yet we can also well imagine the computer file of our film being transferred onto the hard disk of a "server" computer and being available on-line on local or international networks; available free of charge on Internet's World Wide Web or for a certain sum on a service dedicated to vocational training on a rapid telecommunications network such as Integrated Digital Networks; or even on the future information highways on an interactive educational television server.

Separate multimedia aids or integrated and interactive multimedia 

Traditionally, the acquisition of knowledge and know-how implied the use of a number of different documents or materials.
Finding these documents can be a difficult task, so that the trainer usually provides the trainee with the references for these documents.
Yet the trainer might be inclined to furnish the trainee with the documents themselves to facilitate his access to them and to increase his chances of success.

Ever so often documents do not even exist: the trainer may first have to produce them himself.
The natural extension of this procedure tends to make the trainer produce a group of documents which will facilitate the trainee's quest for knowledge, a bit like an integrated method.
Since several types of media are necessary, the practice of presenting a group of documents designed with a specific goal in mind in the form of "multimedia kits" has become widespread.

Multimedia kits 

So the trainer and the trainee have a coherent group of documents adapted to their needs.
The ILO's International Training Centre in Turin has edited several kits of this type.
Multimedia kits are very popular, especially for teaching languages.
All the documents necessary for a training programme: leaflets, booklets, brochures, slides, transparencies, audio or video cassettes, diskettes, CD-ROM are contained in a little box or kit.
Often used in distance learning, the multimedia kit allows the trainee to progress at his own pace in acquiring knowledge, and especially to possess in a compact form the main documents he needs to do practical exercises and achieve the set objectives.
If the kit is well designed, certain sections of the documents may make reference to others, referring the trainee to tests, functioning somewhat like a hypertext manual.

Multimedia applications and the CD-ROM 

Today, on the different microcomputer platforms, can be found several software packages which can be called multimedia applications design software (sometimes called designer systems) which allow a group of already digitized documents of diverse origins, or created using other software somewhat like the CAP software we have described, to be integrated into a single application.
Each enjoys its own specific features and an approach to this type of editing which is peculiar to it, and these packages offer screens which can contain texts, still or moving graphics, photos, sound and video.

A scenario defining the contents and the path options will be designed at the start of the project in order to introduce and run through the application from one screen to another, according to an established logic.

The user can use the mouse to click on buttons in order to access the information he selects.
These multimedia applications are characterized mainly by the fact that they are simple to use and by the quality or timelines of their contents.
All sorts are available today, published on diskettes, CD-ROM, or on-line on national or international telematic servers.
They may be classified according to four functions: to entertain, inform, train and to promote.

There is an increasing practice for book publishers to attach a diskette or a CD-ROM to their publications.
These diskettes recapitulate a part of or all of the contents more or less in the form of a multimedia application.
It is one way of offering the reader another path of access to the contents.
It is a means of allowing an interactive approach to the data on the microcomputer, and the possibility of having the text and pictures for personal use or within the framework of school or university studies.

Publishers of specialized magazines specializing in data processing (and they have mushroomed over the last few years) offer virtually everything by supplying diskettes and CDROMs which recapitulate all or a part of their "paper" contents and giving their readers different extra advantages, for instance the possibility of testing the new versions of the commercial software packages - or the new products - presented in the magazine, or tutor applications for these software; even the possibility of copying software and documents of all kinds: texts, pictures, sounds, game or educational applications proposed by their designers for testing, either as freewares or against a small sum if the reader uses them (sharewares).

As said before, one of the favourite methods of multimedia applications distribution on an off-line material medium is the CD-ROM.
It has a capacity of around 600 megabytes.
It is a derivative of the CD-Audio which was invented more than ten years ago.
It was first used to publish statistical or textual data (legal, medical, economic data banks, etc.).
Then it harnessed the technological developments of these past years for filing and delivering all types of data.
However, the video which needs huge flows of data must be compressed and the size of the window in which it appears is smaller.

Several other compact disk standards, offshoots of the CD-ROM, such as the CD-i, the CD-Video, have appeared on the market.
The first, which was designed for the general public, allowed the user to win his freedom from the computer; it can be read through a specialized case which is connected to an ordinary television set.
It is designed like a platform for reading entertainment-educational interactive applications meant for the general public.
The second is an off-shoot of the CD-i and offers the possibility to publish approximately one hour of compressed video on the screen, but with less interactivity.
It is designed as a medium for editing films on several disks.

Today, it is possible to cut a multimedia application on a CD-ROM using the microcomputer on which it was created.
It can then also be "transposed" onto the different platforms available on the market, then duplicated and distributed.
This technology evolves very quickly: new materials, new techniques for recording much larger quantities of data are about to make their appearance.
They will be used to publish increasingly sophisticated and more complete multimedia applications.

Publishers of traditional teaching materials have lost no time in filling this niche, and many products are coming on the market.
Products for teaching languages or sciences (physics, mathematics, biology, zoology, medicine, etc.) as well as encyclopedias are available.

The sector for publishing works of art and museography is quite dynamic.
Several products of this kind are already being sold by tens of thousands.

The CD-ROM holds several attractive assets for publishers.
Apart from the cost of setting up the contents, it is very economical to produce and distribute.
The duplication and distribution chain of the CD-Audio is very highperformant and is applied in exactly the same manner as the CD-ROM.
It is possible to accommodate several language versions of the same product on one medium even if the change from one culture to another means that a product of this kind needs to be used differently.
Moreover, the number of readers increases significantly every year, and everybody likes this slender, light, practical, shining wafer with its rainbow colours: it is in fashion.

Multimedia applications are also developed by specialized offices to cater to the specific needs of enterprises or institutions.
These training, presentation, promotion, or external communications applications are frequently published on CD-ROM in limited series.
Certain enterprises or institutions are themselves equipped to edit their own multimedia applications internally.
They generally produce the master copy which is entrusted to a printing shop for mass production.

Organizing documents and data 

Documents are usually indexed, arranged or filed in libraries in such a way that they can be easily retrieved.
For a few years now, the notions of reference libraries with multimedia facilities and documentation centres have been emerging.
The multimedia reference library extends this function to all media and the documentation centre extends its coverage even more in the attempt to cater to the needs of the user.
These structures are grouped in the material form of documents carrying information and knowledge, organizing them in such a way as to facilitate their access.
Computerization has long made its appearance in this field as a tool for managing documents and document research.
Indexing, file, thesaurus: these are words which have shifted to computer language.

When documents are numbered, these words alter a little in meaning.
A document becomes a file (numbered), it can be indexed as a document according to a list of words and concepts (thesaurus), but each important element can also be indexed.
Computerized tools thus allow for the recording and organization of these data so that each useful unit can be automatically and rapidly retrieved.
These tools are actually programmes called databases.
ALL sorts of research operations can be practised on them.
In this way, it is possible, for example, to retrieve each location of one word, a group of words, or a group of words excluding another word, in a text or in a group of texts.
The still or moving pictures can also be described by a documentalist using the same thesaurus, but soon they will be automatic.

Databases are created and stored on computer networks; they can be published on mass memories such as CD-ROMs and can be accessible on-line using a microcomputer located far away through the appropriate telematics methods.
Direct connection through a modem is the most elementary method, an extension of the principle being the Minitel standard and system which developed considerably in France during the 1980s.
Internet and all its derived systems and services are much more powerful and economical.
This issue will be discussed further.

Structuring knowledge 

In most contexts, the trainee's work sometimes also involves this unseen process which consists of organizing in his memory knowledge, behaviour and acquired technical gestures so as to find them spontaneously in order to apply them.
This organizational representation of ideas is woven throughout the learning period, establishing links which branch off from one unit of information into another or several other items already listed.
This task, which can be done unconsciously (or consciously), helps to pin down the data in a more or less efficient way.

In certain learning situations, once the study habit is acquired, the trainee sometimes gives concrete expression to his personal effort in the form of coloured cards which select and organize the acquired knowledge into flow-charts with systems of notes and specific cross-references.
The outcome is sets of complex documents which are often quite attractive depending on the individual's creativity.
The designer is sometimes the only user able to find his or her way in this maze.

Based on this principle, publishers also propose handouts to help students revise for their examinations.

In our everyday lives we naturally practise this mental activity which consists of classifying facts and ideas as they arise in our individual consciousness.

Reading is perhaps the exercise which best enables us to observe this phenomenon: as the reader's attention peruses the lines, ideas flow and link up together in relation to the patterns already acquired.
By the very way it is ordered, the written form helps the reader to accomplish this task.
Charts are welcome in this case, for they synthesize the data and knowledge in a compact and already structured form.
Most people find memorizing charts and diagrams more immediate and comprehensive than pages and pages of text.

Books, hypertext and hypermedia 

The book is the fruit of a tradition, handed down as the product of an age-old process.
It is put together in a certain way, obeying a system of ordering of its contents which is no longer noticed, so much has it become a convention: a set of references which help us to steer a course through the information, to find quickly what we are searching for and grasp the contents.
We refer to all those parts that are not the text proper: front and back covers, contents, index, footnotes or end notes, illustrations, etc. 

A book is rarely read linearly - we often go back and forth, especially when we are trying to learn.
For any individual, reading something is a unique adventure.

One person might begin by looking for words in the index so that he can read the corresponding passages.
Another will look for the library references.
One will examine the contents and another will flip through the text in no particular order.

Each sentence, each idea, each word written by the author corresponds to what the reader understands by it in terms of his earlier knowledge.
This understanding and the amount he memorizes and remembers depend on the reader's setting and experience, on the links he can establish with certain elements of his former experience.

He seeks and finds a place to represent and locate his new acquisitions within the general structure of his knowledge.

Taking this reflection on the manner in which ideas are constructed and are structured as their point of departure, researchers introduced the concept of the hypertext .
They had been working on computerized solutions which would make it possible to link up in a more or less automatic manner all the places a word was used in a group of texts, or all the bits of knowledge which were related.
Their dream was to establish links among all areas of mankind's knowledge.

The concept of hypermedia is derived from hypertext, but its scope embraces all media.
It is, as it were, a multimedia hypertext.

Today there are many types of software which make it possible to create hypermedia type applications on different platforms, with more or less sophisticated features.
Certain multimedia applications function on the model of hypermedia.
Certain words appearing in a text, certain icons, certain pictures can be presented in such a way as to make the user understand that he or she can click above to access the contents or a related document.

This is the case of pages developed on Internet's World Wide Web servers, which we shall discuss further on.

Other types of software which function more on the model of a documentary database management system allow the user automatically to establish links between all the places a particular word is used in the body of a given text, to make them appear in bold letters or underlined in order-to indicate to the user that he can thus access them elsewhere.
This is the case of certain multimedia encyclopedias published on CD-ROM.

The advantage of this kind of software is twofold: on the one hand, an author can publish a type of multimedia application offering supplementary access features, thus enriching research or consultation and making it easier.
On the other hand, from a teaching point of view, such software, when it can be easily learned (like Hypercard or Toolbook), allows the trainee to formalize the structure of the knowledge he acquires himself in the form of a hypermedia - a particularly attractive form of creation which can be critically reviewed by the trainer or fellow-trainees.

Certain training programmes may give rise to group work and take shape in such a way as to result in applications designed and produced by teams.
They may give rise to exchanges, or may be used in the form of "shareware" or published.

Team work and local networks 

Group work holds certain advantages, both for training and production.
Interaction and exchanges between individuals contribute to better productivity, quality of production and better skills for workers.
Collaboration makes participants active, makes them pool their ideas and the fruits of their labour, and helps them to be critical and enrich their ideas if the context is favourable.

To achieve this end, communication must flow easily among the persons who must collaborate.

In classic work structures, work stations are often separated by partitions or are distant one from the other.
And it is no easy task trying to knock down the walls or bridge the distance between individuals as long as these obstacles are necessary: the individual needs autonomy and calm to concentrate on his work; but in contradiction he also needs to consult other team members in order to adapt, update and synchronize his work.
Supervisors also need to overview work in progress.

Any attempt at work organization means striking a balance between these different constraints in order to reach an optimal compromise acceptable to everyone.
This equilibrium point characterizes the enterprise culture of a structure.

Certain structures adopt the open space layout, others separate offices.
All the possible variants are practised.
The flow of information in work structures respects very strict protocols and closed circuits.

The traditional methods for circulating information are either direct, verbal, through voluntary gatherings (talks, meetings, seminars, training courses, commemorative events); or through accidental meetings in the workplace (corridors, lifts, canteens) or outside the workplace.

During telephone conversations between two or more persons, the speakers immediately exchange information without being in each other's presence.

Information can also be exchanged in writing : circulars, memos, bulletins, letters, minutes, reports, studies, etc. Each mode has a connotation of its own and corresponds to a specific communication need.
It helps perpetuate the established order which guarantees the endurance of the structure in its form and culture.

Networks: a new method of working 

As seen, introducing new technologies offers scope for increased productivity at the expense of an upheaval in production methods and the nature of work itself.
In a work situation as in a teaching situation, introducing new communication technologies makes it possible to optimize information processing and circulation in terms of all the desirable parameters.

Locally networking computerized jobs allows more high-performance communication procedures to be set up.
Each individual can have ready access to information which is available through the structure.

With an electronic mail software, he can exchange information with his partners.
This access may be free or limited.
The advantage of this type of bulletin is that it is instant and non-intrusive.
The addressee can read it and reply if he is there and if he so desires.
It is a written bulletin which allows for rapid exchanges which can be saved and filed.

Yet exchanges much more meaningful than written messages are possible. 
Collective work software allow the sender to show his partners the work he is currently doing: text, graphics and pictures.
They offer the possibility to share a work environment on the screen with a specific group of collaborators who can intervene to make suggestions or adjustments in actual or deferred time.

Some of these collective work software applications make it possible to accommodate the visioconference to enable each of the speakers to hear and see on the screen, in a window, the video image of the person speaking.
These working methods are independent of the situation of the collaborators on a local network such as LAN (Local Area Network), or enlarged such as WAN (Wide Area Network).

Further on, we shall see that this concept has been extended to connections on the telecommunications networks.

The same features are possible in a training environment.
Furthermore, work procedures can be alternated with or even combined with the training sequences on the same computerized work station.

Organizing on-the job training in the enterprise as a routine activity of the work station, or in a specially equipped room, is a practice which is growing rapidly.

Some call this the learning enterprise.

Networks: new ways of self-training 

In certain educational and training institutions, techniques of this kind are being used.
The trainer can see each trainee's screen and intervene while they are working.
The trainees may be inclined to collaborate with one another, exchange messages, or quite the opposite - focus on their own screen to work alone.

In this way, the trainer can supervise or guide each of the trainee's activities from his desk.
He can send a message to one of them, give instructions to another, observe how another is working in the database, and pilot him.
He can set exercises or tests and have them corrected immediately, enter the results and have each student's marks to visualize the progress being made.
In this way, it is possible to adapt training advancement to the pace of each trainee.

This kind of network can be made up of practically anything, using the necessary elements.

Depending on the chosen platform, the work involved in constructing them will differ.
In certain cases, it is like a construction game.
The material and the programmes will be set up in a quasi-automatic way following the well-known "plug and play" logic.
Different programmes such as network management software, communication software, database or hypermedia software, can help define the work environment and develop the contents.

This is within the reach of any training instructor who can easily master the applications on this kind of platform.
Furthermore, once he gets used to it, the trainer can on his own gradually adapt the technique to suit his needs or the needs of his trainees and, moreover, make it more advanced.
It will be possible to use didactic materials developed using the same tools.

Similarly, he will be able to publish and distribute didactic contents, depending on their relevance.

On less integrated design platforms, only a computer scientist can programme the entire application.

This is a long, difficult and slow process which is burdensome for the instructor.
However, on these types of platforms can be found some integrated applications with fairly usable parameters and interesting features, and on which contents can be developed, but which are difficult to adapt to the context and to the variety of contents available elsewhere.

Telecommunications networks and sub-networks 

Telecommunications networks were developed at the end of the last century and ever since they have untiringly been weaving their spider's web around the world.
In their grip, the size of the earth is gradually shrinking.
Speech and script flash across space at lightning speed in the form of binary numbers.
Only the poorest countries, countries which are not equipped or uninhabited regions still remain almost safe from this onslaught.

Almost yes, because geostationary telecommunications satellites cover virtually the four corners of the earth, and a person equipped with the necessary material (which fits in a kit) can telephone, fax or be connected to any network, including Minitel and Internet, with his portable computer, from the middle of the Atlantic or the Gobi desert.

Within the span of one century, telecommunications have changed the face of the earth, or rather the image man had made of it.
By the same token, it is man's life which has changed, for he lives within the image he has carved out of reality.
There have been increased exchanges of information and goods within frontiers, but also on a global scale.
Those economies which use them have become very dynamic.

Telecommunications systems have long been considered by the majority of countries as strategic infrastructures which States had to monitor closely in order to provide a public service and protect it against potential enemies.

The public service and telematics 

In the past, States have not always proved to be very innovative administrators in the telecommunications field.
Under their exclusive supervision, telecommunications systems have sought security rather than performance, but quality and price have remained secondary.
Some States such as France and Germany have realized the stakes involved and have finally given these administrations the status of public enterprises - a move intended to improve the service, but they still retain the monopoly in the area of telecommunications.
These public enterprises have expanded and modernized their networks, invested in research and made a considerable effort to satisfy their clientele.

Significant breakthroughs have even been made in the field of digitized networks (Integrated Services Digital Networks: ISDN) or telematics.
Minitel is a famous example in France.
Based on the principle of personal initiative, it has enabled a private sector of online telematics services to develop.
It became very popular in France, but it did not take roots anywhere else on a lasting basis.
Canada, Germany, the United Kingdom and other countries have also developed their own standards, but neither have they been successful in exporting them.
It is difficult to tell whether or not the market will adopt a standard, especially if it is unilaterally defined with the intention to impose it far and wide.

This type of telematics services based essentially on texts, rudimentary graphics and reduced features will probably not survive beyond the year 2000, even if the standard were improved to distribute photographs.
It is a costly solution which offers no openings and one which will never work anywhere except in France.
Especially when, in the rest of the world, Internet is in the process of linking up all microcomputers in a free global network of networks, at no charge and much more powerful.

Feeding in the thousands of Minitel telematics services on Internet might be one solution.
But the high prices, slowness and low resolution which come with this standard will restrict the use of these services, even if they gain access to a larger market.
Despite all these problems, certain services may hold great interest for an international clientele.

Deregulation - a stake in the future? 

In free-enterprise countries such as the United States, telecommunications belong to the private sector.

The Bell Company unquestionably dominated the market for a long time.
During the 1980s it was dismantled into several companies by the anti-trust law.
These telecommunications enterprises have in their turn become international giants.
Competition among enterprises worked in favour of improving the quality and prices of services.
For a few years now, certain States with entirely different traditions have undertaken to privatize their administrations or their telecommunications public service operators completely in order to enable them to respond better to the reality of the market in terms of technology and services.
This was particularly the case of the United Kingdom and Japan.

Today, despite some resistance, liberalizing telecommunications is an item on the agenda.
For example, within the European Union, deregulation of telecommunications is carded for 1998.
Even if deregulated enterprises remain giants with authoritarian tendencies, they will be obliged to cope with competition and adjust to the market.
However, it is likely that competitors will find it difficult to create their own networks in the immediate future.

The telecommunications/ computerization family tree 

First came the telegraph which started transmitting coded messages between distant points through manual sequences of optical signals.
Then followed electricity, when current variation sequences on electrical lines began to code messages.
Towards the end of the last century, the telephone began to transmit sound in the form of an analog signal.
The first experiments involving the transmission of images date back to the same period.
But it was not until the beginning of the twentieth century that the first machine which would transmit written messages came on the scene: the Belinograph, which is the forefather of the facsimile (fax), which has only been widely used since the 1980s.

In the meantime, the telex system took over, operating on the principle of a combined telephone-typewriter, and enjoyed considerable pride of place until the fax, updated and perfected by the microcomputer, rendered it obsolete.

It is worth noting that ever since the telephone was invented, observers and teachers saw it as a wonderful means to develop education.
Today, it is a known fact that this is not the sector where the telephone most flourished, even if in distance learning it is sometimes still used in certain contexts periodically to maintain contact between trainee and trainer, tutor or administration.

Copper cable networks made a gradual appearance, and manually operated switchboard systems were automated.

In most countries, the telephone became a vital means of communication which made significant contributions to economic and human development.
In the least developed countries, programmes geared at developing or modernizing telecommunications infrastructures are usually among the main priorities.

Huge computers used to be operated from a distant work station to which they were connected.
The persons who used to work on this computer did so from distant posts and they were already connected to a network.
In the 1960s, telecommunications networks started being used to exchange data between distant computers through modems (modulator-demodulator) which, at the transmitting end, transform digitized data in variations of the electronic signal on the telephone line, and inversely at the receiving end.

The idea of merging telecommunication and computers in order to develop on-line services was launched and telematics was born.
Even before microcomputerization was developed, telecommunications companies designed suitable platforms and standards to create these services.
Minitel, which we have already discussed, is one of them.
Since it was launched an entire range of new customs and services in society has been developed.
Today, there are thousands of telematics services in France, for example.
They range from the electronic directory to "sex chatlines" and games, stock-exchange results, travel bookings and a multitude of others.
Some were very successful and then disappeared.
Others have taken many years to make a breakthrough, such as company telematics services.

Telematics are being widely used by enterprises these days for commercial or promotional purposes.
Some companies send their sales catalogues by correspondence on the telematics network.
Purchasers can select and pay by inserting their credit card into the terminal.
A particular use of telematics which has really spread throughout the world in a most spectacular way is automated banking transactions.
In fact, nowadays, an individual can carry out a series of transactions on his account with his bank card from any automatic teller machine in the world on its network, 24 hours a day and 7 days a week.

Yet it should also be noted in this case that the hopes which had been set on telematics as a revolutionary training and educational method remain unfulfilled to this day.
Of all the teaching applications which were developed in France on telematics aids there are very few which actually made a breakthrough.
This is undoubtedly due, on the one hand, to the nature and the quality of the applications which have been developed, but also to the medium itself which is based on too rudimentary a display of the text and graphics in terms of resolution, as well as to the slowness and poor interactivity of this standard.
Even if it does attain a quality which would allow photograph display, it seems as though closed standards have reached the end of their line.

Developments in telematics 

Meanwhile, microcomputers were coming into their own with platforms we know today slowly making a breakthrough on the market around user-friendly disk operating systems.
Application programmes were published one after the other in all areas of information processing and communications.
Simultaneously, the communication capacity of telephone networks were growing with new, more powerful modems, while Minitel was hardly making any headway at all.

Programmes emulating the Minitel standard are available on all microcomputer platforms which are connected to the telephone network through a modem.

Access can be had to any telematics service created along the lines of the Minitel standard.
The microcomputer is somewhat like a much more sophisticated, user-friendly and practical Minitel terminal.
Furthermore, data consulted on telematics services can be saved and re-used in other applications.

There are also other private networks for microcomputers which are not modelled after the Minitel standard.
These are telematics servers which the user must subscribe to in order to have an address through which he can communicate with all the network members, participate in forums to exchange information, download programmes or access Internet.

The microcomputer has become a universal tool of communication, on-line on net works, or off-line.

As an off-line tool, we briefly discussed its multiple resources for publishing media.
On-line on LAN, telephone, or cable TV networks equipped with suitable software, it can be directly connected to any accessible stand-alone computer (i.e. itself connected to a network and functioning).
The two microcomputers can then exchange any file: a fax, an electronic message, graphics, photos, sound or video.
They can also exchange video and sound files in actual time to ensure a visiophonic conversation.

It can also be connected in this way long distance on any local computer network (LAN or WAN).
The connection programmes, operating software and their extensions which control the exchange of data through communication standards (like H320 or H261) make it possible to communicate among different platforms far away.
They even allow a videoconference among different platforms on heterogeneous networks (bringing together different platforms) and on long-distance platforms on the classic telephone network (ISDN).

The Internet phenomenon 

Today, it is a proven fact that the microcomputer is considered as a universal tool of communication with the global expansion of Internet.
Any microcomputer equipped with suitable software and linked up by the telephone network to a server (the local network itself connected to Internet) can gain access to the store of resources made available on Internet by all the institutions and persons who wish to make them available and freely decide to do so.
Inversely, any microcomputer connected to a network can itself become an Internet server by following certain technical procedures.

Visiophone and videoconference are possible on Internet using software such as the one used at Cornell University and CU-SeeMe, which are available and which can be loaded free of charge.
The sound of the conversation is satisfactory, the picture is brightened only at the pace afforded by the delivery (around one picture every four seconds).
All these connections on Internet are made for the price of telephone conversations between the users and their servers (in addition to the rental charge paid to the server), regardless of the distance separating the servers.

Internet is a network which was designed about twenty years ago in the United States for purposes of military scientific research.
The aim was to connect heterogeneous platform networks functioning under the Unix operating system to enable communication between them.
That is why it is called a network of networks .
Internet is defined by the TCP/IP (Transmission Control Protocol/Internet Protocol) protocol.

Each computer on Internet has a specific address which allows it to be contacted by all the other members of connected networks.
In this way, it is possible to exchange electronic messages, participate in information forums (Newsgroups), send mail, import files with Telenet and FTP (File Transfer Protocol) or explore the network with research systems such as Archie, the Gopher, WAIS or the famous World Wide Web.
At the outset it was created by scientific researchers for their own needs, then students, and later managers of enterprises took them over.
It belongs to no one and its possibilities for development are untold.

Through its concept of freedom, free use and its different research functions which allow for increasingly user-friendly access, Internet is in the process of foreshadowing the global information exchange network of the future.
World Wide Web (created by the European Organization for Nuclear Research (CERN) in Geneva) allows for hypermedia type research on all the information available on Internet by simply clicking on words or icons, especially with navigation software such as Mosaic or Netscape.

Information highways 

The physical networks which support Internet are heterogeneous (different telephone networks, LAN, WAN).

Some are fast, others slow, but as a whole they function well.
Improving the physical support would help develop Internet, but would not replace it.
Internet is an immaterial network based on a logic, one of disseminating information.
The different projects called "information highways" are projects for installing physical networks which set out to connect the majority of rich countries on an extremely high output communications network.
This would allow television viewers to receive up to 500 channels of interactive television in their homes.
The Internet model is different from the one whereby information circulates only in one direction, where the user is passive and surfs on the network, zapping from one service to another.
The logic of Internet supposes that the user is searching for a precise bit of information, or that he or she has something to say.

The G7 Summit of 26 February 1995 in Brussels recommended eight key principles for developing these networks.
- promoting dynamic competition 
- encouraging private investment 
- defining an adaptable regulatory framework
- interoperability and interconnection of networks 
- open access to services for future highways
- equality of access to citizens 
- promoting diversity of contents (including cultural and linguistic diversity)
- recognizing world-wide cooperation with particular attention to developing countries 


- education 
- training 
- libraries 
- electronic museums and galleries 
- environment and natural disaster management 
- health 
- governmental and administrative information management 
- PME-PMI commercial information 


For the time being, the different information highways projects with optical fibre bases which are launched in the United States, Europe or Japan rather openly advertise their trade worries.
The stated goal is to increase the output of networks so as to transport greater quantities of information faster to enable the transmission of interactive video programmes in actual time in each home.

The currently existing telecommunications networks are not yet being exploited to their maximum capacity.
This is the case, for example, in Europe, where there are very high-performance digitized networks, a part of which are in optical fibres.
They are sometimes underutilized by a large percentage of their potential users who refuse to have anything to do with them because of the costs which they feel are too high.
Furthermore, copper networks will still be functioning for many years to come, and this all the more since the ADSL (Asymmetrical Digital Subscriber Loop) technology, which is currently being tested, allows one-way transit of 8 megabytes per second on the current telephone.
As such, two television programmes can be received simultaneously in one's home when engaging in a telephone conversation and connected to Internet at the same time.
Other unforeseen uses will no doubt emerge in due course.

Internet and training 

According to a study carried out by the Computer Technology Research Corporation (USA) in 1993, 47 per cent of the computers connected to Internet were for educational purposes.
In fact, it is an ideal tool for documentary research for students and a tool of communication for researchers.
Yet from now on it seems that Internet will also be called on to become a privileged media for distance learning and training.

Several universities or public or private training institutes do offer teaching programmes on Internet's World Wide Web, which is the most "navigable" network.

Some private universities of good standing advertise that theirs is a global campus and offer courses leading to official qualifications: diplomas, certificates, degrees, Master's degrees, doctorates in several fields.

Some universities which have long been specialized in distance learning, such as the Télé Université of Quebec, Canada, or the Open University in England, now use Internet and World Wide Web simultaneously with other media.
Its wide range of options, universality and freedom of access, its global hypermedia character, the ergonomical nature of its navigation software and its low cost (different according to the economic or administrative context, region or country) make World Wide Web a choice medium for lifelong vocational training and formal education.

The main drawback of Internet is, paradoxically, one of its major advantages - its freedom.
Anyone can create a service.
In France, when Minitel came on the scene, despite the tight control, the administration could not prevent the development of sex lines, for example.
In theory, anything is possible on Internet and there are some materials which may be harmful to children.
It is a major problem and a solution must be found in order to give education a chance.
Yet the problem is not fundamentally different from the one posed when youngsters gain access to specialized documents used by adults such as pornographic magazines, or certain radio or television programmes.

Another problem which often arises is the abundance of information on Internet.
Just when is there too much information?
This is a matter which falls within the area of research and information management.

One can spend many tiring hours before actually finding the right answer to the question, but it is certain that the chances of finding it are more likely on Internet.

Furthermore, services or relatively automated research assistance software on Internet (called intelligent agents) are already on the market.
In any case, there is no question that networking does make it possible to share and better disseminate knowledge and to ensure that organizations are steeped in it.

The technical tools 

The description of the technological system we have entered can be illustrated in the following manner.

Figure 5

The media system

We shall now look at the systems and material networks for information processing and management .
This level brings together all the material elements, but also the software which are their partners.
This concerns everything that goes on in a computer, as well as the input and output with their peripheral elements.
As we discussed in the previous chapter and illustrated in figure 5, the computer is at the heart of information processing and management systems.

Brief history of the microcomputer 

Today it is almost a legend that microcomputers were invented in a parents' garage by some clever students fresh out of university, towards the end of the 1970s.
Yet this was really the case of Steve Job and Steve Wozniack, founders of the Apple company (USA), who were among the first to produce and put a microcomputer on the market, and whose slogan, revolutionary in those times, was "a computer for everyone".

At that time, computers were huge monsters which cost a fortune and only big companies or organizations could afford them.
Only very highly paid computer scientists could use them.
But IBM (USA), which had virtually the monopoly of the market for these huge machines, designed in its turn at the beginning of the 1980s a microcomputer model called IBM PC (PC as in Personal Computer).
This was carried out with the help of several partners and was inspired by the concept of a computer for everyone.
This machine was to enjoy a special fate.

So the microcomputer was first designed as a personal machine which would fit in a corner of the user's desk.
It was supposed to be a machine on the human scale.
This ran counter to the huge centralized computer to which users were connected through a screen and a keyboard.
Within what turned out to be a merciless logic, the user in this way became more autonomous and can now use as he pleases the full processing power of his machine, within the limits of the features or the performance of his system and his own skills.

Microcomputers linked on a network should not be confused with the network of terminals connected to a huge computer.
If we wanted to evaluate the performance of such a network, in the first case the power is a multiple of the number of connected machines; in the second case, the power of the central computer is divided by the number of terminals.
To simplify matters, one might say that these two models are combined today within local networks on the Clients/ Servers principle which we shall deal with further on.

Enter clones 

The competition among all the standards which spontaneously emerged at the beginning was very stiff.

The advent of the IBM PC surprisingly helped to shed some light on the situation.
As this product was not well protected from the legal point of view, many manufacturers copied it and it became an ordinary standard.
They began manufacturing what is called clones, i.e., IBM standard compatible microcomputers.
The computer giant did, in fact, try to prosecute the copiers, but soon gave up.
The Intel company which supplied 8086 microprocessors (and those that followed), and the still fledgling Microsoft company which supplied the MS DOS operating system, were free to sell their chips or their operating system to IBM or anyone else - they were not bound by contract to any exclusive partner.

Apple, IBM and other market operators of the time such as Atari or Commodore, tried to set up resistance by proposing innovations in terms of power, dialogue features or the operating system.
This led to their securing a share of the market which in turn allowed them to develop in a parallel fashion, with varying fortunes.

In this way, Apple Computers safely resisted the proliferation of IBM compatible PCs by proposing its famous Macintosh, whose graphic interface was leagues ahead and was used as a model for the other platforms.

Meanwhile IBM was trying to thwart the copiers by launching a new standard: the Personal System (PS) with the OS/2 operating system which was much more user-friendly than MS DOS.
But nothing could stop the development of the MS DOS/IBM PC compatible platform.
All the more so since the Microsoft company had the good idea to develop, in addition to MS DOS, the graphic interface layer called Windows.
In a single shot, it could provide better user-friendliness to the computer population of PCs powerful enough to accommodate it.

A new strategy 

Faced with the success of Microsoft, Intel and the cloners, in the early 1990s IBM, Apple and Motorola decided to join their efforts to produce a new generation of microprocessors - the Power PCs, based on RISC technology and thus more powerful than the earlier generations.
New, very powerful microcomputers using this new technology have recently appeared on the market: Apple's Power Mac and IBM servers.
Their power already allows them to acquire a software compatibility to operate in a satisfactory manner most of the programmes set up for MS DOS and Windows, in addition to those for Macintosh or OS/2.
Another option for gaining compatibility with the Macintosh PC platform consists of equipping them with an expansion board which integrates an Intel microprocessor and makes them completely compatible with the entire range of applications designed for IBM compatible PCs, like any compatible one.

Moreover, Apple decided to yield to the pressures it had been under for a long time to disseminate its technology and sell its license to enterprises which wished to produce Macintosh clones.
Only recently, several manufacturers have started proposing cheaper MacOS compatibles.

As for IBM, Apple and Motorola, the trio united their efforts yet again to define a "Common Hardware Reference Platform" (CHRP) based on the new Power PC microprocessors.
Machines similar to this new standard should soon be available on the market.
This hardware platform is designed to be compatible with all the operating systems and thus all the existing applications.

In this way, users will be able to work with software which offer them maximum personal comfort and features on the operating system of their choice.

- PC (Personal Computer) IBM compatible (which are by far the most numerous) 
- IBM's PS (Personal System) 
- Macintosh and Macintosh compatible 
- machines built around the Power PC microprocessor 


A fifth platform of compatibles with the CHRP (Common Hardware Reference Platform) standard should soon be out.

There are also several work station platforms operating under Unix.
They are very powerful computers which physically resemble microcomputers, but are much more expensive.
We shall not be dealing with this area.

Disk operating systems, ergonomics and the man-machine relationship 

All the elements which make up the microcomputer are controlled by what is called the operating system.

This system is a programme which defines and manages the majority of the computer's functions.
It defines the different forms of dialogue the user may have with his machine in order to ask it to carry out the tasks he expects of it.
When computer science was first introduced, mastering the dialogue with the computer was a science on its own.
One had to write a language according to a bizarre syntax in order to be understood by the computer and to be able to use it.
This was not very easy, but fortunately things have changed since then.

Today, thanks to the sophisticated operating systems which have been developed, we can give the computer orders with a mouse that turns into an indicator, i.e., a little arrow appears on the screen.
We can move this indicator on the screen by moving the mouse on the table.
We can thus point at words or icons which are displayed on the screen and which correspond to operations which we can have executed.
We need only point at the icon of the chosen operation and click on the mouse's button to select it, or double click to carry out the operation.
Thus, computer scientists have taught computers to interact with us using a virtual derivative of the primitive arrowhead used by our hunting ancestors, and it is much better so.
We can even give them commands by talking to them, but that is another issue altogether.

Operating systems and software in general are called ergonomical when they have been designed to satisfy the needs of the user, to make their work easier and more pleasant.

A long road had to be travelled before reaching this stage: it meant breaking with the kind of corporatism that wants work to be a dull chore.
For too long, far too many computer scientists believed that they would always be the only persons to use computers.
In order to interact with a computer, i.e., to ask it to carry out a task, or to get the result, we needed to know how to write the same language it used: we needed to know about computer science.
Yet as we have seen, it occurred to some young people with vision that everyone must be able to use a computer, and especially that it was the computer that needed to learn how to communicate with its user.
Between the language of the machine and the language of man, there stood the need to create an interface dialogue which would be easily accessed and ever so simple to learn, i.e., as immediate or intuitive as possible.
It even meant avoiding any impression whatsoever of learning anything.

MS DOS 

IBM started out by designing the Personal Computer and asked a small company called Microsoft, unknown at the time, to supply the MS DOS (Microsoft Disk Operating System).
The clone manufacturers multiplied their sales and a genuine free software market was created for this platform.
In search of the largest possible market, several design teams decided to launch out freely and develop application programmes as they felt they had no obstacles in their way.
Each team came up with its own solutions in order to make the most of the MS DOS potential.
Yet no coordination was possible between these different teams since each was trying to make a breakthrough with its own solutions.
The user was the victim and had to learn different commands again as he went from one application software to another.
Daily use of computers by an average operator was felt to be difficult and unpleasant.

Working under MS DOS means writing specific commands for the tasks you want the machine to execute with thorough respect for the syntax.
There are several commands which must be carried out in a specific order.

This rather looks like computer programming.

This is the plat.form computer scientists prefer.

According to this logic, the end user must first learn the basics about programming and MS DOS.
He is not considered able to master the smooth running of his work environment easily and on his own without special skills.

Even if MS DOS has come a long way since then, it remains very elementary in terms of ergonomics until Microsoft produced a top layer to offer more user-friendly features with Windows.

The Apple Macintosh Disk Operating System 

The Macintosh was designed in 1984 with the 68000 microprocessor from the Motorola company as a compact and portable system which is still famous.
But what was really original and even revolutionary was its operating system which equipped it with the first user-friendly graphic interface based on icons and the first mouse.
It quickly became very popular.
Anyone could use it without knowing anything about computers, without typing even one line of codes.
Furthermore, it was equipped with very high-performance graphic programmes for its time.
More and more sophisticated versions followed until the 7.5 System and finally the MacOS.

When you turn on your Macintosh, the arrowhead we talked about shows up on the screen, as well as a simulated office: the hard drive, the diskette and the wastepaper basket are each represented by an icon, i.e., a graphic symbol which resembles them and bears the name (up to 31 characters) they were given.
You need only click twice on a file or disk icon to open it and see its contents appear in a window in the form of other file or document icons.
Any file or document can be taken in the window where it appears by pointing above it and pressing on the button of the mouse to move it about as you would manually and put it in another window, file or copy it on another diskette or even put it into the bin to erase it.

The name of a file or document can be instantly changed by clicking on the rectangle where it is written and writing the new name over it.
The icon can also be changed and replaced by any 32 x 32 pixels graphic.
The menu bar at the top of the screen allows the user to go through all the functions offered by pointing at the different headings.
The user just has to activate the function he wants with the indicator to carry it out.

The entire set of operations on the Macintosh can be mastered in this way by simple, almost routine gestures.
It has always been based from the very start on the "plug and play" principle.
This means that installing a new software, a peripheral of any kind or what is called an expansion board can be carried out simply by dragging the icon of a little programme called "extension" on the icon of the system file with the mouse, or by clicking twice on the icon of the installation programme.
The user can do it himself: he does not need to turn to a computer scientist for that.

Apple's homogeneous specifications of the operating system and the microcomputer account for a reliable and simple platform which makes the work of application programme designers easier, and which ensures better ergonomics.
In fact the commands and functions of the different application programmes have the same unfolding menus for the same functions, in the same form, without sidetracking the user who can better concentrate on the creative aspect of his task.
He can call up an online help function at any time about a question he has by clicking on the question mark in the menu bar.
He will get the available answers in a dialogue window, or in the form of cartoon bubbles which will be displayed with the track of the mouse on the different objects appearing on the screen.

Self-taught MacOS utilization is very widespread.

From the first steps, the user can go through a very friendly tutor software.
The corresponding icon appears on the screen once the software is opened and this icon teaches him through practice the first steps of using his microcomputer.
By moving the mouse, creating and going through documents, files and stored peripherals, opening, closing or changing the dimensions of windows, he can learn to use his machine on his own.

Each new version brings its own share of innovations, with features that are constantly being improved.
It is a user-friendly operation system whose qualities, features and ergonomics have been recognized by specialists who do research in the field of man/machine communication.
It has become a very dynamic field of research which brings together multidisciplinary teams: psychologists, semiologists, computer scientists, ergonomists, designers, etc. 

Yet its cost, which was relatively high at first, hampered a very wide distribution, except in the case of wealthy non-computer scientists and designers.
Little by little, in keeping with its rate of development, the Macintosh became a reliable and easy-to-use professional tool for printers, graphic artists, musicians, photographers, audio-visual producers and institutions which did not want to (or could not) invest in setting up a heavyduty computer service.
Today, the prices of the different platforms with the same configuration and hardware performance are similar.

Windows 

After selling MS DOS to all the clone manufacturers or owners, Microsoft's fortune grew yet again when it proposed Windows.
It was rather like adjusting the entire range of MS DOS platforms to an ergonomic level to enable a qualitative leap forward in utilization.
The new features offered by this graphic interface furthermore added to the numbers of the clientele and the commercial success of this platform, and reached into homes - a place more responsive to Apple's approach.
These new Windows features are more or less copied from those on the Macintosh operating system at the end of the 1980s, yet they could not boast of such a high degree of ergonomics.
Besides, the launching of Windows gave rise to a law suit at the time between Apple and Microsoft.

The ideas of windows, arrowheads, icons, unfolding menus were used again, but in the windows programme it was not possible to move files manually with the mouse to put them in as many files as necessary, or destroy them by putting them into the wastepaper basket.

To do so meant turning to a sub-file manager programme which organized the directory hierarchy rather rigidly and awkwardly by activating various functions from the unfolding menu to name or delete files.

Much progress was made, however, with Windows 3.1, for it meant anyone could use MS DOS without knowing too many command codes.
It meant easily using an application software to save and file the document without having to resort to too many bizarre signs.
Yet the limits of MS DOS were still very obvious.
For example, the length of file names was limited to 8 characters.
The windows and icons could be moved, of course, but it was not the same as eliminating a document or removing it from one file to put it into another, etc. 

The Windows 3.11 Workgroups version has interesting features, not only for managing local networks, but also to setting up the parameters of one's work space.
The Windows NT version is a very powerful multifunctional operating system which is rather difficult to master but which holds several interesting advantages.
For the time being, its complexity has stood in the way of wider distribution.

The new Windows version, 'Windows 95' , which was presented in its pilot version to professionals, is said to have made much progress in terms of ergonomics.
It offers the "plug and play" principle, very long file names and many other very interesting features.
Yet users will need to own at least an Intel 486 microprocessor, have 15 free megabytes to copy the programme on the hard drive and 8 megabytes of RAM.

OS/2 Warp 

As we saw earlier, designing the PS (Personal System) corresponds to IBM's attempt to thwart the clone manufacturers who had taken over the PC and invaded the market.
Mastering the MS DOS operating system also slipped from their control.
In collaboration with Microsoft (before this company published Windows 3.1), IBM developed OS/2 (Operation System) - a very user-friendly and powerful multifunctional operating system, based on a 32 bits architecture and inspired by the concepts developed by Apple.
However, this very high-quality platform did not achieve the commercial success expected, largely owing to its cost.

The latest version of OS/2, called OS/2 Warp, works on PS and IBM-compatible PCs.
The OS/2 Warp operating system is thus a very interesting alternative to Windows.
It carries a series of well-designed-and very useful integrated utilities.
Installing extensions such as the CD-ROM is made much easier.
It is a multifunctional operating system.
For example, it can display several video sequences at the same time, each in its little window.
In fact, it contains the software extensions needed to process sound and video if the computer has the necessary input and/or output interfaces.
Furthermore, it can run software applications designed for MS DOS and Windows.

UNIX systems 

Unix is an operating system and a very powerful development environment, which means that several operations can be carried out simultaneously.
It is said to be a "genuine", so-called "preemptive" multifunctional system, i.e., it can really carry out several functions at the same time.
This system can function indiscriminately on work stations as well as on all microcomputer platforms.
It is very popular with students and the scientific research community.
There are several specific versions and learning them requires special knowledge in areas of computer science.

The Unix family has made its breakthrough on its own by virtue of its universality, its design of Internet.
It is partly owing to Unix that information can circulate freely on Internet from one platform to another with no frontiers.
Linux, a new, free and particularly interesting version of Unix, has recently appeared due to international collaboration on Internet.

The main elements of the microcomputer 

Just for the sake of simplicity we shall say that the computer is a very powerful and sophisticated calculator which can also be used to create documents of all kinds, file them on the hard drive (or any other computer medium), search for them, edit them, distribute them and eventually constitute a database which can be consulted on a local or telematics network.

To acquire more in-depth knowledge about this system which at first glance seems complex, we shall try to break it down into its different components.

The microprocessor 

A microcomputer bears the stamp of its microprocessor which is the element that does the calculations.
It is an electronic chip on which hundreds of thousands of microscopic electronic components are engraved.
The number of components (equivalent to transistors) which can be engraved on a chip is continuously increasing.
Today, there can be several millions.
So they are becoming more and more powerful.

The frequency at with which they do calculations is becoming faster and faster.
This frequency is expressed in megahertz.

They can process instructions according to the technology they were designed with, either CISC (Complex Instruction Set Computer) technology or RISC (Reduced Instruction Set Computer) technology.

Memory 

The data of operations to be executed as well as those corresponding to the results of the calculations are stored in what is generally known as RAM (Random Access Memory).
RAM is another type of chip in which information expressed in bytes can be recorded rather fast.
Nowadays, slides of RAM can stock between 1-64 million bytes (1 to 64 megabytes).
It affords access to this information very quickly, in no more than 70 nanoseconds.
It is usually possible to add slides of RAM on the original computer card in order to work with more powerful applications on graphic documents or high quality video.

There is also another type of memory which is soldered in the mother board of the microcomputer: the ROM (Read Only Memory), where the information necessary for the functioning of the system, which is not included in the operating system, is loaded.

The hard drive is another memory with a greater capacity and can hold between 100-9,000 megabytes, or even much more, can either be internal or external, and on which all the necessary programmes are recorded.
First there is the operating system which takes control of the RAM from the time the computer is turned on, and which generates on the screen its graphic interface of commands and the mouse indicator.
Then there is the application software with which documents can be created, edited and even distributed.

Inputs and outputs 

The input interfaces like the keyboard, mouse, scanner, digitization sound and video card, etc., provide digitized information that make their way to the microprocessor through complex circuits on the mother board by circuits and chips that control the data flow as they enter and leave.
At the end of the circuit, the results are displayed on the screen.
The user can record them on a peripheral such as the internal (or external) hard drive, a magnetic or magneto-optical diskette, or a magnetic saving tape.
If not, he can print them on a sheet of paper or a transparency.
If there are sounds or pictures developed by the system, they will be recorded on an audio or video magnetic tape, or even projected on to a big screen by a video projector.

Each additional element (printer, scanner, camera, microphone, tape recorder or video cassette recorder, projector or any other) is connected to the central unit by a power point called a "port" or specific interface.
If this interface is not present in the machine, it is often possible to generate one by adding a suitable expansion board to the mother board.
The input /output dialogue interface with the telephone network is called a modem ( mo dulator- dem odulator).

Data display 

Data is displayed on the screen by a system which accelerates and optimizes the operation.

Figure 6

Sample of a digitized picture

This system may be presented in the form of an expansion board which includes a specialized 32 or 64 bits processor, and a relatively large VRAM (Video Random Access Memory).
These cards are connected to the mother board of the central unit using a standard NuBUS, VESA or PCI power point.
Certain microcomputers include in their mother board all the elements necessary for managing the display function in an optimal manner without the need to add a board.
Different parameters are involved in the display operation: screen size, number of colours, picture dimensions and available video memory.

The picture is displayed on the screen in the form of contiguous points which succeed each other on the lines of the screen.
These points are called "pixels" ("picture elements").

Depending on the size of the screen, the picture will be: xxx .

Table 1

Picture size

Picture dimensions x no. of bits = needed memory.

Buses 

Inside the computer, the data passes through canals which are called "buses".
There are several types of buses: ISA, VISA, EISA, PCM-CIA, NuBUS.

The ones which have most appeal are those which flow at the highest speed, such as PCI (Peripheral Component Interconnect), which offer up to 100 megabytes.

Several kinds of buses can be fitted on a single machine, depending on the type of expansion boards it can accept.

Through a standard SCSI (Small Computer System Interface), one can link up several peripheral elements to the central unit such as the CD-ROM, external hard drive, magneto-optical diskette recorder, saving on the magnetic tape, etc. But this type of connector only allows for a throughput lower than 2 megabytes, which is not enough for certain video-inclusive applications.

These call for the SCSI2 standard.

Multimedia features 

A microcomputer is said to be multimedia if it can process both input and output of audio and video files.
As pointed out, towards the mid-1980s, Macintosh lost no time in developing the standard for processing sound as both input and output.
Expansion boards made it possible to improve processing quality at a professional level.
As early as the beginning of the 1990s, the extension of the operating system called QuickTime made it possible to process compressed video, run in a little window, and as input with several digitizing boards.

Since then, certain models from the Macintosh or PowerMac range have been equipped to process multimedia standardized data at different quality levels.

The PS/2 multimedia standard is called "Ultimedia".
It is based on the DVI (Digital Video Interactive) technology developed by Intel and was the only one to offer the possibility of having video on the screen and sound when it was launched.
It does so with an expansion board which allows it to process audio and video in satisfactory conditions.

Table 2

MPC Standards (Mhz = megahertz)

Figure 7

The multimedia system and digital media

As regards IBM-compatible PCs, in order to allow independent developers to design multimedia expansion boards, the MPC standard was published at the initiative of the MPC Marketing Council and several enterprises of the sector, especially Microsoft, in order to add sound and video processing features to the IBM compatible machines.
There ale now several kits for bringing IBM compatible PCs into line, all carrying an expansion board, a CD-ROM reader and software for installing and using compatible multimedia applications fairly easily.

Lack of specifications of the ports in the MPC standard which made compatibility difficult favoured the development of solutions by the owners.
However, the SCSI port is being used increasingly as an established standard.
The quality level differs according to the power of the microprocessor which is used.

However, models of several IBM compatible brands are being sold although they are configured entirely according to the MPC standard.
Those targeted to a wide public include a CD-ROM reader, and do no more than process the sound and video of the CD-ROMs.
Others, more powerful, are targeted to multimedia professionals who can thus process sound and video as input and output on the selected media.

We shall deal later with the different solutions for producing multimedia applications from different platforms.
We shall now examine the different media which can be used in the data processing environment to process, file or distribute documents.

Magnetic media 

On magnetic media, coding digital data occurs through punctual modification of the medium's magnetism.

Magnetic media come in the form of tapes or disks .
Tapes can be read in sequences.
They are run against a fixed reader/recorder head and take some time to rewind.
The disks spin around a mobile reader/ recording head and allow instant access to the desired data.
However, magnetic media are not very reliable because they can become accidentally demagnetized due to magnetic fields, or gradually with time.
Moreover, as a rule, due to rewinding time, tapes are less interactive than disks.

Available in the form of tapes are cassettes and cartridges.
Disks come in hard drives and diskettes.

Figure 8

The principle behind the 3,/2" reader-recorder diskette 

The magnetic diskette 

Economical and practical, this is the most popular and most universally used magnetic medium.
This technology was largely responsible for democratizing data processing.
The 5 1/4" floppy disk was in the beginning the aid for engraving data on the first microcomputers and its capacity was limited.
The diskette format which took over was the 3 1/2".
Well-protected in its durable plastic envelope, the magnetic disk can carry on its two sides either 744 kilobytes in the Double Density format, or 1.44 megabytes in the High Density format.

Light and small enough to carry in a pocket or envelope, it is an ideal format which has been adopted by all the big publishers to sell programmes.
Its rather limited capacity is, of course, a handicap.
Yet it is by far the universal standard for distributing software packages.
Reader-recorders for these diskettes now exist on all the computers manufactured.
Only the block formatting varies between Macintosh and the IBM compatible computer.
However, any 3 1/2" PC formatted diskette can be read on a Macintosh, thanks to the Exchange PC/Macintosh extension.

A 3 1/2" magnetic disk with a capacity of 120 megabytes based on a technology that controls the playing-recording head and a new magnetic medium, both more high-performance, was released at the end of 1995.

DAT (Digital Audio Tape) magnetic tapes 

This type of audio magnetic tape is widely used periodically as a backup aid to ensure the safety of computerized data.
As for a DAT tape recorder, the computerized data is engraved on the tape as it passes in front of the magnetic head, in standardized blocks which have an address.
Upon request, a search for the address of the required data is carried out.
But rewinding to access the spot on the tape where the information required is found takes time.
Since this medium is not very quick at accessing information, it is used mainly for filing large quantities of information (up to 14 gigabytes).
Automatic backup programmes on this kind of medium are very practical because they secure network management in case something happens to the hard drive.

Cartridges 

SyQuest cartridges, like Bernouilli cartridges, are magnetic cassette tapes with special winding systems which allow for optimal access time and play speed to file several dozen megabytes (88,120, 240 megabytes or more).
The SyQuest cartridge is widely used throughout the DTP world; e.g. it means design studios and printers can exchange huge files which make up the dummies of books, review and graphic documents.
Owing to their fragile nature, these formats were not suitable as editorial media despite proper distribution within professional circles.

Figure 9

Diagram showing a very powerful hard drive 

The hard drive 

The hard drive is an organ which is usually built into the microcomputer.
Files containing documents, files corresponding to these documents and the programmes which are used to develop them are stored in its mass memory.

The programme you wish to use must be loaded on RAM (random access memory) as well as the data it must process.
Once the processing over, the result is saved in the form of a file on the hard drive.
Then it can be used and produced on a chosen medium: paper, diskette.
So the hard drive is not actually an editing aid.
However, it is not difficult to imagine transporting a particularly voluminous application (with certain precautions) to another microcomputer on an external hard drive.

Externally installed, the hard drive comes in the form of a case which is connected to the data exchange bus connector.
In this way, it can be added or removed from the system if need be.
An internal hard drive is a module which is connected to the same bus, but inside the case.

Currently, the hard drive is the most high performance recording and computerized data access medium.
There are hard drives with a capacity of several gigabytes (thousands of megabytes), transfer rates of 10 megabytes per second on SCSI 2 buses, and information access time of around 5 milliseconds, at very affordable prices.
This has fostered the development of multimedia applications which are full of pictures and full-screen video - something impossible until now.
These features can be further improved by using the hard drive pop-up technology which means higher speed and secures the recording of data by copying them on two hard drives at the same time.

Optical media 

Figure 10

The different stages of the Compact Disk 

Among optical media we shall be dealing mainly with the CD-ROM or Compact Disk Read Only Memory family, a product of Philips and Sony, developed when they defined the format and fine-tuned the technology required for the Digital Audio Compact Disk in the early 1980s.
In principle, disks of this family are not Write Once, i.e., the user cannot use them to record data.
They can only be read, hence the name CD-ROM: Read Only Memory.

Today, the CD-ROM is approaching compatibility with most CDs: it reads just as well CD-ROM XA, CD-Audio and Photo-CD.
If the technology of duplicating these disks in large quantities cannot be mastered using a microcomputer, then the Kodak PCD 200 Optical Compact Disk reader-recorder foreshadows the current development of such technology.
It records the following different formats: CDROM, CD-i, CD-Audio, all built to the ISO 9660 standard, on an indelible, Write Once disk, the Writable CD (of the WORM type: Write Once Read Many), using the appropriate formatting programmes.

On the market today there are many reader-recorders of this kind.
They make it possible to print out one by one a document edited and designed on CD for internal needs, but also to test it on a sample selection of users.
It is also possible to file on a CD of this kind huge application files which crowd hard drives after they have been produced and used.
Photo laboratories offer a new service to customers: insetting their photos on Photo-CD.

The CD-ROM 

As has already been shown, the CD-ROM (Compact Disk-Read Only Memory) can only be used, in principle, for reading disks.
As in the case of the CD-Audio, it is a publishing medium which allows the publication of around 600 megabytes of multimedia data on a very economical medium.
The advantage of the CDROM as compared to other CDs is its portable nature.
Anything published on a CD-ROM can be run very easily on any other kind of CD since they are all derived from the same format.

The reading function is controlled by the computer to which it is connected.
Minimal configuration is required to use it properly.
Depending on the titles, the constraints will be different, but we recommend either a four megabyte RAM Macintosh, displaying a minimum of 256 colours or an MPC standard PC, i.e., with an 80386 processor with a 16 megahertz frequency (or even better, an 80486), four megabytes of RAM, a screen with a VGA 256 colour card, a sound restitution card compatible with the CD-ROM version required and Windows 3.1.

Figure 11

The principle behind the Compact Disk 

A new generation of compact disks with special features is being developed.
Depending on the technology used, they will be able to record between 7 gigabytes (on one side of the Multimedia CD format from Sony-Philips, compatible with the CD-ROMs) and 10 gigabytes (on two sides of the Matsushita-Toshiba format, non-compatible with old CD-ROMs).
It is now possible to record 72 minutes of compressed video MPEG (Moving Picture Expert Group) format on a CD-ROM.
In the future, it will be possible to record more than four-and-a-half hours on these new standards.
However a new video compression technology from the Houston Advanced Research Centre (Harc-C) allows for compression of five times more video.

The magneto-optical medium 

The technology of magneto-optical media is based on the principle of modifying the magnetic orientation of one point on the surface of a metallic oxide alloy disk subjected to the heat of a laser ray focused on this point.
Once cold, the medium cannot be modified by a magnetic field.
This explains why magneto-optical media are quite long lasting, and very useful for filing purposes.
When being read, the laser switches to low power.
Thus, the ray reads the magnetic points without heating them.

Compared with the speed of the hard drives, magneto-optical disks are rather slow at recording as at reading.
This is due to the process of inscribing the data.
First, there is a data erasing phase on the portion of the disk concerned, then there is a new data inscription phase, and then finally a third phase of data checking .
So while it is not a detachable hard drive, it is on the contrary an excellent storage medium with a large capacity and faster than a magnetic floppy diskette.

Figure 12

The principle behind the 3 1/2" magneto-optical diskette player-recorder 

Magneto-optical disks can be taped and erased like all magnetic medium.
Unlike CDs, they are not, in fact, considered editorial media.
However, these magneto-optical disks are not considered cheap, although a recorded megabyte costs $US 0.23, which compares favourably with a magnetic diskette which fetches $US 1.40 per megabyte.

With the same dimensions as the 3 1/2" magnetic floppy diskette, and barely twice as thick, the magneto-optical diskette can be despatched with a letter.

There are two versions: 128 megabytes and 230 megabytes.

There are also disks with a 5 1/2" format which carry around 600 megabytes.
The block format is not compatible with magnetic floppy diskettes and CDs.
This type of medium has many advantages and should be widely distributed as a mass storage medium with a large capacity.

Local networks 

Networks connect computers one to the other for the purpose of exchanging data.
What characterizes a network is the type of cable and interface it requires, its transmission flow, i.e., the quantity of information it can transmit in one second, the number of machines it can house, the type(s) of platforms it can control, its cost, the quantity and quality of its features and its reliability.

In reality, the causes of microcomputer dysfunction, and more so of network dysfunction, are many: sudden electrical voltage variation, viruses, programme conflicts, etc. These accidents may take on disaster dimensions if the processed data is not saved and filed.
If there is a problem with the hard drive of the server and there is no backup, all the data belonging to an organization can disappear forever, causing a major catastrophe for the organization.
If there has been a recent backup, only the work carried out after that backup would be lost.
In some cases, it could even be recovered in the hard drive of each terminal.

The network concept is based on the principle that a server computer placed at the head of the network controls communication between all the client computers of the network.

In general, the server is much more powerful than the client terminals.
It controls the database, communication and access between the different terminals in relation to the tasks they are assigned.
The server also controls the periodical file backup to ensure greater security.
This backup is carried out on the server's hard drive, then on DAT magnetic tapes, or powerful magneto-optical tapes.

There are several solutions for setting up local networks (LAN).
The most interesting ones are those which afford the fastest flow at the best price, and which offer access to all the different platforms.
The Ethernet technology is currently taking over.
It allows for a flow of up to 100 megabytes depending on the chosen options, and it is available on all platforms.
Interfacing expansion boards for the Ethernet networks are very low-cost.
Certain platforms or models are equipped with a standard Ethernet plug.

Figure 13

General diagram of a network

The physical network is managed by a programme which can work with MS DOS, Windows, OS/2, MacOS or Unix depending on the platform used as a server.
This network management programme usually enjoys the features and the graphic interface of the operating system under which it is functioning; but network management programmes benefit, above all, from their own features.
First, there are the installation features which enable an open and very quick installation, or on the contrary, a long and difficult one.
The effectiveness of the dialogue features between the terminals and the server, in terms of gaining fast access to information frequently used, allows the users to save a lot of time.
The simplicity of running the network with display of the processes underway at each terminal means better control of the operations.
The features offered for linking up the network to a computerized environment- already in place are quite welcome in the case, for example, of connection to a centralized or heterogenous computerized system.

The parameters of certain network management systems can be automatically fixed to take into account the development of the server or the network, or to oversee how the overall task of updating data is shared among the client working stations.
Automatic backup is an interesting feature: it can be applied, for example, during the night, to the server's data, but also to that of the client terminals which may be turned off after the backup has been completed.

The main network management programmes are: NetWare from Novell, Windows Workgroup Server and NT Server from Microsoft.
They all function on IBM PC compatible computers.
The NetWare 4.1 version includes the OS/2 and the MacOS media, but this programme is for the moment targeted to sharing text type files, and is hardly suitable for multimedia applications.
Workgroup Server AppleShare and PowerShare are the solutions proposed by Apple.
Also worthy of mention is Workgroup Server 4D from ACI, which integrates a database management system, a word processor, a graphics software and which can be run under Windows and MacOS.
Workgroup Server 4D can thus work under IBM compatible PCs, Macintosh or Power Mac and accommodates the connection of systems or heterogenous networks.

The applications development features available on these network management programmes are quite handy: not only do they let the user develop applications which optimize enterprise management or administration, but, above all, for our purposes, they allow scope for the design of training applications on networks which hold particular interest from the teaching perspective.
This question has already been addressed in the section "Networks and Training".

Multimedia applications can be divided on an entire physical network which has a transmission flow close to 10 megabytes.
The faster the flow, the smoother and more functional the multimedia applications.
As seen in the case of the videoconference, even the ordinary telephone network can be used for exchanging video images in actual time at a frequency corresponding to the rate of the line and the modem used.

The methodology used to produce media 

The world of education and training is not a closed world which is set apart and withdrawn.
It is in touch with its environment, society and its partners alongside the fact that its vocation is to impart knowledge to learners.
Training institutes should manage their business consciously, fully and pertinently in terms of their communications.
Their communication programmes or operations should be designed with the means and skills required to achieve the best results.

In general 

What happens at the hub of information exchanges expressed in the form of information and knowledge transmission?
Although the complex nature of the processes at work in this kind of operation has not been studied in a scientific manner until fairly recently, the practice dates back to the dawn of time.
Animals themselves communicate, and so do plants and cells.
Life is communication.

Figure 14

How information flows

A source produces a coded message to be transmitted on a "route".
The message is picked up by the target, which decodes and stores it.
This mechanical type of scenario applies to most communication situations.

Information transfer is effectuated from the source to the target, with a drop in information entropy (the source creates order in the mind of the target).
The coder registers in the energy flow of the media the modulations of the message which the decoder translates for transmission to the target.
Energy is expended between the transmitter and the receiver along the entire transmission route, with a rise in energy entropy (energy consumption creates disorder on the transmission route).

Energy factors can affect the quality of transmission.
They can weaken or interrupt transmission or interfere with the energy flow that conveys the information and which may come from the route itself.
We speak then of noise.
A route is thus characterized by the noise it causes, but also by its frequency range .
If the amplitude modulation of the energy flow which crosses it oversteps the thresholds of its frequency range, the signal is "clipped" at each end and when decoded, all the information is not there.

Information factors can also disrupt the transmission of the message, especially in terms of the coding, which supposes a convention determined beforehand between the source and the target if decoding problems are to be avoided.
This convention agreement is a code , i.e., a list of signs .
Any sign has two sides: the first is the significans (the modulation which registers it within the energy flow), and the second is the signification (the conventional definition of the sign).
Yet a single signification does not necessarily correspond to one significans and vice versa.

A sign can have several meanings, as homonyms (a single word can refer to different things) or by polysemy (a single image can be interpreted in different ways depending on the context or the observer).

The different types of documents 

At first glance, documents can be distinguished by their nature, origin, target, form and contents.
They are usually classified according to these criteria.

Yet they also bear the stamp of the technological media which accommodate them, which convey them and of their applications.
In the field of education and training, documents are used for information, training, promotion or public relations purposes.

We shall classify them into three categories: traditional, analogue and digital media.
By traditional media we mean anything that is printed on paper or on transparencies, used as much for administrative as for informative purposes and geared towards a large public or any other.
Analogue media are those which use so- called "analogue" technologies and which transmit information in the form of a modulated electromagnetic signal, meaning audio visual techniques and more specifically audio and video cassettes.
Digital media mean all the peripheral computer media which, as they develop, have become publications and communications media and which are currently bringing radical changes in work, training and information methods.

Objectives 

These few general observations should allow us better to situate the problems faced by the document designer and producer.
First of all, as with any production or training process, it is a matter of properly identifying the source of the demand in order to have a firm grasp of the nature of the need expressed.

Recording this control interview in writing or on a tape recorder would be useful for developing the project.
The exercise should focus on understanding all the useful items, including those implied in the language of the transmitter, in order to pinpoint all the objectives and expected results of the operation in question.

Figure 15

Document identification 

Once the job is commissioned, the designer must be able to identify the means involved and the right approach for processing the contents.
He should be able to identify the target group and know the details of the project: place, date and time when the final document is to be submitted and distributed, identification of the distribution method and the distributor.
The time set for implementation of the project should be specified as well as the main deadlines and the available budget.

Recapitulation of all these points in detail, with the aim of clarifying any vague points in the presence of the client, will be important in order to check that there is in fact an accurate mutual understanding about the job commissioned.
The problem the designer will always try to avoid is finding himself in the end with a document that does not meet the expectations of the client, or to the needs he specified.
That would be a disaster, for it would mean a lost investment.
Sometimes it is even necessary to go beyond the order specified to see whether it does not imply a different approach and to develop it.
In this way, the client benefits by refining his approach to the problem and by specifying the objectives according to data he had not previously perceived.

Figure 16

Classification of media.

Table 3

The main stages involved in the production of a document

Design 

The commission might be ideally concluded in the concrete form of a written document co-signed by the two parties.
This is often called "production specifications".
The designing stage of the document(s) in question can begin.
The designer may well need to check the validity of the information he has been given and search for complementary information or documents.
Next, he will have to find the answers to such ritual questions as: Where?
When? 
How?
Why?
For whom?
With whom? 
With what? 
To lead to what? 
In this way, little by little, the resources to be used can be arranged down to the last detail, and the first elements of message design and processing solutions will become apparent.
Risks of any kind will be signalled as far as possible.

One of the key elements for a successful operation is to identify the average client as clearly as possible at the initial stage of design.
It is necessary to know his standard profile, socio-professional group, cultural practices, pastimes, his home, and the media he is in the habit of using so as to design a document which corresponds to his usual cultural environment.

The aim is to stay as close as possible to the language and concerns of the average client's profile, so as to win him over without troubling him.

The producer has to make a check-list - not forgetting any - (and we always do!) of all the required items he must coordinate in order to produce the right document for the deadline and with the means he has at his disposal.
First of all, these items are inherent to the media he must use; each has its own characteristics which are peculiar to the technological paths they have followed.

Next, these items are characteristic of the type of document to be produced.

As early as the design phase, the ideas and the layout must be tested with a target group to check whether the product is relevant.
Documents already in hand used in production should be released from copyright, or authorization to use them should be sought from their authors or the persons who hold the different rights involving the use of these documents.
This is a very important matter which must be settled at once when the document is ready for design so as to avoid serious setbacks or tiresome legal problems.
During the design phase, it is advisable for the producer not to use documents for which he does not hold the rights or which he does not have the means to acquire or produce himself.
It is better to respect this rule.
Moreover, when developing the contents, care must be taken to respect the information produced in these documents.

In the ideal situation, depending on the type of document or media involved, the design should result in actual print-out of "intermediary design documents" which are useful for communicating the project contents to the different partners; especially to the client as well as the different collaborators involved in production.

Figure 17

Coordinating all the constraints involved in setting up the work plan.

Planning the operations is an activity which should yield results as soon as possible in the form of the schedule in which should appear, in chronological order, all the tasks which must be accomplished by the different persons involved according to priority.
The dates for delivery and distribution of the document should be included in the schedule down to the last detail.
Unforeseen events or delays are generally common occurrences, so safety deadlines should be set which would leave room for catching up and meeting the dates fixed for the end of the operations.
To do so, it serves no purpose to underestimate the time needed to perform the different tasks in order to gain time.
The tasks should be accurately defined and fairly shared.

Production 

During the production phase, it is imperative that all the necessary resources be made available to the different persons involved at the right time.
All the technical resources needed throughout the whole production process should be listed as soon as possible in order to have them at hand when they are needed without any delays.

They can be acquired, rented or borrowed free of charge and tested before they are actually used so as not to have any unpleasant surprises at the last moment.
This also refers to the availability of basic documents, materials or software programmes involved in digitizing and processing the different elements, assembling the contents, developing the applications and producing the finished product.

The different skills will be mobilized within the production structure and, if need be, outside for specific and occasional services provided by fine-tuning specialists.
Financing should be taken care of at the outset with a budget estimate agreed to by the different persons in charge.
The funds should be disbursed and rigorously managed.
The logistics should also be carefully prepared: premises, vehicles, transportation as may be required, accommodation, meals and various services should be booked well in advance and re-confirmed before the starting date to avoid any untoward situations at the last minute.

The project's implementation will be carried out under the supervision of a general coordinator called either "project chief", "director", "supervisor", or "editor", depending on the kind of document being produced: multimedia, film, television programme, book.
In all cases, development of the contents should respect the intermediary design documents (summary, production specifications, synopsis, scenario, technical cut-outs, story-board or dummy) which were previously adopted by common accord.
Each person involved must provide the most relevant service by striving for optimal performance in terms of the expression, enrichment and degree of meaning, and creativity in conformity with the entire document being produced.
The director is responsible for the coherence and the general harmony of the overall document.
He will guide each person's work according to the final form he wishes it to take so as to make it fully relevant to the overall product.
Imbued with the idea of what the product must be before it actually exists, he will imagine it in his own way in terms of impact and style and will do all in his power to motivate all the persons involved to pool their efforts so that the result corresponds to this creative vision.

So this director's task calls for management both in terms of elaboration and processing of contents.
It is also a risk management job, for each decision taken implies that the final document must be in conformity with the request and the fulfilment of the objectives.
Not least is it a job of managing communication with every one of the collaborators and people in charge.
The director must find words to express accurately what he expects from each individual, words to conjure up all the required elements, but also to inspire motivation and emulation and stimulate creativity.

Once all these elements in place, they should be organized, edited and assembled into a dummy which will be fine-tuned and then presented to the client and a sample of the target group.
Any changes needed will be made to the dummy to finalize the document which will be ready for duplication, print-run, distribution or use depending on the case.

Distribution 

Figure 18

Distribution of documents. 

Even if the documents are designed and produced with much care and evaluation throughout the production process, they should be properly checked to ensure that there are no latent flaws.
The impact of the document on a sample of the target public will be tested and evaluated using a detailed and rigorous protocol once before distribution.
If after the evaluation it turns out that the document in question does not correctly fulfil its role, the producer should not hesitate to modify it.
One should think seriously before distributing it so as to avoid getting involved in a pointless, ineffective or senseless undertaking which would bring into question the image and the reputation of the enterprise on whose behalf one is acting.

The distribution will be carefully prepared and carried out.
Evaluation of the real impact on the target group will be organized down to the last detail.
Care should be taken to deliver all documents to their clients at the right time and in good condition.
The evaluation procedure should be ready for implementation.
The results should be collated processed, and quickly analysed for the purpose of making any new changes without delay on the document itself or the distribution or evaluation method if need be.

In the field of communication, nothing is ever certain until the addressee of the message subscribes to it without reservations and even then questions may be posed as to the vanity of appearances, people's reasons for subscribing, or the validity of the steps taken.

Nothing is ever won once and for all, and everything may be called into question.

PART THREE 

Production and distribution of media 

We have seen earlier that the microcomputer linked to networks is at the heart of the current technico-economic crisis and evolution.
It is a basic tool which makes it possible to optimize information production, management and distribution in general and in the area of training in particular.
In the previous chapter we have just looked at the precautions to be taken in order to match the needs and objectives which motivated the production of the documents with the real impact of these documents on their addressees.
Now we shall examine how solutions can be used to optimize the publication cycle of training media.

As we have seen before, the microcomputer has to communicate with peripheral elements to which it is connected in order to carry out a certain number of specific tasks.
These connections are made through special "serial" or "parallel" plugs which are usually installed at the back of the microcomputer.
The different peripherals are connected to these plugs.
With the serial type plugs (usually SCSI standardized), it is possible to connect several peripherals one after the other to install configurations which correspond to different needs.

Figure 19

Diagram of an SCSI system

This is a modular set-up which can be adapted to different needs.
A configuration for word processing may only include a black and white or colour laser or jet printer (the latter are more economical).
The printer could eventually be shared on the network with other terminals.
If the task includes page setting for various kinds of documents, a DTP work station must be set up with a scanner, a graphics tablet and a storage peripheral.

A configuration devoted to computer-assisted presentation (CAPres) will be connected to a liquid crystal projection screen or a video projector adapted to digital projection.

The material configuration will of course be complemented with suitable programmes which enable the user to achieve the level of performance corresponding to the expected results.
The necessary software programmes should as much as possible be presented as a coherent set, offering the best possible user-friendliness.
In this way, any part of a text, illustration, photo, sound or video can be cut and moved (pasted) elsewhere in the same document or to another, on another software programme.

This is the main principle underlying computer-processed documents.

Printed matter 

If we want to describe a black square, for example, we could choose between two methods: either describe the picture of the square in the middle of the screen point by point, or describe it in an analytical manner through a series of four vectors placed in the plane which define its shape and given the information: "colour the inside of the closed shape in black and leave the outside white".

Using the first method, which is called "bitmap" (binary map), each pixel of the screen is described by information coded on 2,8,16 or 24 bits.This represents a large quantity of information.
Using the second method, which is called the "vectorial" method, we shall only have very little information in our document.

If we want to describe the letter "A", we can thus describe it in the form of a bitmap or with vectors.

If it is a large drawing with only this letter, the most economical and quickest method is the vectorial.
But if it is a text of 500,000 characters, that will mean a rather lengthy file.

Figure 20

Picture coding

In fact, the text is the first language man taught the computer.
When we press the "A" key on the computer, the computer translates it into a binary digital code on 8 bits, called ASCII code (Standard Code for Information Interchange).
It is the simplest and most economical way to represent a letter: by a single binary number rather than describing its shape.
In this case, our text of 500,000 characters only represents a document of 500 kilobytes.

Printed documents are, in fact, a complex sets of texts, graphics or photographs which are arranged in a specific order on the page.
In fact, there is a page description language which has become a standard in the publishing world - the PostScript language.
It defines documents to be printed and makes it possible to print them on office printers, or for excellent quality in a printery.
Any text, graphics, photographic or composite file can be converted to the PostScript format.

Printable media usually come in paper form, but any smooth surface is printable in theory: cardboard, fabrics, metal, plastic and transparencies of course.
A document can be composed of one or several pages, even several hundreds.
The dimensions can vary from a postage stamp to a giant poster, including all the standardized formats: A5, A4, A3, A2, A1, A0.

Peripherals 

All this can be designed on a microcomputer and eventually printed on a laser or jet, black and white or colour peripheral printer.
From the first copies, the quality of the document can be viewed and tested on the potential user.
If the quantity to be produced is limited to a few dozen or hundred documents with an A4 or possibly A3 format, they can be produced autonomously.

Once the document is designed in this way, the corresponding file can also be handed over to a printer to mass produce the document.
Office printers cannot process large quantities, dimensions, binding, nor can they reproduce the quality of colours available in printeries which are equipped with extremely sophisticated, computerized and modern machines.

To create a document on a printable medium, there must be a text, possibly composed of titles, sub-titles, paragraphs and graphic documents for illustration purposes.
It might be that these elements were first created externally on a computer and then made available on a magnetic, magneto-optical diskette, or SyQuest cartridge in the form of one or several digital files which must eventually be converted to the format in which the design and page setting work must be carried out.

Or it might be that all of these textual and graphic elements are created entirely on the microcomputer.
The text can be directly typed on the keyboard with a word processing software.
Illustrations can be drawn with a graphics, drawing (bitmap or vectorial), painting software programme using the mouse, or with more creative features on a graphic tablet with a magnetic pencil.
They will then be available in the form of digital files in the chosen format.

It also occurs that if these elements were already created on no matter what medium, they would need to be digitized with a scanner connected to the SCSI outlet of the microcomputer.

Figure 21

Diagram of a CAP station

The flat scanner is used to digitize texts, graphics or photographs already produced on a paper medium.
Each digitized document is then saved in the form of a bitmap file on the hard drive.
The graphic files contain all the data which describe the graphic aspect of the document point by point.
This file may be opened with a drawing, photo touch-up, word processing (if it has a graphic illustration utility) or page setting software.
If the digitized text is not handwritten, it may be recognized in the form of ASCII characters through a character recognition programme.
This would avoid having to re-type the text in order to transform it into the ASCII code.

The slide scanner is used to digitize photographs directly on the photographic slide (negative or positive) without having to make a copy on paper.
In this way, there is better picture quality.
The file can then be opened in a photo processing software and countless operations will be possible to modify this photography.

The size can be enlarged or reduced; certain predominant colours can be removed; the negative can be made into a positive or vice versa; one part of the picture can be chosen and substituted for another; text or graphics can be added; and super-impositions and several other operations can be carried out.

Figure 22

Diagram of a scanner 

The digital camera can be used to process internal shots when those are possible if it is sufficiently powerful.
If such is the case, digitized files of the photographs can be immediately made available and ready to be processed, touched up and inserted in the documents to be produced without any assistance from laboratories or any external services.

Low-cost digital camera solutions enable the production of quality standards suitable for certain types of publications.
More costly solutions attain quality standards to satisfy those who are more demanding.

An interesting solution consists of transferring slide photographs directly from the slide onto a Photo-CD.
The Photo-CD is a format of photo editing on a special kind of CD-ROM designed by Kodak in collaboration with Philips in the early 1990s.
It was designed to be compatible with the entire series of products from the OCD range.
The aim is to supply professional or amateur photographers with a tool which allows them to process and publish their photographs on a range of digital aids compatible with the practices developed for publishers, writers and photographers in the different fields of publication.

On the Photo-CD, each photo exists in a whole range of document formats for a photo resolution which guarantees the best reproduction quality possible according to the final use that will be made of it.

The Photo-CD is produced from negatives, slides or paper copies.
The photos are digitized on specialized and high quality transfer stations made by Kodak.
These stations are used in photo laboratories.
At the end of the process, a so-called "Master" Photo-CD(s) is (are) delivered to the client.
It can contain up to 100 pictures which are available on this disk in the five different formats: Base 16, Base 4, Base, 4 Base, 16 Base, which lend themselves to the five corresponding types of applications.
High-quality paper copies can be made with the Photo-CD on a special printer.

Photos which are digitized in this manner are available on this practical medium and can be used in any document designed on a photo-processing or page-setting software.

Finally, the video camera is yet another means of obtaining pictures.
Yet it does not adapt very well to publication on printed media, unless it is a special camera with a very high resolution.
In fact, the resolution obtained with video cameras (even professional ones) do not attain sufficient resolution for a good quality of printing.

Word-processing software 

The main features of word-processing softwares consist of enabling the user to design the contents in the form of a layout with different parts, chapter headings, sections.
Writing the text should be made easy through possibilities to correct, rapid and easy transfer of any given sentence to any given paragraph or any given paragraph to a given chapter by a simple "cut and paste" technique.
Usually one only has to select the part in question with the mouse, choose the "cut" function from the menu and then put the insertion point in the desired place with the mouse, and choose the function "paste".
An even simpler feature is the "click and drag" technique which consists of taking the selected portion of the text by indicating it with the mouse and pressing on the mouse and dragging it to the desired spot.

The features concerning character appearance such as changing the size or the font, making the letters bold , italic or underlined , in superscript or in subscript are usually very easy to use: the portion of the text must be selected and the function must be chosen from the menu.

At any moment a "tabulation" (a movement) can be inserted at the beginning of a line, a chart with a specific number of lines and columns, an illustration created with a graphic software and then "imported", or created directly with the integrated illustration utility in a word-processing software (like most of the illustrations appearing in this text).

Searching for all the places a word appears in a text offers the possibility to have the word "replaced" successively, or throughout the entire text by a single command.
The word can be replaced by any given word placed in the dialogue window on the screen.

It is also possible to resort to several help functions: a thesaurus, a spell check or a (relatively efficient) grammar check.

The documents are designed in all the current page formats, or in whatever dimensions desired.
The margins can be adjusted to suit one's taste, and page numbering is automatic once requested.
Footnotes are placed and taken care of automatically.

Illustrations are imported (or created) and placed in the text with an integrated graphic function.

In fact, with certain limitations concerning the quality of the imported or created graphics, a text can be illustrated.
A piece of work corresponding to professional publication criteria will not, however, be spared the process of going through a page-setting software to turn out a layout, and a photo and graphics processing software programme for the illustrations.

Files created with word-processing softwares are recorded in a specific format.
In principle, they cannot be read on other platforms.
However, there are programmes which convert files from one format to another.
Some of these programmes are integrated in word-processing softwares, but the conversion is only relatively efficient, especially as regards illustrations.
A conversion software like MacLinkPlus is efficient in almost all cases.

Table 4

The main word processing programmes

2D graphics software programmes 

As we have seen, graphics software programmes are divided into two main categories: bitmap or vectorials, depending on the method of defining the picture.
They are also classified into two-dimensional (2D) or three-dimensional (3D) drawing softwares.
When an object is represented on a sheet of paper or a computer screen, it can be drawn in a fairly formal manner with perspective on a two-dimension flat surface, simulating the third dimension.
A software programme which enables us to do a normal drawing by hand is called a 2D graphics software.
Furthermore, we include in this category photo-processing softwares which usually have a certain number of drawing features.

The 2D drawing softwares have a drawing sheet and a fairly sophisticated and rich palette of tools, leaving the user scope for freehand drawing with a fairly thick pencil, fine or thick brush, water-colours, poster paints, charcoal, pastel, felt pens, etc. Depending on the number of colours afforded by the work station, it may be possible to work with 256 colours, thousands or millions of colours.
The colours can also be separated from a picture into different transparent layers (e.g. red-green-blue) like tracings, where the colours can be masked or mixed (to suit one's taste) in the super-imposition.
As for a text, any part of the drawing can be selected in a fairly detailed manner by using a selection tool, and then copied or cut and pasted elsewhere in the same drawing or an entirely different one.

Technical drawings can also be facilitated by a package outfitted with instruments for drawing geometrical shapes with specific bases (straight lines, Béziers curves, quadrilaterals, polygons, ellipses, etc.), which can be moved, superimposed, assembled, thrown out of shape, duplicated or coloured so as to represent the given object.
Texts, captions or titles can be added.

Photo-processing softwares enable the user to process photographs by modifying the dimensional parameters (dimensions, resolution, etc.), the photometric parameters (contrast brightness, etc.), the colourmetric parameters (tint, saturation, etc.), and to apply filters to them which modify their texture and appearance.
The user can also separate the different red-green-blue (RGB) or cyan-magenta-yellow-black (CMYB) layers, to make selections on one or a group of these layers in order to remove or modify an element in the picture.

Table 5

The main 2D graphics softwares

Seldom does a single software offer full scope to do all these types of drawings, paintings and picture processing.
In general, they are specialized either in painting-type illustrations or in line-drawing type illustrations or in photo processing.
The drawings are preferably saved in the format corresponding to their final use: PostScript, EPS, PICT, GIF, TIFF, etc. Not all the software programmes allow the user to convert all the formats.
However, there are specialized softwares for converting graphic files, such as DeBabelizer or Graph Converter.

The main graphic conversion programmes, apart from the features contained in the graphic programmes themselves are: DeBabelizer and Graphic Converter.

3D graphics software(picture synthesis) 

If the object to be represented is drawn with a 3D drawing software, it will be modelled in its three dimensions from simple geometric shapes called "primitive" shapes (straight lines, planes, cubes, spheres, cones, pyramids, etc.) which can be re-dimensioned, thrown out of shape and assembled and then placed in the space in relation to the required environment in order to constitute what is called a scene.

It is possible to use different kinds of lighting (direct lighting with a focused or enlarged, diffused, coloured beam, etc.); to give material attributes (glass, metal, wood, plastic, etc.), to apply a picture (photo or drawing) on a surface; to define the focus of the shot, for example.
It is possible, too, to ask the software programme to calculate a view of this scene under any angle by positioning the camera on any point.
The multitude of successive reflections of the light on all the surfaces will thus be calculated according to their nature and the different lights which illuminate the scene, so as to give it as realistic an appearance as possible.

Table 6

The main 3D graphic softwares

Generally, the picture of the model object is vectorial (as in a wire) and the final picture calculated by the computer is bitmap.
We shall also see how important this 3D concept is for creating motion pictures and for virtual reality.
In fact, certain 3D graphic softwares are supplied with features which allow them to calculate a series of pictures corresponding to the movement of the camera or of one or several objects according to a specific course.
The visualization of this succession of pictures restitutes the different movements and makes a motion picture.

Special effects software 

There are a certain number of so-called "special effects" softwares which allow the user to apply to the picture special effects of texture and materials, mosaic, hazy and solarization effects or distortions and metamorphoses.
These effects can be applied to a picture, but also on a sequence of pictures to create animation.

Page-setting software 

Page-setting softwares offer the professional features required to create printable documents.
They enable the user to define the format and the model document in the greatest detail, with maximum flexibility and precision: titles, margins, rules, columns, borders, backgrounds.
Texts, graphics, lettering and photo graphs are imported in their usual format in specific blocks, any number of which can be created, with their dimensions set, and moved anywhere inside the page, or from one page to another.

The inherent "rigidity" of word processors where the illustrations must be placed on lines gives way here to complete freedom as regards the position and the balance between the picture and the text blocks.

The page-setting software should allow the user to supply the printer, in the form of a digital file, with all the elements it needs to produce the printed document: the written and graphic contents, but also all the dimensions concerning the positioning of the items among them.
The presentation of a printed textual document is of great importance.
Much care must be taken in laying out the major parts.
The documents are set to be printed, but their conversion for a publication on World Wide Web servers is a major objective as we shall see further.
Certain page-setting softwares now carry this feature.

Integrated software 

Integrated software programmes are sets of software which communicate with one another and complement one another.
They usually include a word processor, a graphics software programme, a spreadsheet, a database and a communications software programme.
These tools are geared to the large market of individual users.

They are economical products with interesting features and performance, but they do not correspond to the level of professional software.

Table 7

The main special effects softwares

Character recognition software 

Once a document is digitized with a scanner, we have seen that it is saved in the form of a graphic bitmap-type file.
If it is a printed text, the letters can perhaps be read on the screen, but they will not be recognized by the computer as being ASCII characters, so they could not be used in a word processor.
To do so, this "picture" text will have to be translated by a character recognition software.
This software examines each letter and deduces the corresponding ASCII code.
In this way, a text can be recovered with around 99 per cent of success.
The spell check in a word processor can usually correct most mistakes.
So in this case it is not necessary to type printed texts on the keyboard if you have a scanner and a character recognition software.
This means saving time in proportion to the number of pages.

If the text is hand-written, the problem is different, more difficult, but not impossible to resolve.

There are software programmes based on certain concepts of artificial intelligence which can learn to identify the shape of handwritten letters produced by a given person and convert them into ASCII characters.
Before any recognition is possible, the software must be taught how to recognize the handwriting of the person who wrote the document.
An entire learning procedure is carried out beforehand to achieve this.

Table 8

The main page-setting softwares 

Presentation 

CAPre techniques offer the user scope to edit and present a document illustrated with various documents to an audience.
The clear and interesting nature of such a presentation helps participants to retain more than would be the case if only the classic talk were given.
To achieve this, the speaker will have had to do quite a job.
Starting with the text of his presentation, which may well be keyed in on a word processor, he must draw up the plan, the major issues, the main ideas, and the strong points of his argument, creating with the CAPre software at his disposal the screens he will be presenting to the participants in support of his argument.
He must design and produce his presentation before delivering it.

Producing a presentation 

Depending on the type of document he chooses as illustrations, he will need the suitable different digitized peripherals.
He will need a scanner to incorporate the texts, graphics or photos and possibly a digital photo camera to get shots which can be directly incorporated into the application.
If he chooses to include video and sound, he will need a microcomputer capable of managing this type of data, and then a video camera or video cassette recorder and a microphone.

The majority of presentation software programmes function on the principle of a series of screens.
The duration can be specified (and modified) as in automatic pilot, or left to the judgement of the presenter on manual gear.
In this way, at the right time during his presentation, he can freely request the screen to display what he wants when he feels it necessary.
Certain presentation software programmes offer a representation of the application being created with a series of screens on a temporal line.
Some also offer buttons which enable the user to link the contents among themselves and to pass from one screen to another.
They all offer transition effects among the screens in varying numbers: blendings, changes, sections, substitutions, etc. Most of them contain drawing and painting tools and graphic animation features.
ALL those shown in the following chart allow the user to incorporate sound, video and processed graphic animations imported in the application in different formats.
Some offer the possibility to create an application on one platform and eventually read it on another.

Figure 23

Diagram of a CAPre publication station. 

Models and features for borders, backgrounds, boxes, titles, models of histograms, shading effects, gradations or ready-to-use materials, are offered by these softwares to facilitate the creation of screens for persons who are not very experienced in graphics.
Some, such as Movieworks, also offer picture, video and sound processing features.

Table 9

The main computer-assisted presentation softwares

Overhead projection 

Most of these software programmes allow the user to print the transparencies corresponding to the different edited screens on a suitable office printer if he does not have a liquid crystal screen and the presentation will be taking place in several places, some of which are not equipped with multimedia projectors but only overhead projectors.

Liquid crystal screens are usually fairly compact and light enough to be carried about and connected to a desk-top or portable microcomputer.
However, it should be remembered that liquid crystal screens only transmit on average five per cent of the light which passes through it from the overhead projector.
A powerful overhead projector with at least 3,000 lumens is needed to have a sufficiently bright and clear picture on the projection screen.
This, of course, also depends on the size of the screen the room and the window of the panel itself.
The picture size is usually 640x480 points, but the window can be for example 8.4 or 10.4 inches diagonally.

Certain liquid crystal panels include a loudspeaker and a microphone and even tolerate remote interactivity control with an infra-red mouse, like any remote control.
Some also contain a video outlet to project a video cassette read by a video cassette recorder.
They all function on the different platforms with cables or suitable interfaces.

This kind of projection, rather economical, is particularly well suited to small setups, but the picture quality is not excellent because of light loss due to the overhead projection.

Figure 24

The overhead projection system

Table 10

The main liquid crystal projection panels

Multimedia video projection 

Multimedia video projection gives very good picture quality on up to four-metre base screens, in large rooms and before a packed audience.
Video projection is a technology which enjoys both the improvements of classic video projection and the soundness of liquid crystals.

Video projection of computerized pictures used to be done on 3-tube video picture projectors which were high in quality but very heavy, cumbersome and difficult to operate.
Furthermore, a costly interface was needed to convert the signal with adequate quality level.

Today, liquid crystal video projectors come together in a kit (heavy but transportable), offer similar quality, and are more economical to use and more reliable.

Liquid crystal panels or projectors do not only project computer-assisted presentation applications.
On the one hand, we have seen that they have a video outlet which enables them to project video from a camera or a video cassette recorder.
On the other hand, when connected to the central unit, they can project everything that appears on the screen of the microcomputer, on a CAPre application and any other application.
This means that any demonstration that takes place on a computer can thus be projected in actual time.

Figure 25

The multimedia projection system

Table 11

The main liquid crystal projectors

Table 12

The main screen activity capture softwares 

Yet there are some extremely useful softwares for training purposes which allow the user to record all the activity displayed on the screen when a particular task is carried out on an application software.
The result of this information on a screen of this type is a file which can be executed without the original application software: it looks like a film whereby the computer screen is represented in its entirety and where the mouse indicator can move, open a file or a document, move a dialogue window, etc. However, it should be noted that with this type of software, the video picture cannot be displayed regularly enough to allow for the normal video pace of 30 (NTSC) or 25 (PAL, SECAM) pictures per second.

On the other hand, the graphic image of the computer screen is perfectly displayed with no surge if the user has a microcomputer with average power.

Certain software programmes such as CameraMan even allow the user to add one or several sound tapes synchronized with the pictures for commentaries or sound illustrations.
Of course, these files can be projected as they are for teaching purposes.
Yet some of these software programmes also offer scope for video recordings which can also be used in the form of cassettes.
The same applies to certain CAPre softwares.

Projection constraints 

The distribution context should be the constant concern of the presentation application designer.
In fact, all illustrations and all lettering should be adapted to the viewing in the best conditions by all the members of the audience in all the venues where the presentation will be delivered.
No detail should be smaller than the minimal dimension visible from the spot in the room farthest from the screen.

When projecting, the screen should be clean and placed perpendicular to the axis of projection so that the picture is rectangular and does not become distorted.

The projector should be positioned so as to not hinder view of the screen.
If necessary, the room should be darkened so that no other light falls on the screen but that from the projector, so that the picture is striking in its contrast and the colours are not watered down.

Cassettes 

The cost of media, like the different formats of audio and video cassettes, has dropped and they have become popular for distributing audio and audiovisual documents.
They are extremely flexible to use, easy to produce and particularly adapted to producing teaching aids.
This has been especially so since digital technologies developed new production features which place these tools at the disposal of everyone, enriching the contents, lowering the prices and further improving the quality.

The audio cassette (like the video cassette) is an efficient and practical response to the reel of magnetic tape which can accidentally unwind, become soiled and which had to be loaded correctly in the machine following a number of rather complex steps.
The cassette protects the tape, and shelters it from accidents and dust in its closed casing.
It automates and secures the complex process of loading into the player-recorder.
It can be tagged, wrapped and filed in a compact form.

Sound 

Sound is the natural vector of oral communication.
In this regard, its role is paramount in the pedagogical and emotional relationship which is established between teacher and learner.
The tone of the voice, its rhythm and music carry specific effects which have a powerful impact on the transmission of knowledge.

Sound processing techniques involving radio broadcasting and the recording of wax cylinders started to be mastered rather early at the end of the last century.
Ever since, after the 78 speed single record, the long-play record and the magnetic tape, the magnetic audio cassette became a universal standard.
Other audio aids appeared later on the market such as the CDAudio, which engendered the entire Compact Disk family.

There has been steady progress in the broadcasting of radio programmes in the world, sometimes giving rise to important teaching applications, mainly as regards the teaching of languages.
Yet specific pedagogical radio programmes are rather often used in certain contexts which offer scope, with modest means (radio receptors are very popular, even in the poorest countries), to reach ambitious goals with a very large audience.

Analogue sound recording and processing techniques have long remained stable around the legendary portable Nagra and then Uher cassette players which provided very high quality recording in the most difficult conditions.

Editing on magnetic tape reels can take place by manually locating the sounds on the tape, or by scissors cutting and then pasting with adhesive tape.

Ever since the beginning of the 1960s when it appeared, the audio cassette meant significant progress for broadcasting recorded tapes and for more popular use of audio recording.
Since copies had to be used for editing, its use became restricted to downmarket operations.
However, there are professional cassette recorders today which are reasonably priced, which fit properly into professional environments and produce audio documents of excellent quality.

The technology of microphones, loudspeakers, mixers, amplifiers and other analogue studio up-market equipment has also progressed by leaps and bounds in terms of quality and miniaturization.
This gave rise in the 1970s to a flourishing industry (mainly Japanese) and a global market of domestic (and professional) audio material.
Radio receptors, stereo system record players with amplifiers, loudspeakers and cassette players were incorporated in the form of connectable modules which could be configured to suit each taste, and offered to tempt consumers.
Having become international giants of the industry, the main actors in this market next found themselves on the television and video market, then on the market of domestic applications derived from microcomputers.

The audio cassette is often used as a notebook for students who record their courses in this way.
All the same, the production of audio teaching cassettes involves, however, a technical process which, as we have already seen calls for tested professional skills and specialized material.

Generally, original documents are recorded on excellent quality material, and possibly on a DAT (Digital Audio Tape) digital technology machine.
Editing can also take place on digital material, to be copied at the end of the production process on an analogue audio cassette to be broadcast.
The microcomputer is particularly to be recommended for editing and sound processing.
The quality of the original is preserved and the processing, mixing and editing features are extremely powerful and produce very sophisticated documents.

Figure 26

Diagram of a sound editing station 

The microcomputer and sound processing 

The microcomputer thus has entered forcefully into the world of audio document production.
Today, it is in the process of supplanting the cassette player as a sound recording medium.
The microcomputer has the advantage over the cassette player of being able to process sound as well as to edit it, mix it and file it on any medium without altering the quality (except improving it).
For the time being, portable computers are not used to capture external sound, but it is common to record in the studio onto the hard drive of a microcomputer.

Due to the fact that the Macintosh incorporates standard sound management, it was for a long time the only platform which could process sound.
Several professional cards appeared on this platform to digitize sound and to interface different elements of the sound creation chain in and out.
But even if the Macintosh (and especially the PowerMac) remains a favourite medium in this field the 1990s have borne the stamp of rapid development of sound cards for standard MPC PCs.
Yet, today, the power and architecture of certain Macintosh (from the AV range) and PowerMac allow the user to digitize and process sound without an expansion board, using only one piece of extremely powerful, professional software: Deck II from the OSC Company.

These cards and systems add features to the different Macintosh and PowerMac models.
Certain cards can only be used to digitize, others are MIDI standard synthesizers, others still are samplers.
Certain systems are external modules which are connected to a Macintosh and which allow the user to obtain very high quality processing.

Table 13

Sound digitization and processing cards and systems for Macintosh and PowerMac

Table 14

A selection of sound cards for IBM PC compatible (MPC II standard) 

At the outset, IBM PC-compatible computers did not usually have audio features.
Sound expansion boards were designed by different suppliers to add digitization or sound processing features to them.
The MPC II standard established by Microsoft specifies sound sampling on 16 bits at 44.1 kHz.
These cards allow the user to digitize sound, control a CD-ROM player and often accommodate a rather high-performance sound synthesizer.

There is quite a collection and it is not always easy to find the one which suits the system you use and the application you wish.
They are often difficult to install and configure.
It is hoped that the new version of the Windows 95 operating system will really facilitate the integration of peripherals through the introduction of the "plug and play" concept.

Sound-processing and editing softwares allow the user to digitize and save them in the form of audio files, to display them in a win dow in different forms to represent the characteristics in them and to modify the physical and acoustic properties.
The editing features allow the user to copy, cut and paste any part of the selected file, exactly as texts or graphics to incorporate it elsewhere in the same file or in another.

The possibility of working on several tracks allows the user to mix different sounds, such as pieces of music, commentaries or interferences and to adjust them to each other.

Table 15

The main sound processing and editing softwares 

All these material tools or software programmes offer scope for the design and production of very high quality and very powerful sound tapes which can be recorded on audio or video cassettes at the end of the editing process, but also on any suitable material digital medium such as CD-Audio and CDROM, or even on-line on networks as we shall see further.

Video 

The simultaneous recording of picture and sound on magnetic tape is a technology that dates back to the beginning of the 1950s, the result of the merge between television (which, until then, could only be seen live) and audio recording.
The first video magnetic tapes in reels measured almost 50 cm in diameter and two inches in width.
Cassette players were huge metallic coffers.

Video-cameras as television products were very heavy and the entire system was extremely difficult to manage and maintain in good working condition.
Until the beginning of the 1970s, editing was done manually through random copying.

The first portable, colour, so-called "professional" video cassette recorder - the U'Matic - despite its limited picture quality, appeared in the mid-1970s as a veritable revolution.
It offered scope for a fair automation of editing, especially due to the 3/4 inch video cassette.
The technological strides which launched the development of computerization found applications, of course, on new markets, such as the video market for the public at large.
The definition of these standards towards the end of the 1970s gave rise to a historic feud between the promoters of Betamax (from Sony) and VHS (from JVC-Matsushita).
The V 2000 standard from the European consoffium Philips-Thomson had come too late.
We know that VHS won the fight due to its marketing policy.
Its promoters had chosen to share the operating licence as widely as possible with the majority of television, electrical and home appliances manufacturers.
Today, video cassette recorders range in the dozens of millions and are still making headway.

Sony had to recognize its failure in this sector of the market.
But by improving Betamax (renamed Betacam ) and placing it in an upmarket professional standard used by all television stations, Sony took over a new market the broadcast market where Betacam reigns supreme today.
The counter attack of the Matsushita group to distribute its professional broadcast standard M2 was somewhat of a failure commercially.

In the 1980s, as regards camcorders for the general public which carry camera, microphone and video cassette player in a single machine, these self-same electronic giants were divided into two camps exactly as before to define the new standards.
Sony launched the V8 mm to which Matsushita retorted with the S-VHS , against which Sony proposed the H18 .
In short, today on the general public market we can find a wide range of picture and sound recording machines in the VHS, V8mm, S-VHS and H18 formats with unequal output depending on the models, but often very satisfactory in the best of cases.
For professional and especially training needs it is known that the SVHS and the H18 are the accepted standards.
There is a range of equipment in each of these two standards which conform to professional standards.

For productions to be broadcast on television, it is even usual today in difficult filming conditions where weight and bulk need to be reduced (or for economical reasons) to do reporting in these light formats and finalize the editing on Betacam SP .

In the context of training, if the video is required to be in an autoscope configuration, it is recommended that the VHS format be used for economic reasons.
If the task consists of collating pictures and sounds which need to be edited, the S-VHS or H18 formats are favourites, for they allow one or two generations (copy and second copy for the editing) without drastic loss in quality.
If a high-quality picture is necessary for the broadcast, after an original recording on S-VHS or H18, the editing can be carried out on a superior quality standard.

Viewing the original, selecting the useful sequences and designing the editing demands several runs of the tape.
To do so, it is recommended that you copy the piece on VHS, with the coded time clearly displayed at the bottom of the picture.
This copy can be run and re-run with no qualms, for the original is preserved.
The clearly indicated coded time allows the user to note accurately by reading the coded time on the screen all the in and out points of each selected sequence.
The editing model is thus carried out on this copy.
Once the model is finished and tested, the final inputs and outputs of the set-up are noted and the final editing can take place to obtain the film matrix.
From this matrix, all the necessary copies which will be broadcast shall be made.

Figure 27

Diagram of a video editing configuration and standards panel 

The editing is usually carried out on a mechanism made up of two video cassette recorders controlled by a command panel.
This command panel allows the user to programme the coded time of the input and output points of each sequence on a keyboard and to launch the copy operation, layout by layout, one after the other in this way.
Certain command panels allow the user to control two players to mix the pictures and obtain superimposed or successive blending effects.
This process can be computerized as is illustrated in Fig. 27.
There are types of software which very accurately control video cassette recorders and on which it is possible to programme a great many input and output points to edit automatically and have transition video effects.
Editing the sound of the recording of the original video cassette is also carried out in the same way, on sound tracks.

Music and commentaries can be introduced successively on the free sound track.
Yet a sound mixing panel can also be connected to this mechanism to make mixing easier.

The H18 format provides excellent picture and sound quality, especially on professional camcorders, but the magnetic tape of the cassette is fragile since it is very thin and narrow (8mm).
The S-VHS format provides an altogether similar quality on equivalent machines, on a more sturdy and wider tape (1/2 inch) on compact or normal cassettes.
Yet the material is sometimes less reliable depending on the brand.
Furthermore, there is more and more compatibility with the VHS format (any VHS cassette can be played on an S-VHS machine, but not vice versa).
This leaves scope for incorporating, if need be, VHS elements into SVHS documents.

- optical and signal processing quality;
- characteristics, the number and resolution of device cells for charge transfer (CCD) which define the picture; 
- microphone performance; 
- possibility to release the automatic functioning to allow manual regulation of all the desirable parameters; 
- presence of BNC or DIN connectors; 
- sturdiness, ergonomics, weight and reliability of the equipment. 


As regards professional video cassette players, they are sturdy, equipped with BNC connectors and a RS232 or RS422 interface which means that they can be controlled from a command panel or a computer.
They often come with a so-called "time base corrector" (TBC) mechanism which corrects minor picture defects (drops) as may arise due to accidental imperfections in the tape or the signal.

Over the last years, the VHS cassette has become an extremely popular film editing aid which has invaded the specialized shelves of supermarkets and shops specializing in cultural products.
Editing didactic video programmes made strides on this medium, not only in the field of language learning but also for programming and specific computer software training.
Quite a few collections are available in this field.
Documentary programmes with an educational purpose are also edited on physical sciences, human sciences, medicine, maternity, animals, vegetation and many other subjects.
Universities or scientific associations also publish courses or didactic materials in very advanced technological fields on video cassettes.

Alongside these developments, video recording of courses or lectures and diverse teaching services is becoming more and more widespread for reference and filing needs.
Autoscopy is also widely used in teaching situations.
Nothing can compare with the services which the video can render in this field.
Before, there was no medium which could in this way record in a box the picture and sound of a delivery with a certain time frequency (25 to 30 pictures per second on normal video, but also one picture per minute or per hour, etc.).
In this way, the learner can observe the manner in which he delivers his presentation and he can fine-tune it to perfection.
The trainer can use it to refine the breakdown of the operations, or the description of the objects and mechanisms in question.

The problem of data compression 

It was only in the beginning of the 1990s that video digitization really became possible on microcomputers.
To achieve this, several problems had to be resolved.
The enormous quantity of data represented by the video display on a full screen, 25 or 30 pictures per second in millions of colours, was one of the major problems which had to be addressed in terms of the processing power of the then available microprocessors.

Sound management and its synchronization with picture was another problem.
A proper solution had to be found to enter, process and exit the audio and video signals, but the operating systems also had to be adapted to this type of data processing which involves constraints that are hard to eliminate.

The central unit's microprocessor operates within the limits of its processing power.
The only way to let it process the quantity of data needed to have a simultaneous, acceptable display of video and sound is to reduce this quantity of data which codes the different parameters as much as possible.
It is possible to reduce the size and number of pictures to be displayed per second as well as the number of colours.
But this is still not enough.
In fact, video digitization mechanisms are based on the principle of data compression.
It is a matter of eliminating all the redundant data in the picture file, keeping only the pertinent ones.
A minute of full screen video in millions of colours is the equivalent of a digital file of around 1,700 megabytes of data.
Compressing this minute in a factor of 1/20 means cutting down a file to the much more manageable size of 85 megabytes.

The first solution consists of replacing the coding system of several contiguous, identical (or similar) pixels with a single datum.
The JPEG (Joint Photographic Expert Group) standard of photo compression works in this way.
With this standard very good results can be obtained in terms of quality for still pictures even though there is loss of pertinent information.
M-JPEG (Motion-JPEG) is a version of this standard adapted to video picture compression, but the resulting files are still rather large.
However, it is possible to process this type of file on rather powerful microcomputer configurations.
Each video frame is described by a bitmap file, the picture quality is excellent and can reach full screen size.
Yet once processing is over, the new file can again be compressed with other compression algorithms to reduce its size without altering the output too much.

The most useful ones currently are Cinepack, Indeo and TrueMotion.

The MPEG standard 

Yet there is another method for video compression which is less high-performance than the M-PEG, but more adapted to wide broadcasting.
It consists of deleting all the elements which are similar in a succession of pictures, coding only the difference between the pictures.
It is a very interesting technique, reducing as it does the quantity of data needed to code the video by a ratio of 1/100.
It is used in the MPEG 1 (Motion Picture Expert Group) standard, which was designed to broadcast digital video applications on general public platforms.
- at 30 pictures per second (704 x 480) with the American television standard and 
- at 25 pictures per second (704 x 576) with the European television standard, 
with a rather good resolution on microcomputers equipped with MPEG 1 decompression cards.
Another even more advanced version of this standard, MPEG 2 , gives even better picture quality.
So far, there are few MPEG 2 decompression cards, but decompression programmes which do not need decompression cards are starting to appear.
In any case, the best results for viewing video sequences are obtained on the most powerful microcomputers.

The operating system extensions needed to manage digital video now exist on the different platforms.
This guarantees all the systems compatible with this standard and equipped with extensions the capacity to run the corresponding video files.
It should be noted that QuickTime, the extension of the MacOS operating system, is trans-platformable.
In fact, QuickTime allows the user to edit a video application on one platform and transfer it to be used on another.

As in the case of text, graphics and sound, there are also several useful programmes for video which allow the user to convert the files edited in one format to another format.
In this way, it is possible, for example, to convert (taking certain constraints into account) a QuickTime file to MPEG1 or MPEG2 and vice versa with the Sparkle programme available in freeware on Internet.

Table 16

Operating system extensions for digital video 

Digital video application production systems 

Digitizing video and sound can be carried out on the most powerful microcomputers which contain hardware and software digitization features.
We recommend the most powerful microprocessors on the market: the 130 megaHertz Pentium or the P6 for IBM PC compatible computers; the 601 to 110 megaHertz PowerPC or the PowerPC 604 and 620 for PowerMac.
Digitization cards will preferably be chosen among those functioning on the most rapid buses, such as the PCI bus which now figures on the majority of platforms and offers scope for a transfer rate of around 100 megabytes per second.
However, certain tested solutions work very well on the traditional buses of the different platforms (ISA or NuBUS).
There will never be enough RAM: 20 megabytes of RAM are a minimum.
It is better to have 64 megabytes to work at ease.
Furthermore, the RAM is much quicker than the hard drives, and the digitization will be much more fluid if it is possible in the RAM.

In fact, as we have already mentioned, the quantity of data which code the digital picture and sound is quite considerable.
This data must be recorded, transferred and retrieved at just as high speeds in order to come on display at the required frequency.
Inscribing this data on hard drives should be a non-stop process with no breaks due to the automatic maintenance routines which usually operate on all drives.
It follows that not only are very high capacity hard drives necessary (2 gigabytes and above, 9 gigabytes are frequent), but in addition they must also provide very rapid access time, with transfer rates above 5 megabytes per second, and allow uninterrupted recording of the data.
Hard drives of this kind correspond to what certain manufacturers call the AV standard.

Figure 28

Diagram of a digital video station 

For applications such as video, transfer rates authorized by the buses and the interfaces may appear as bottlenecks which limit data flow.
The SCSI interface, for example, only allows 1.5 megabytes per second while for the full screen video the minimum is 3.5 megabytes per second.
With an expansion board it is possible to generate a SCSI-2 bus (Fast '&' Wide) which allows for rates between 10 and 20 megabytes per second.
Certain platforms are outfitted with an SCSI-2 standard bus.
If the majority of microcomputer manufacturers adopted PCI buses, this problem would be solved by allowing a rate around 100 megabytes per second.

The hard drives 

The configurations of RAID (Redundant Arrays of Inexpensive Disks) hard drives offer scope for very high transfer rates.
The management software for these configurations divides the inscription of the data on several disks successively or simultaneously in order to speed up and guarantee the safety of process according to eight methods offering different levels of security and transfer rates.

Table 17

The main video digitization cards 

If the performance of the editing (or final use) platform is not optimal, the size of the pictures and their frequency (at the risk of causing jerks) must be reduced or the compression rate increased during the final compilation file.

Digitization cards 

Digitization cards are characterized by their performance in terms of power, features and the quality of the connection.
Certain cards process video and sound and are equipped with inputs and outputs.
Others process only video or only have inputs.
The size of the video window depending on the case may be: 160x120, 320x240 or 640x480 pixels in NTSC (American standard of television coding).
Digitization of the picture can be carried out in millions of colours, in thousands or 256 colours, at 30 pictures per second or at 60 frames per second in NTSC (30 pictures or 50 frames in PAL).
Digitization of sound can be carried out on 8 or 16 bits.
Great differences may be observed from one card to another.

The video and/or audio signals provided by a video cassette recorder, a camcorder, a television set, a cassette recorder or a sound amplifier are thus received in input and processed by the digitization card.

A software related to this card allows the user to define the digitization parameters and to launch the picture and sound capture in order to save them in the form of a compressed file in the corresponding format.

Video editing softwares 

These capture softwares are usually included in video processing programmes which also carry other features for editing, regulating the photometric parameters of the picture, generating transition effects between the sequences, applying filters to the entire sequence or a part of it, etc. The film director (or editor) must adjust the length of the video sequences by defining the entry and exit points of each layout in a close-up by eventually basing himself on the SMPTE standard coded time which gives a temporary digital code to each picture.

Each sequence used in the editing will be imported on the "editing panel".
As in the case of sound, the video is represented on the screen in a window in the form of a succession of "little pictures", the number of which is proportional to the length of the sequence in question.
Several video tracks may be superimposed and special effects may be used to pass from one sequence to another.
It is possible to apply a filter to any selected segment to modify the appearance of the picture.
The sound is under the picture on one or several tracks which can be adjusted in terms of volume or length; they can be mixed, brought forward, shortened or extended to correspond exactly to the corresponding pictures.

Table 18

The main video editing software programmes 

Before finally saving a new digital file, the actual editing done can be visualized in a window.

Additional adjustments can then be made, alternating from one to the other until you are satisfied.
So the final file of the edited sequence may be compiled in the form of a digital film.
To do so, the window size, the compression algorithms and the number of pictures per second should be chosen and adjusted so as to obtain the best match between power and performance of the final operating platform, size of the file and the best quality possible.

Some of these software programmes allow the user to synchronize the picture display on the coded time while the video cassette is being played or recorded.

But, on the one hand, if the video cassette recorder is to be controlled by the computer, it must contain an RS 232 or RS 422 outlet.
On the other hand, the user must also have a machine control system.
This system may be composed of an expansion board and a piloting software programme, or simply a software programme.
If it is outfitted with this kind of mechanism, the microcomputer tends to take the shape of an editing system.
However, there are coherent systems which come in kits containing all the necessary elements for making a digital video editing system.
The distinction should be made between off-line editing systems (otherwise known as virtual editing systems) and on-line editing systems (computer-controlled).

On-line video editing systems 

On-line video editing systems are derived from video cassette recorder piloting systems.
They are made up of a group of expansion boards, cables and specific software programmes which allow the user to pilot one or several video cassette players and a video cassette recorder (see figure) in order to edit classic video sequences without digitization.
This is traditional video editing, but controlled by a computer.
The main limitation of this method is that the editing is carried out by copying and with each copy the quality of the signal drops.

Figure 29

Diagram of an on-line video editing configuration 

On-line editing systems are targeted to different clients.
On the one hand, there are very economical systems which allow the user to carry out "amateur" video and sound editing with very few tools: a home video cassette recorder, a camcorder and a home computer.
These systems are usually not very reliable, but very useful.

Moreover, an entire range of professional systems are in fierce competition in terms of features with the classic video editing systems, but they are much more economical.
There are also a few configurations which have up-market features that are not included in traditional analogue video systems.

Off-line video editing systems 

Table 19

The main on-line video editing systems 

Off-line editing systems are groups of software programmes, cables and expansion boards which allow the user to set up a complete editing system with a microcomputer and a group of specific peripherals.
With this station, he can digitize and edit digital video sequences from sequences recorded on a video cassette.

The difference with on-line editing systems lies in this digitization: the video is processed by the computer.

This digital processing offers the asset of not altering the quality of the picture nor the sound, however complex, unlike classic analogue processing required with on-line editing systems.

The video sequences are digitized successively, either automatically, in batches, after locating the coded time for the beginning and end of each layout or manually.
The shots are edited and mounted.
Special effects, sounds and titles are added.
They are viewed before being collated, rather as we saw in the previous chapter.
Once the sequence has been edited in this way, it can then be recorded very accurately on a video cassette.
Several sequences can then be assembled in this way, one after the other, to make a film.

Table 20

The main off-line video editing systems

These kits offer features and performances which the grouping of elements from different sources does not permit.
They enjoy the advantage of being coherent and powerful to the extent that some users are able to work in actual time when viewing editing and effects.
The editing softwares they contain are perfectly adapted to the hardware configuration, and this optimizes the processing time which is often long on composite systems.

But the major advantage is that these nonlinear systems allow the user to import text, graphics, photo, sound and video files developed beforehand or at the same time, in the majority of formats corresponding to their platform.
These mechanisms are particularly economical, high-performance and functional for the production of very high quality video documents for training purposes and most other television or audio-visual programme production needs.
Their modular form allows them to be perfectly incorporated between the digital editing set-up for graphic, photographic and sound documents on the one hand, and the traditional editing technologies for video cassettes.
Use of techniques of this kind lowers considerably the production costs of video document production, and places these features at everyone's disposal.

Optical media 

Earlier, we introduced the large family of compact disks which have only recently become quite distinct editing aids whose performance equals their commercial success.
Any group of digital files can be published on a compact disk.

Figure 30

Compact disk editing system 

To do so, all one needs is a rather powerful microcomputer with a high storage capacity on the different standards of peripheral supports linked through a rapid SCSI set-up: a stable, high-capacity hard drive, a SyQuest cartridge player-recorder, a magneto-optical diskette player-recorder, a DAT cassette player-recorder and a compact disk player-recorder, with the different formatting softwares corresponding to the different disk formats: CD-Audio, CDROM, Photo-CD, CD-Video and CD-i.

Obviously, the user needs to have the different digital files which were developed in the proper format in order to edit them with the selected editing softwares.
The presentation and the ergonomics of the graphic interface should be very carefully designed so as to respond to the user's questions and needs, and make the location process, assimilation and incorporation of the contents easier for him.

So compact disk is edited in this way on a single copy in order to be tested and evaluated on the platforms for which the application was designed.
After corrections or necessary adjustments are made, the file is transmitted in the required form to the duplication laboratory which produces the number of required copies.

Using the file it is given, the processor produces a glass mould in spotlessly clean conditions.
This glass mould is used to duplicate the user copies in whatever number required.
It may be noted that currently, formatting 600 megabytes of CD-ROM data costs less than US$ 2,000.
Run off in series of 1000, each disk costs less than US$ 2.00.
Finishing touches - printing of labels or an accompanying booklet, and packaging - must be added to the cost.

To run it, all you must do is simply insert the disk in the drive.
Playing the first CDROMs was a slow process.
They worked at 250 turns per minute and the access time to the data was close to 360 milli-seconds, with a pace of only 150 kilobytes per second.
But today there are drives which work two, three, four or even six times faster than the earlier ones, with a pace of 900 kilobytes per second.
Twice the speed and upwards, the pace is sufficient to provide a smooth playing of video files.
The CD-ROM player is becoming more and more a part of the microcomputer.

The specifics for producing different compact disk formats 

Each type of compact disk is determined by a series of characteristics peculiar to its history, to the media it is used to edit and to its utilization.
We shall try to make the distinction between them.

The CD-Audio 

The CD-Audio was the first arrival, designed by Philips in the late 1970s in a document called the "red book".
It has enjoyed outstanding commercial success as a musical editing aid ever since it was launched.
The production of these disks by hundreds of millions gave rise to a booming industry which made this editing technology popular and even a household name due to a drop in production costs.
The same applied to the manufacture of players which to date are sold by the scores of millions and are very cheap.

Any digital sound file developed on a microcomputer or a dedicated digital station can be published on a CD-Audio using a microcomputer connected to a CD recorder which, in turn, is equipped with the proper formatting software.

The CD-ROM 

The Yellow Book published by Philips defines the specifications of the physical organization of the data on the CD-ROM.
But the developer is free to choose in terms of the structure of the directory and files and formatting of data.
There are three CD-ROM standards: first, the ISO 9660 adopted in 1987, which provides a file structure which allows the data to be accessible from all platforms.
The two other versions of the ISO 9660 are FS (Hierarchial File System) for the Mac platform and UFS (Unix File System) which corresponds to the Unix platform.

Different versions of the same application can be carried on the same disk.
This divides the disk capacity in two for the programme part, but the advantage is that it can be played indiscriminately on several platforms.

The part dealing with video, photo, graphic, text or sound files can be shared by the different versions if they are in a format which is common to the two platforms.

CD-ROM has become an accepted standard for multimedia publication.
It is used as a medium for all kinds of applications, from games to specialized periodical magazines, as well as encyclopaedias, teaching programmes, museum visits, etc. Today there are several thousands of different titles sold all over the world, and each year their number doubles.
It is very easy to update the contents and each year a new version of the same title may appear.

The Photo-CD 

The Photo-CD is a data editing format which was designed by Kodak in collaboration with Philips in the early 90s.
The aim is to provide professional or amateur photographers with a tool they can use to process and edit their photographs on a range of digital media compatible with the practices which have been developed for photographers in the different fields of editing.

The Photo-CD is produced from negatives, slides or paper copies.
The photos are digitized on specialized transfer stations designed by Kodak.
These stations are used in photo laboratories which wish to advance in this field.
At the end of the process, one (or several) PhotoCDs called "Master" is delivered to the client.

Figure 31

The Photo-CD editing system

It may contain up to 100 pictures which are available on this disk in the five different formats: Base 16, Base 4, Base, 4 Base, 16 Base, which lend themselves to the five corresponding types of applications.
Extremely high quality paper copies can be made using the PhotoCD on a special printer.

The basic principle consists in first defining file formats which would ensure a digitization of the photos with high-quality reproduction depending on their final use.

Table 21

The different Photo-CD formats

This Photo-CD can be operated according to two methods which correspond to amateur or professional use.

In the first case, the user needs only to insert the disk in the Photo-CD (or CD-i) player which is connected to the television set to view the photos with a remote control.
The quality and viewing comfort offered are amazing.
The creative recentring, zoom, automatic display or programming features offer scope to create interactive applications.

In the second case, processing, editing and using the photos take place on a microcomputer with a Photo-CD compatible CD-ROM player (as is the case with the majority of the new models).
This permits the user to obtain photos which have been digitized in excellent conditions on this medium, and which can be used in the different file sizes corresponding to the different applications possible.

Several types of Photo-CDs are possible depending on the end purpose of the applications.
We talked about the Master PhotoCD , but there is also the Master Pro PhotoCD which can hold 25 pictures in the 64 Base file format (4096 x 6144 pixels) with a copyright notice and a picture protection system against pirating.
There is also the Photo-CD Catalogue which allows the user to catalogue up to 10,000 pictures in the Base 16 format (128 x 192 pixels).
Sound, text and graphics can be added.
Each picture is identified by a number of key words which allow the user to carry out picture searches per subject using the computer.

We shall also mention another format: the Photo-CD Portfolio .
It is particularly interesting for us within the perspective we have developed.
It allows the user to create interactive multimedia applications from photos which have already been digitized on a Photo-CD disk or from any 512 x 768 pixels picture with the TIFF or PICT computer formats.
It may contain up to 700 pictures of this size (or a combination of 400 pictures and 20 minutes of sound, for example).

Using Kodak's Arrange it software, it is possible to create the tree diagram structure which will define the possible application routes, and to include photos, sounds, texts and graphics which may be derived from current picture and sound design or processing softwares.
Once the model has been tested and validated, it is compiled in the form of a single file which will be used to press the final Photo-CD Portfolio, which can then be used on any Photo-CD player.
However, this final operation is only possible using an available integrated station in a specialized laboratory which has signed a license agreement with Kodak.
Yet direct operation of a multimedia application of this kind is possible on CD-ROM in small series edited on the Kodak PCD 200 player-recorder, for example, or any equivalent machine as seen in fig. 31.

The CD-i 

CD-i is the abbreviation for Compact Disk Interactive.
It is a standard derived from the CD-ROM in terms of its specifications.
The essential difference lies in the fact that it is used on a specific player connected directly to a television set.
There is no need to resort to a microcomputer to pilot the disk.
This simplifies its use considerably- anyone can insert the disk in the drive, as with a CDAudio, take the remote control and navigate instantly in the application.

Figure 32

The CD-i system

This format was designed as early as 1987 jointly by Philips and Sony to be used as a medium for interactive multimedia applications targeted to a very wide public.
At the outset, the constraints due to restrictions of compression technologies only allowed the video to be displayed on one-eighth of the screen.
The catalogue of available titles has been considerably enhanced over the last years.
The number of players has increased a great deal, especially in Japan, where the range sold around 1 million copies thanks to karaoke titles.

The CD-i player plays the CD-Audio, but it also plays the Photo-CD.
This means that the CD-i player is a machine full of resources which should make its way to all homes and perhaps also offices, or training rooms if this medium does, in fact, become accessible to professional applications.

The titles currently available in Europe are classified in six categories: 25 titles in Early Education, just as many for Games, a dozen titles in Leisure, just as many for Culture, Music and Film.
Yet the catalogue grows daily and the supply ought to hold more and more appeal for the general public.
Alongside these strides, professional applications can also be developed in greater numbers on this medium when the model and production methods of applications on CD-i become more accessible.

For the while, the CD-ROM still seems to be a more competitive medium for this kind of market, for it can be completely mastered internally at rather reasonable costs if one remains within certain acceptable limits as far as professional needs go.
This is not the case with the CD-i, which resorts to specific mechanisms which have not yet become very common and which are still rather costly to use.
Engaging specialized services greatly increases the costs.

The CD-Video 

Recently, a module which fits into the back of the CD-i player came on the scene, complementing the configuration and making it possible to display compressed video in the MPEG format on the entire screen.

This module is built into the new models of CD-i players.

It is now possible to record up to 70 minutes of full screen video on a CD-i.
With the new generation of compression standards and the new compact disk formats from the new generation we spoke of earlier, it will again be possible to increase the duration of video on a CD.

CD recording systems 

Recordable disks are cut by a laser ray which modifies at precise intervals in actual time the structure of the recording layer in order to code the information.
The life-span of these disks is between 10 and 100 years, depending on the quality.
They can be played on any player corresponding to the platform and to the format for which they were edited (CD-Audio, CDROM, Photo-CD, CD-i and CD-Video).

Compact Disks recording software programmes each have particular features which enable them to engrave CD-ROMs just as well for the HFS standard Macintosh as for the ISO 9660 standard PC or mixed CD-ROMs which can be played indiscriminately by both.
They can even record CD-Audios, CD-is, CD-Videos or even be used to record data as a backup.
These softwares may be compatible with different types of player-recorders or, on the contrary, may correspond to only one player.

The player-recorder is an SCSI peripheral which is known for its rotation speed (two, four or six times faster than the normal CD-ROM rotation) which increases all the more the quantity of data transferred per second.

This accelerates all the more the recording speed of a disc.
Players which are six times faster record a CDROM in about ten minutes.
To record small series, the same file can be recorded simultaneously on several recorders connected to the SCSI set-up.

But before recording a Compact Disk, the user must have at his disposal the application he has to record.

Multimedia applications design software programmes 

In the section on production methodology we saw that designing the application usually means a scenario which defines precisely the general architecture of the application, all the contents as well as the features offered to the user and possibly to the evaluator.
So at this point the documents corresponding to the contents to be communicated are available in different forms.
The first task will have been to digitize and save them in the form of a digital file in the appropriate formats corresponding to the way they need to be incorporated and their final use.

The next phase consists of putting these documents together in the form of a single application to fit the scenario so as to offer the user all the tracking features of these documents needed for proper transmission of the message.
The choice of design software for this multimedia application (sometimes called system or author software) is strategic and of prime importance.
This choice will have been made before researching or designing the contents and will have been determined after analysing several factors concerning the type of application, features offered to the user, publication medium, and average performance of the targeted platforms.

Is the editing platform at our disposal adapted to the type of application we want to create, and to the dissemination platform we are targeting? 

First of all, the editing functions and the performance of the editing system must tally with the expected results.
But it is important not to make a mistake, to safeguard the investment agreed to in the design of this application in order to adapt it to all the desired media and platforms in the future.
Nowadays, multimedia applications design software programmes are very advanced - the user can edit an application on one platform and convert it for use on others.
It is said that multimedia applications design softwares are trans-platformable, so making the right choice is crucial.
An application designed on a platform with a limited performance author system to be used on a platform with more sophisticated features and output will produce disappointing results.
It is of the utmost importance not to make the wrong choice.

Table 22

The main optical compact disc recording softwares 

Table 23

The main multimedia applications design softwares 

The choice of software is critical, for it defines the design features offered to the designer by the author system, such as design features, document processing or editing features, for example.
Of course the documents would have been designed beforehand, but their collation implies a minimum of design features in order put it all together.
So some software programmes are better outfitted with editing tools than others.

Moreover, the design itself of the author system is based on a concept, a metaphor which guides its utilization and dictates its functioning logic.
This manner of functioning is more or less adapted to the mentality and work methods of the author who will be using it, but will also suit more or less the type of application and the platform in mind.

The most traditional metaphor in the world of computers is the programme, the script , the list of instructions given to the processor so it can execute them.
It is the most difficult one for the author (who must first master the language) to use.
It is the most elementary, but the most reliable.

All the other metaphors revert to this solution, but they enable non-computer scientists to edit multimedia applications rather easily by accessing the objects on the screen with the mouse.

Accessing the objects is in itself a metaphor, since the elements to be incorporated may be considered as objects.
The combinations of icons and diagrams or of organization charts provide a very practical logic for representing the general architecture of the application or for locating the desired element and modifying the desired parameters.

The metaphor of screens and of score is beyond doubt the one which conquered the designers of the most famous products currently on the market through the Director software programme.
This applies to CD-ROMs on all the platforms as well as game consoles.
At the outset, this software enjoyed the features of the Macintosh platform for which it was developed; furthermore, it is very reliable.
Very soon, it was possible to convert applications to be able to use them on PCs.
Now, Director can be used straight on PCs in the Windows programme.
This metaphor consists of defining screens by organizing the different elements inside and of predicting, as on a musical score, the succession in time of these screens according to certain methods and an interactivity defined by the author and which the user will avail himself of as he pleases.

Cards (like a deck of cards or an index of cards) is a metaphor derived from the famous HyperCard software which was a pioneer in application design of this type of hypermedia, but which was not mentioned in our table because it is not fundamentally trans-platformable (even if the user can compile HyperCard piles in order to use them on PC in the ToolBook programme with a software called ConvertIt).

This metaphor was used by several new software programmes.
The cards are designed in the desired format and buttons are available to stick on them.
These buttons allow the user to pass from one card to another, defined in advance.
Any document may be stuck on a card and any element (document or section of a document) may in turn become a button which refers you to another document in another card.

Some of the editors of these multimedia applications design software programmes subject distribution on the market of applications derived from their product to rather strict rights which must be bought according to certain particular steps defined in the operating licence.
Fortunately, not all editors choose this policy and do not impose rights on the distribution of the applications derived from their products.
This question should also be taken into consideration and examined carefully before choosing the author system so as to avoid any unpleasant surprises at the last moment.

Among the functions available on an author system, the presence of tools for converting applications to the Internet format is a highly precious asset.
In fact, among all the possible media for distributing multimedia applications, Internet currently offers the most interesting prospects, but this question will be dealt with in Chapter 6.

Kiosks and simulation 

A kiosk is an interactive system built around a multimedia microcomputer whose screen and loudspeakers are the only visible elements.
This screen is, in fact, the dialogue interface.
It is touch sensitive: the term used is "tactile".
A message displayed on the screen invites the user to touch the desired icon and the sequence of corresponding information appears immediately.
These mechanisms are usually installed in public places and are at the disposal of users.
They are designed to deliver information.
It is a very attractive way of presenting messages.
Touching the screen avoids any mobile dialogue interface using the keyboard, the mouse, the handle or the ball which are always thrown out of function when in contact with clumsy or careless users.

Figure 33

Diagram of an interactive kiosk 

The touch screen does not run any risk of going out of order since nothing moves.
Furthermore, there is a magic quality in contact with the screen, for the user has only to point his finger like a magician on the desired object to obtain the corresponding information.

These systems are very widely used by banks, for example, in order to deliver information on client account management, and even as automatic tellers in order to deliver bank notes when these systems are accompanied by a specialized robot.
It is by far the most widely used application and the most popular in rich countries.

Kiosks are also often used for promotional commercial applications in shops or malls, to guide passengers in railway stations and airports, to provide information in museums, exhibitions or professional fairs.
Yet kiosks are also used in the context of the company to monitor production processes or give didactic information.

The touch screen 

This is usually a frame which is placed on the screen and which locates the coordinates of the point where the finger is placed in order to signal it to the system's management programme.
Sometimes the frame is incorporated into the screen, making the mechanism more reliable.
The tube screen may also be replaced by a liquid crystal panel, making the system very light and flat.
Several methods of detection are used by touch screen manufacturers: infra-red rays, magnetic fields or acoustic waves.
Each type of technology is characterized by its own sensitivity and response time.
These screens must be shock resistant, withstand chemical or organic substances left by the user's fingers, and liquids which may be spilt on them such as hot or cold beverages, detergents or other cleaning products.

Any interactive multimedia application developed to be used with a mouse on a microcomputer can in principle be transposed for use on an interactive kiosk.

To do so, the constraints peculiar to the kiosk application and environment must be taken into consideration.
This not only means the physical environment (space, location, light, noise) or the psychological context (stress, distraction, nervousness) in which the user is situated but the width of the user's finger must also be taken into consideration.

Simulation and virtual reality 

Modelling complex systems or virtual worlds as well as their management or navigation system allows the user to simulate his exploration of them and to learn about them.

"Simulation" is the generic term for the interactive representation of objects or scenes in 3D through calculations on a computer screen.
It is also called "picture synthesis".
Several games and didactic applications currently on the market allow the user to simulate the motion of machines, vehicles or more or less fantastic contraptions on dedicated stations which, in fact, are like arcade game machines in parks or specialized shops.
A great many of these games or didactic applications are also sold in the form of CD-ROMs, cassettes or diskettes for personal games or microcomputer consoles.
This market has made particularly significant strides these last years and has given rise to an entire industry.

What we call virtual reality is much more complex.
First of all, it is the representation of objects or scenes in 3D in the form of two pictures calculated in actual time (instantly) within two slightly different angles in order to restore the third dimension in stereoscopic vision.
The user can see these pictures by wearing a helmet.
When the helmet moves, the pictures are modified immediately in proportion through a monitoring system.
Furthermore, a manual dialogue interface, handle or digital glove allows the operator to intervene within this space, to move and seize objects in order to examine them, move them or change their shape.

A sound dimension can also be added to the scene and processed in the same way as pictures.

Systems of this kind exist for specific, didactic or game applications.
Yet they are very powerful, special systems, dedicated machines used in enterprises, theme parks or game arcades which bring into play several processors working in parallel.
The extent to which the operator becomes involved is proportional to the output and the realism of the pictures and sounds.
The experience is a gripping one and allows the learner or user to become very engrossed.
It is possible to represent very abstract concepts in this form and to explain their complexity in a didactic perspective and in a very concrete environment.

Figure 34

The principle behind a virtual reality system 

Although the software programmes which allow the user to model spaces and complex scenes in 3D with light, shadow, reflections, refractions, material output and animation features are available and are rather easy to use, virtual reality systems are not so common.

Calculating a single picture may take several hours depending on the available system and microprocessor.

Calculating the animation of objects which move along a course, like a succession of pictures, is even longer.
So the calculation in actual time with a realistic output of the picture implies a very powerful calculator which is still not available on microcomputer or even on a simple work station.

Of course there are card and programme kits which allow calculation in actual time of the pictures (and even sounds) corresponding, as the user's head moves, to the modification of the stereoscopic vision of a previously modelled space.
But the pictures you get are not very realistic and the involvement of the user is not as deep as on dedicated machines.
The number of colours in the pictures is limited, the edge of the pictures is jagged, the colours are flat and the movement of the pictures is rather jerky.
Yet these systems do exist and do offer scope for a certain number of experiments or specific uses, whether in industry, education or entertainment.

Currently, one of the most typical trends in enterprises is to use virtual reality to evaluate and virtually test their products, systems, machines and vehicles which are designed in research offices on computer-assisted design (CAD) stations.
Their life-span utilization or functioning features and performance can be checked and improved before even manufacturing the first prototypes.
In this way, it is possible to shorten the design cycle of products, match them better with the needs of the market and reduce the number of commercial flops.

Networks 

The production and distribution of digital files, documents or multimedia applications on networks are activities with a great future in store.
New communications features are now emerging, the virtues and limitations of which we know nothing.
The advantages are interesting and the services they provide today are of strategic importance to enterprises and public or private, national or international institutions.
These new activities in order of complexity are: electronic mail, long-distance exchange of electronic documents, the availability of and the possibility to research information, visual conferences, distance training, team work, the automatic updating of work documents, simultaneous monitoring of the design process, manufacture and marketing of products, and follow-up in terms of management and results.
The model hovering on the horizon is no other than that of the virtual enterprise.
A utopian model with an interactive structure immediately responding to all the risks of the present economic juncture is needed to offer an optimal response.

Electronic mail is making its way little by little into enterprises and institutions which use new communications and which enjoy the space, time and the number or availability of addressees.
The sender writes the message on the computer using a utility which is somewhat like a simplified word processor.
The message is addressed in the form of an ASCII file and delivered almost instantly and transparently in the electronic mailbox of the addressee.
The same message can be sent simultaneously to a list of different addressees just as easily.
The addressee reads the message when he can or when he wishes to, and he replies depending on his priorities.
Transmission of the message is in both directions immediate, immaterial and independent of the location of the persons involved or the distance separating them.
This written dialogue may go on for a long time if necessary.
Internal electronic newspapers or delivery services can also be created using multi-platform communications softwares such as First Class.

On the proper network and with the necessary tools, the persons involved can prolong this dialogue orally and even see one another in a window on their screens.
If need be, other persons can also join them.

Exchanges of digital (or real) documents can take place while the partners are communicating with each other.

They will study them and respond if need be in actual time.
With team work software programmes digital documents can even be shared by the partners on their individual screens and they can work together simultaneously, on the same subject, each making his own contribution, observations on his colleagues' work and giving his adjustment proposals.

Training in the enterprise enjoys the advantage of this media support to de-locate and accelerate periodical sessions of re-training or upgrading when employees need them.
Tutors and trainees no longer need to move from place to place and even the documents can be sent in digital form.
Work documents can be shared so that each person can do exercises in turn, either individually or with all the participants looking on, depending on the teaching objectives.
Interventions or corrections can be made on the spot by anyone: trainee, peers, or trainer.
The interest from the teaching point of view as compared with training in a classroom situation is that each participant can have the same document or presentation in front of his eyes at the same time, that he is alone in front of his screen and that the trainer can observe his attitudes as well as his responses.

Coordination tools perform the task of updating documents continuously in terms of each person's work so that the boundaries of the integration of distance team work in actual time recede even farther.
In this way, forms and control panels are always updated and provide an instant and overall view of the state of the work structure.
This automatic updating of work documents in actual time according to the tasks carried out at each station means that office work is being rationalized.

Such work typically belongs to the tertiary sector which, until now had remained a craft industry and had remained out of the reach of this industrialization phenomenon.

Sharing agendas provides each member of the team with an update of the schedule, the content or the nature of the tasks to be performed by each member.
Using these different solutions tends to reduce gaps if one considers the fast unpredictable development of the different contexts in which the enterprise participates: administration, markets, operations design, marketing, communication, training, product development, stocks, and production or distribution.

Technical solutions 

In keeping with what have seen earlier, a local network is a relatively large group of microcomputers called "clients" which are connected to each other by specific cables in order to communicate according to certain methods.
This network is managed by a microcomputer called the "server", using a network management software programme.
Several local network solutions allow for the connection of microcomputers to exchange information with transfer rates of between 4-16 megabytes per second.
These networks are designed to transfer data in bulk in an intermittent manner over distances no more than a few kilometres.
They can function within a building for office needs.
They are not suited to transmit a continuous flow of data of an audio or video kind with the classic professional criteria which demand technical solutions which we term as "heavy duty".
Yet "light" solutions do exist for transmitting video or audio data on simple local networks in acceptable conditions.

Ethernet is the most widely used solution for creating local networks.
It is a protocol which offers free scope to transmit data in bulk on co-axial cable, ordinary copper wire or fiber optic networks with a transfer rate which can reach up to 10 megabytes per second.
A more powerful version of this protocol, called Fast-Ethernet, allows for a retraction of 100 megabytes per second and can co-exist on a standard Ethernet network to transmit sound and video data in excellent conditions.

Mention should also be made of the FDDI (Fiber Distributed Data Interface) and the ATM (Asynchronous Transfer Mode) standards, which are more expensive solutions but do allow networks operating on other protocols to be connected on wide band networks by providing additional guarantees for multimedia data transfer.

Several local networks can be connected to set up an extended network (WAN: Wide Area Network) on specialized, high retraction telecommunications lines.

Common protocols and dialogue interfaces enable different local networks to communicate with one another.
One of the solutions for connecting heterogeneous local networks is the TCP/IP protocol (Transmission Control Protocol/ Internet Protocol).
It is currently one of the most economical and one of the easiest to implement.
It is widely used by individual users and enterprises alike.

Yet the transfer rate on Internet is variable and depends on rate of the specialized line traffic or on the traffic which the network bears simultaneously on the circuit used during the connection.

The direct telephone connection between two distant microcomputers (or between a distant microcomputer and a local network) through modems is a solution which is practical and easy to implement for temporary connections between partners, collaborators or private and public owner networks such as Internet.

Suitable communication software programmes are then used to transmit or receive electronic mail, faxes or any document in the form of a digital file.
Long-distance control softwares also enable the hard drive to be shared among several distant microcomputers connected by telephone lines.

E-mail 

The basic features of E-mail (electronic mail) are today incorporated in the form of extensions in the operating systems of the different platforms.
The messages or multimedia documents can be edited, then addressed by selecting the address of the partner from the directory, sent on the different networks to any type of platform in a transparent manner by using the "mail" command.
The messages are received in the letter box which is opened by clicking on it, and are then filed.

Not a single sheet of paper has been wasted.
But specialized software programmes are also offered on the market to perform these very tasks on microcomputers which are not usually equipped with them.

Internet has become a universal standard for electronic mail, whether international, national and even internal.
E-mail is even the most commonly used application on this network.
The user is not charged for the message as in the case of the X400 systems or private courier services.
On Internet, the price does not depend on distance as in the case of direct telephone connections.
It is the most economical solution.

Furthermore, it is multi-platform.

Internet electronic mail extensions allow the user to send and receive multimedia information through the MIME (Multimedia Internet Mail Extensions) standard.

This information is entered and disguised in an electronic message and if the system allows display of these data, the multimedia message will be read.

There is also an electronic mail feature peculiar to Internet: information groups (often called "Newsgroups").
These information groups are specific thematic public forums where the user can go to read information pertinent to the subjects which were previously discussed and available in the FAQ (Frequently Asked Questions) file.
It is then possible to attend or participate in the ongoing discussions.
Several thousand groups exist in this way on the most varied of themes.

They are classified by categories and the user can thus go through them in order to discover them or create new ones on original themes of particular interest.
A number of mailing lists exist in these programmes.
This allows the user to send the same message simultaneously to each of the addressees on the list in order to ask a question related to the group's theme or to provide a piece of information or a specific answer to a question which was previously asked.
Extremely precise contents and information are at hand in this way on the most diverse of subjects and constitute an inexhaustible wealth of resources for training.

The teleconference 

Like other authors, we have chosen the term "teleconference" which is the term used for digitization and transmission of audio and video signals on computer for the purpose of holding distance conferences.
To mark the distinction, the term "videoconference" is used when referring to conferences held using the traditional analogue methods of hertz or cable television without a microcomputer.
Notwithstanding, there is more and more talk of videoconferences on the microcomputer.

There are several solutions which offer scope for connecting several faraway microcomputers so that the users can see and hear their partners, work on the same document and exchange files.
The first set of solutions means directly connecting computers to one another through the classic telephone networks or digital networks such as the ISDN (Integrated Services Digital Networks).
To do so, connection kits are available on the market for all platforms and some exist on several platforms.

Figure 35

Diagram of a visual conference configuration 

These solutions usually involve an expansion board which allows video and sound signals to enter and be connected on the normal telephone or ISDN network.
The signals are digitized and compressed to be transferred across the network.
Once there, the signals are decompressed and displayed on the receiver's system.

Remote control of the long-distance camera by software is provided for on the Visit Video model.

All these solutions allow users to share what is called the "white board", on which the proposed documents or applications for the group work session are displayed.

Respecting the H 320 standard and other related standards guarantees a minimal interoperability between the different solutions on the same platform.
Some of these kits exist both in the MacOS and the Windows versions, but they do not enable the user to communicate between different platforms.

Table 24

The main visual conference kits 

The second set of solutions is based on Internet.

In fact, as mentioned earlier, several software programmes such as CU-SeeMe distributed freely on the network allow two users to exchange picture and sound if each has a microcomputer equipped with video and sound digitization capacities connected simultaneously to rapid modems.
The frequency of black and white pictures is variable, around 4 pictures per second; the sound is rather clear and this suffices for use, all the more since both acquiring it and using it are very economical.

The VideoVu kit currently sold by Future Communications Systems allows users to send and receive a 15-picture per second, audio and video signal using a standard modem and camcorder through packets of data in the normal TCP/IP protocol format called Unicast.
But other, more high-performance technologies like those using the Multicast TCP/IP protocol are already being tested and offer significantly better results: high quality sound and video will soon be possible.
Internet is fated to become a favourite medium for teleconferences.

Groupwares 

Groupwares are sometimes also called synergy-wares.
They are groups of integrated tools which make long-distance group work possible.
- office (word processing, graphics, tables, CAPre); 
- communications (courier, directory); 
- resource management (calendar, schedule);
- coordination ( forms, workflow); 
- information management (database, forum). 


Table 25

The main groupwares 

These integrated tools allow users to create group applications designed around the professional needs of each enterprise or organization.
Individual tools - word processing, graphic and table software programmes - become group work tools which distribute, share and update the information according to the activity, allowing users to optimize project and team work management.

The wealth and ease offered to develop these group work applications are proportional to the features of the different solutions which currently exist on the market.
Certain software programmes such as Notes and 4D became popular on account of their relative simplicity in terms of developing the applications and the proper integration of all their tools.
A simple and economical solution would be to use ready-made solutions which are usually offered and gradually adapt them to the specific needs of the organization.
Failing that, the organization would have to launch a rather complex computer development programme for amateurs or would have to resort to a specialist.

These solutions work on different platforms and on heterogeneous local networks, yet their interoperability is not yet effective, i.e., they still cannot fully communicate among one another.
Some which do conform to the OLE or AOCE interoperability standards can exchange documents, but collaboration goes no further for the moment.
In other words, an organization which uses Notes cannot fully collaborate with another organization which uses Exchange or Workgroup, for example.

The main logic which dominates in developing these optimization or automation of work methods and project management is, however, confronted with a need to connect and communicate with the outside world.
Opening on Internet is an increasingly high-priority objective for enterprises and organizations.
If the information processed on the groupware is often highly confidential, strategic and under tight security, it should also give rise to partial communications with the organization's partners.

For example, Notes' two external modules: Tile and InterNotes, allow the user to convert a Notes database to World Wide Web server pages on Internet.
Any Internet user can thus consult this Notes database if it is available in a non-coded mode on a World Wide Web server.
He may consult the database but he does not have at his disposal all the group work features of Notes.
He cannot erase or update documents from the database.
Yet the interest lies in the fact that these new Notes features increase the number of interactions possible with the organization.

Internet 

For two or three years now, Internet has become an international phenomenon of paramount importance.
The number of host servers, features, applications and users is growing at amazing speed and in astounding proportions.
At a time when the microcomputer was still in the shadows, the first seeds of Internet were sown when Paul Baran's publication (Rand Corporation) appeared describing the switching on networks of data paquets, entitled "On Distributed Communication Networks", followed by the implementation of this concept within the framework of the experimental network ARPANET in 1969 by the US Defense Department.

The principle consists of fractioning the data to be transmitted in little, uniform packets and channelling them to their final destination by any available path on the network.
The origin and destination is inscribed on each packet, as well as which packet it is to be linked to in order to re-compose the original data on arrival.

Along the entire network, the packets are guided by computers which calculate which is the least crowded available segment of the network in order to reach their destination in the quickest way.
With this system, communication may continue even if a part of the network has been destroyed.
The message switching is controlled by the terminal's intelligence and relayed as long as it is being broadcast, unlike the telephone or television mode, where switching and broadcasting intelligence are centralized.

Figure 36

Diagram of an Internet network

The TCP/IP (Transmission Control Protocol/Internet Protocol) communication protocol, which is at the foundation of Internet, functions on this model.
It enables microcomputer networks to communicate among themselves regardless of their operating system, type of cabling or their local network protocol.
But furthermore, as we mentioned when quoting Gilder in the introductory quotation, the network's power increases in the order of the number of its host servers squared.
Each new server becomes a resource for the others and increases the power and the quality of the network in an exponential ratio.
In fact, Internet is rather like an immense planetary information processor.
At every moment, the users present on the network share the entire available communication power.
As opposed to the telephone where the network has a maximum capacity which is saturated during peak hours ("Due to an overcrowding of circuits, your call cannot be completed"), on Internet, the place allotted to each user at any given moment is proportional to the number of users present at that moment.

How to get connected 

Internet is not a uniform network; it is composed of heterogeneous elements which it encompasses.
In fact, it is a network which interconnects other networks.
Each server computer on a network connected to Internet is what we called a host.
It can also quite simply be called a server.
The server provides access to Internet to any microcomputer connected to it carrying the utility programmes necessary to communicate.
The server is linked to a specialized telephone line which is itself connected to Internet and which has a rate of transmission proportional to the number of clients.
Connection to this line is supplied by a telecommunications company which charges for the installation and utilization according to its size.

If I get connected to Internet, I become the client of a server.
Suppose I am in an enterprise or an organization which pays the connection and rental charges.
I pay nothing more to consult it for one hour, eight hours, or forty-eight hours, because the line is usually fully paid.
But suppose I am a private individual and wish to connect my microcomputer to Internet.
I have to go to a private commercial server who can offer me access against payment of a full rental or for the interval.
In this case, I must pay the rental, because I am connected on the server network of my choice through my telephone line, through a modem, and in addition I pay my telephone bill at the end of the month.
The further away my server from my place of connection, the higher is my telephone bill.
It is a sorry fact, but this budget line tends to be the most costly one if the billing policies of telecommunication companies are not competitive in my country.

Today, Internet is no longer the anarchic, public and ever-changing phenomenon it was in the beginning, but it has not yet quite recovered commercially - it is going through a transition phase.
The Internet Society (a non-profit-making organization) which supports IETF (Internet Engineering Task Force) and IAB (Internet Architecture Board), two organizations of international voluntary pioneers and specialists, still monitor the network's progress and its standards.
From the outset, the network was financed by the US Defense Department.
In the early 1980s, NSF (National Science Foundation) took over to develop the main physical infrastructure of the specialized lines (commonly called "backbones") which link the servers.
This infrastructure was maintained by its technical operator: ANS (Advanced Network & Services), a non-profit-making research organization established in 1990 by IBM, MCI and Merit Network.
ANS recently sold this infrastructure to America OnLine, one of the leading on-line commercial service companies.
The maintenance of the infrastructure is now jointly in the hands of ANS, America OnLine and two other powerful communication service companies: MCI and Sprint.

Characterized by its TCP/IP communication protocol which is at its base, Internet is like a succession of increasingly sophisticated software layers which successively add richer and easier-to-use features.
The basic feature is E-mail.
Each microcomputer connected to Internet has an address provided by its server.
The address takes the form of a number of characters, the first part being an abridged version of the person's name and the last part being the name of the server and the country in which the sender server is located.
These two parts are separated by the sign xxx , which is a shortened version of the English word "at", e.g. xxx .

Figure 37

Diagrammatic structure of Internet 

In order to send a message, the sender needs to have the address of the receiver(s).
As we have seen, sound, graphic or video files may be included in a message using a MIME extension.
The user also has the possibility of participating in thematic information groups (called "Newsgroups") and he can have access to the lists of frequently asked questions in each group as well as their answers.
Currently, there are more than thirty thousand Newsgroups and just as many Mailing Lists.
A large number of these groups concern general or technical educational institutes, educational science, human and exact science institutes.

Telnet is an Internet application which allows the user to be directly connected long-distance on a public access computer to conduct research in its database.
This application is used to research references or files, but the commands are not easy to master and require private training.

FTP (File Transfer Protocol) is another Internet application which allows the user to transfer to his own hard drive files located on faraway computers which allow this.
In fact there are several servers which place an enormous quantity of databases and services available through the FTP protocol at the disposal of the user.
Connection requires an entire procedure which implies learning the necessary commands.

Telnet and FTP assume that the user knows where to go.
To do so, the user has to locate the information he seeks. 
Archie is a system which enables the user to locate an available file on a public server.
Currently, Archie indexes several millions of files spread over more than one thousand servers.
The user needs to define the research criteria using a word.

Then Archie can propose file names and the description of their functions.
At the end of the search, it provides a list of all the files corresponding to the defined criteria and the servers at which they are located.
Once the file is found, the user has only to retrieve it using FTP.
Client versions of Archie exist on the different platforms and now also carry FTP.

Gopher of Internet is an advanced tool which enables the user to access Internet's resources in a simplified way.
Gopher's client programmes are available on all platforms and allow the user to access Gopher's servers.
Once it is launched, the client programme asks its server for the main menu.
All Gopher servers are frequently interconnected and have a standard interface.
The user makes his choice by defining his search and the process is launched.
The files identified can then be retrieved.
But the procedure is still not very easy to use without previous training.

WAIS (Wide Area Information Servers) allows the user to do textual searches using key words in databases, not through file names or the definition of their functions any more, but through their content.
The client programmes of WAIS servers which exist on the major platforms allow the user to access these resources using a simplified interface.

The user does not need to worry about locating the server which contains the information being sought.
In spite of these interesting new features, its user interface is still too much of a puzzle to have earned itself a wide public.

World Wide Web is a client/server system like Gopher, Archie or WAIS, enabling the user to access Internet's resources, but it is much more flexible to use.
Of all the systems developed on Internet, Web is the most successful.

It was designed and developed on the hypertext concept by Tim Berners-Lee, at that time a researcher at GERN in Geneva.
It was first implemented in late 1992, and by March 1995 there were 30,000 servers throughout the world.
Currently, the estimated traffic generated by the Web on Internet represents 20 per cent of the total traffic.

The Web can be explored using a navigation programme (also called "browser", "navigation software programmes" or simply "navigators").
These navigators are sophisticated graphic interfaces which make it easier to go through the Web server resources on which the information is organized according to the hypermedia model.
Web servers lodge documents which contain text, graphics, sound and video.
These elements, when connected by virtual links (hyperlinks) to other elements in the same server or in another far away, are underlined or framed in such a way as to show the user that he can click on them to access the information they contain.

Web uses the HTTP (HyperText Transfer Protocol) protocol which finds and displays the documents as quickly as the platform and the connection permit.

Documents published on the Web can be edited by using an author programme which is based on HTML (HyperText Markup Language).
The text is integrated in the ASCII format.

When it is underlined, it is used as a pointer for multimedia files and to locate their servers.
When the user clicks on the underlined word, the hyperlink tells the Web which server it should be connected to and which file it should retrieve.

Each Web server has an address which always begins with the prefix xxx and which may be, for example: xxx .

The user connected to Internet can obtain free of charge all the elements necessary to navigate on the Web servers of his choice.
He needs most of all a navigation programme such as Mosaic or Netscape, which is more recent, more rapid and more advanced.
He just has to indicate the server address on which he wishes to be connected in the appropriate dialogue window, the connection is made and the opening page displayed.
He carries out the search by clicking successively on the underlined bold or framed elements which correspond to his questions.

These services are accessible on all platforms using Mosaic and Netscape navigators, but there will be new programmes which will be directly incorporated to the operating systems.
World Wide Web has become a standard within everyone's reach to access multimedia information in a simple way.
It is simple to access this information and almost as easy to publish it.

Today, big trade union offices are opening Web servers, such as AFL-CIO, for example, or the Worker's Education magazine and other IFWEA (International Federation of Workers' Education Associations) publications, which also edit their archives on this medium.
Formerly, certain universities were dedicated to long-distance teaching such as the Télé-Université of Quebec, but today distance-learning universities, schools and training centres are adopting this concept on Internet and the Web.

Publishing information on World Wide Web 

These days, there are four solutions for information publication on the Web for lone users throughout the world and for work groups.

Figure 38

Number of World Wide Web servers 

The first consists of completely subprocessing the editing and distribution of the pages.

The second consists of editing the pages yourself and handing them over to the server for distribution.

The third consists of editing yourself and distributing from your own network, connected to the server network.

Finally, the fourth consists of producing and distributing yourself in an entirely autonomous manner.

Everything depends on your budget or skills and the autonomy you need.

Until recently, not only did the user need to have a good idea of how to produce multimedia documents on a microcomputer, but he also needed to have access to a network under Unix in order to publish and distribute information on the Web.
Now there are kits which allow the user to set up a configuration on the platform of his choice in order to edit and distribute pages in the HTML (HyperText Markup Language) format.
We saw that multimedia applications design softwares such as M/Tropolis or PowerMedia which have recently appeared, allow the user not only to edit multimedia applications, but also to convert them to the HTML format.

The latest versions of the main Word or WordPerfect word processing standards as well as the Xpress and FrameMaker page setting software programmes also enable the user to convert the documents to edit to the HTML format.
Before, the user needed to code the page-setting of previously created documents and the interactivity of applications with the specific syntax.
HTML editor programmes such as W3 or BBEdit on Macintosh also allow the user to carry out this coding task automatically using an existing document.

Computer manufacturers such as Apple Computers, Performance Technology or Sun Microsystems currently supply key servers on the market.
In the case of Apple, there are AIS (Apple Internet Servers) based on PowerMac microcomputers.
These AIS are equipped with a group of programmes distributed on CDROM and designed to install the server automatically and guarantee all the functions needed for its design, without having any notion of Unix, starting with the design of Web's pages and going as far as managing the local network and the transactions with all the client platforms (Windows or Unix) on Internet and the Web.
Of all the programmes provided, WebStar (or MacHTTP) has graphic interface which makes it very easy to use and very efficiently assures this function of monitoring the server and the simultaneous calls of the clients. For its part, Performance Technology proposes Instant Internet.
It is a complete solution in the form of a case which is connected to the local network such as NetWare IPX LAN in Windows to serve as a bridge between the network and Internet.
It is not a server in the strict sense of the word.
It is simply an economical way of connecting a network of microcomputers to Internet.
It contains all the hardware and software elements necessary for its installation and functioning: Ethernet port, modem and Intel Processor 486SX.
The installation is rather easy and the programmes needed to manage the network and the navigation process are supplied.
However, it is an exclusive Windows server and only the clients operating under this operating system can gain access to the services it supplies.

Usually in the PC environment, services are designed on powerful Intel Pentium (or DEC Alpha) microprocessor-based machines under Microsoft Windows NT 3.5, with the EMWAC Web Server (European Microsoft Windows Academic Consortium) programme.

Sun has for long been involved in the design and management of TCP/IP servers under Unix, and it is now proposing a key server: the Netra Internet Server station.
It is a case containing a main Sun work station card operating under Solaris (the Unix version of Sun).

Inserted in a network of work stations, it gives any machine on the network access to Internet.
Yet, like Instant Internet, it is not really a server.
The Netra Internet Server case must be installed by a specialist, but it can be managed by a network user who is already familiar with Unix.

The future of networks 

We have seen that installing a network or a server based on the TCP/IP protocol is fairly easy.
This solution has the advantage of being both universal and economical.
It can be operated on local networks for communicating on Internet, exchanging occasional electronic messages, holding visual conferences, searching for information and training materials or working in collaboration with colleagues far away or at the other end of the world.

This technology is hurtling along by leaps and bounds, relying as it does on a corps of students and researchers who, always in concert, already use it as a group work medium, a fact which accounts all the more for the rapid rate at which it is developing.
The performance and features we know today are nothing compared to what they will be tomorrow.
The commercial applications coming to the fore today are whetting appetites.
All the big companies in the sector are mobilizing all their efforts to occupy strategic niches.
The security of commercial transactions is still a problem on this type of public network where "hackers", these young digital pirates, break into the networks of big companies or public institutions and get hold of strategic information.
Some even commit destructive acts by introducing computer viruses in the systems and networks they visit.

Until recently, Internet presented the disadvantage of not offering sufficient guarantees of confidentiality and of exposing servers and local networks to the actions of these pirates.
Ciphering solutions are now available.
Only those national legislations which limit the use of these ciphering methods are still opposed to their use.
Whatever the case, if there does exist a subject which above all does not need to remain the domain of the select few, it is surely the availability of the most effective information and didactic resources for the defence of workers' rights and for workers' education, which on the contrary should be conducted with the tool which guarantee the widest possible dissemination.

Conclusion 

All along this somewhat unusual course we have been treading in cross-section, we have attempted to identify the technologies and methods now offered in this period of change and crisis to trade unions and institutions dedicated to educating and training workers.

These partners must by all means pursue efficient communication techniques and training programmes adapted to today's needs.
They must have a share in producing or providing the corresponding communication documents and the suitable didactic resources using the proper means.

As we have seen throughout this document, these training or communication programmes imply establishing an appropriate technological context.
In conclusion, we shall try to provide in the list below a synopsis of the main advantages of implementing these new technologies in the context of training.

Economical and world-wide publication and distribution of information.
Digital technologies in general and Internet in particular provide rapid, efficient communication with all the partners in the working world.
Each partner can place all the information he deems useful at everyone's disposal.

Optimization of documentary research.
The practice of using networks enables users to experiment and develop more sophisticated research methods in an increasing number of data banks with more and more storage space which give access to detailed and relevant information.

On-the-spot training.
Training can be brought to the individual where he needs it.
The worker can receive in his very workplace, at home on his microcomputer or at the trade union centre the information or training he needs to become better at his job, or adapt his knowledge and know-how to the new tasks society expects him to accomplish.

Adapting to the training pace.
The computer, not temperamental, is an untiring tutor which adjusts, with discretion and without partiality, to the pace, availability, location or intellectual or manual abilities of each trainee.

Actual time and dimension environment simulation.
Computers allow users to simulate all complex systems: real market occurrences or production systems needed to study and analyse a situation and take the most effective decision.

Increased interactivity among sectors.

The application of information research methods, work processes or group training techniques on the networks increases the amount of contact among the workers and their partners by consolidating their assimilation of information and knowledge as well as improving the swiftness and quality of decision-making.

Confidentiality of training.
The individual in front of his microcomputer may be isolated and therefore be rid of any fear or block regarding the fact that his colleagues are present or are looking on.

In this way, he feels more confident and is consequently in a better position to understand and assimilate information in order to transform this into knowledge and know-how.

Regulating trainee behaviour.
The decisions the user must make in order to forge ahead with the operations require his attention and make demands on his judgement and his sense of responsibility.
His attention is channelled through the running of programme which he himself controls at his own pace.
All this helps to regulate the behaviour of unstable, difficult or obstinate individuals.

Changing the trainer's role.
Since information and knowledge are not exclusively transmitted by the trainer, his role now consists rather of being a programme designer, a methodologist, a guide for the trainees so that they may autonomously gain access to the resources and use them efficiently according to their needs.

Stimulating curiosity, creativity and team work.
Applying communication techniques of this kind may bring to the forefront the qualities of each member of the team by generally improving each individual's responsibility and his share in the fate of the organization of which he is a part.

The media are here today, making it possible for all organizations concerned, in all countries, to work together right away, in concert, on an international scale and on a permanent basis, and to take part in this sweeping movement of reflection and renewal in trade union action and workers' training programmes.
So the better off among them can quickly and easily place their resources, information, knowledge, advice and assistance at the service of the less fortunate in a healthy gesture of solidarity.
